{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Documentation page for the Deep learning based Model Discovery package DeepMoD. DeePyMoD is a PyTorch-based implementation of the DeepMoD algorithm for model discovery of PDEs and ODEs. github.com/PhIMaL/DeePyMoD . This work is based on two papers: The original DeepMoD paper arXiv:1904.09406 , presenting the foundation of this neural network driven model discovery and a follow-up paper [xxx] describing a modular plug and play framework. Summary DeepMoD is a modular model discovery framewrok aimed at discovering the ODE/PDE underlying a spatio-temporal dataset. Essentially the framework is comprised of four components: Function approximator, e.g. a neural network to represent the dataset, Function library on which the model discovery is performed, Constraint function that constrains the neural network with the obtained solution Sparsity selection algorithm.","title":"Home"},{"location":"#summary","text":"DeepMoD is a modular model discovery framewrok aimed at discovering the ODE/PDE underlying a spatio-temporal dataset. Essentially the framework is comprised of four components: Function approximator, e.g. a neural network to represent the dataset, Function library on which the model discovery is performed, Constraint function that constrains the neural network with the obtained solution Sparsity selection algorithm.","title":"Summary"},{"location":"api/analysis/","text":"Results Deprecated? load_tensorboard ( path ) Loads tensorboard files into a pandas dataframe. Assumes one run per folder! Parameters: Name Type Description Default path string path of folder with tensorboard files. required Returns: Type Description DataFrame Pandas dataframe with all run data. Source code in deepymod/analysis/load_tensorboard.py def load_tensorboard ( path ): \"\"\" Loads tensorboard files into a pandas dataframe. Assumes one run per folder! Args: path (string): path of folder with tensorboard files. Returns: DataFrame: Pandas dataframe with all run data. \"\"\" event_paths = [ file for file in os . walk ( path , topdown = True ) if file [ 2 ][ 0 ][: len ( 'events' )] == 'events' ] df = pd . DataFrame () steps = None # steps are the same for all files for event_idx , path in enumerate ( event_paths ): summary_iterator = EventAccumulator ( os . path . join ( path [ 0 ], path [ 2 ][ 0 ])) . Reload () tags = summary_iterator . Tags ()[ 'scalars' ] data = [[ event . value for event in summary_iterator . Scalars ( tag )] for tag in tags ] if steps is None : steps = [ event . step for event in summary_iterator . Scalars ( tags [ 0 ])] # Adding to dataframe tags = [ tag . replace ( '/' , '_' ) for tag in tags ] # for name consistency if event_idx > 0 : # We have one file in the top level, so after we need to use folder name tags = [ path [ 0 ] . split ( '/' )[ - 1 ]] for idx , tag in enumerate ( tags ): df [ tag ] = data [ idx ] df . index = steps return df","title":"Analysis"},{"location":"api/analysis/#deepymod.analysis.load_tensorboard","text":"","title":"deepymod.analysis.load_tensorboard"},{"location":"api/analysis/#deepymod.analysis.load_tensorboard.Results","text":"Deprecated?","title":"Results"},{"location":"api/analysis/#deepymod.analysis.load_tensorboard.load_tensorboard","text":"Loads tensorboard files into a pandas dataframe. Assumes one run per folder! Parameters: Name Type Description Default path string path of folder with tensorboard files. required Returns: Type Description DataFrame Pandas dataframe with all run data. Source code in deepymod/analysis/load_tensorboard.py def load_tensorboard ( path ): \"\"\" Loads tensorboard files into a pandas dataframe. Assumes one run per folder! Args: path (string): path of folder with tensorboard files. Returns: DataFrame: Pandas dataframe with all run data. \"\"\" event_paths = [ file for file in os . walk ( path , topdown = True ) if file [ 2 ][ 0 ][: len ( 'events' )] == 'events' ] df = pd . DataFrame () steps = None # steps are the same for all files for event_idx , path in enumerate ( event_paths ): summary_iterator = EventAccumulator ( os . path . join ( path [ 0 ], path [ 2 ][ 0 ])) . Reload () tags = summary_iterator . Tags ()[ 'scalars' ] data = [[ event . value for event in summary_iterator . Scalars ( tag )] for tag in tags ] if steps is None : steps = [ event . step for event in summary_iterator . Scalars ( tags [ 0 ])] # Adding to dataframe tags = [ tag . replace ( '/' , '_' ) for tag in tags ] # for name consistency if event_idx > 0 : # We have one file in the top level, so after we need to use folder name tags = [ path [ 0 ] . split ( '/' )[ - 1 ]] for idx , tag in enumerate ( tags ): df [ tag ] = data [ idx ] df . index = steps return df","title":"load_tensorboard()"},{"location":"api/constraint/","text":"This module contains concrete implementations of the constraint component. LeastSquares Implements the constraint as a least squares problem solved by QR decomposition. calculate_coeffs ( self , sparse_thetas , time_derivs ) Calculates the coefficients of the constraint using the QR decomposition for every pair of sparse feature matrix and time derivative. Parameters: Name Type Description Default sparse_thetas List[torch.Tensor] List containing the sparse feature tensors. required time_derivs List[torch.Tensor] List containing the time derivatives. required Returns: Type Description List[torch.Tensor] [TensorList]: Calculated coefficients. Source code in deepymod/model/constraint.py def calculate_coeffs ( self , sparse_thetas : TensorList , time_derivs : TensorList ) -> TensorList : \"\"\"Calculates the coefficients of the constraint using the QR decomposition for every pair of sparse feature matrix and time derivative. Args: sparse_thetas (TensorList): List containing the sparse feature tensors. time_derivs (TensorList): List containing the time derivatives. Returns: [TensorList]: Calculated coefficients. \"\"\" opt_coeff = [] for theta , dt in zip ( sparse_thetas , time_derivs ): Q , R = torch . qr ( theta ) # solution of lst. sq. by QR decomp. opt_coeff . append ( torch . inverse ( R ) @ Q . T @ dt ) # Putting them in the right spot coeff_vectors = [ torch . zeros (( mask . shape [ 0 ], 1 )) . to ( coeff_vector . device ) . masked_scatter_ ( mask [:, None ], coeff_vector ) for mask , coeff_vector in zip ( self . sparsity_masks , opt_coeff )] return coeff_vectors","title":"Constraints"},{"location":"api/constraint/#deepymod.model.constraint","text":"This module contains concrete implementations of the constraint component.","title":"deepymod.model.constraint"},{"location":"api/constraint/#deepymod.model.constraint.LeastSquares","text":"Implements the constraint as a least squares problem solved by QR decomposition.","title":"LeastSquares"},{"location":"api/constraint/#deepymod.model.constraint.LeastSquares.calculate_coeffs","text":"Calculates the coefficients of the constraint using the QR decomposition for every pair of sparse feature matrix and time derivative. Parameters: Name Type Description Default sparse_thetas List[torch.Tensor] List containing the sparse feature tensors. required time_derivs List[torch.Tensor] List containing the time derivatives. required Returns: Type Description List[torch.Tensor] [TensorList]: Calculated coefficients. Source code in deepymod/model/constraint.py def calculate_coeffs ( self , sparse_thetas : TensorList , time_derivs : TensorList ) -> TensorList : \"\"\"Calculates the coefficients of the constraint using the QR decomposition for every pair of sparse feature matrix and time derivative. Args: sparse_thetas (TensorList): List containing the sparse feature tensors. time_derivs (TensorList): List containing the time derivatives. Returns: [TensorList]: Calculated coefficients. \"\"\" opt_coeff = [] for theta , dt in zip ( sparse_thetas , time_derivs ): Q , R = torch . qr ( theta ) # solution of lst. sq. by QR decomp. opt_coeff . append ( torch . inverse ( R ) @ Q . T @ dt ) # Putting them in the right spot coeff_vectors = [ torch . zeros (( mask . shape [ 0 ], 1 )) . to ( coeff_vector . device ) . masked_scatter_ ( mask [:, None ], coeff_vector ) for mask , coeff_vector in zip ( self . sparsity_masks , opt_coeff )] return coeff_vectors","title":"calculate_coeffs()"},{"location":"api/convergence/","text":"Convergence Implements convergence criterium. Convergence is when change in patience epochs is smaller than delta. __call__ ( self , epoch , l1_norm ) special Parameters: Name Type Description Default epoch int Current epoch of the optimization required l1_norm Tensor Value of the L1 norm required Source code in deepymod/training/convergence.py def __call__ ( self , epoch : int , l1_norm : torch . Tensor ) -> None : \"\"\" Args: epoch (int): Current epoch of the optimization l1_norm (torch.Tensor): Value of the L1 norm \"\"\" if self . start_l1 is None : self . start_l1 = l1_norm elif torch . abs ( self . start_l1 - l1_norm ) . item () < self . delta : if ( epoch - self . best_iteration ) >= self . patience : self . converged = True else : self . start_l1 = l1_norm self . best_iteration = epoch","title":"Convergence"},{"location":"api/convergence/#deepymod.training.convergence","text":"","title":"deepymod.training.convergence"},{"location":"api/convergence/#deepymod.training.convergence.Convergence","text":"Implements convergence criterium. Convergence is when change in patience epochs is smaller than delta.","title":"Convergence"},{"location":"api/convergence/#deepymod.training.convergence.Convergence.__call__","text":"Parameters: Name Type Description Default epoch int Current epoch of the optimization required l1_norm Tensor Value of the L1 norm required Source code in deepymod/training/convergence.py def __call__ ( self , epoch : int , l1_norm : torch . Tensor ) -> None : \"\"\" Args: epoch (int): Current epoch of the optimization l1_norm (torch.Tensor): Value of the L1 norm \"\"\" if self . start_l1 is None : self . start_l1 = l1_norm elif torch . abs ( self . start_l1 - l1_norm ) . item () < self . delta : if ( epoch - self . best_iteration ) >= self . patience : self . converged = True else : self . start_l1 = l1_norm self . best_iteration = epoch","title":"__call__()"},{"location":"api/deepmod/","text":"This file contains the four building blocks for the deepmod framework: 1) Function approximator, e.g. a neural network to represent the dataset, XXXX Not present yet 2) Function library on which the model discovery is performed, 3) Constraint function that constrains the neural network with the obtained solution 4) Sparsity selection algorithm. These are all abstract classes and implement the flow logic, rather than the specifics. Constraint Abstract class implementing the constraint set to the function approximator. Parameters: Name Type Description Default nn PyTorch Class Module of the function approximator, typically a neural network. required apply_mask ( self , thetas ) Function that applies the sparsity mask to the library function. Parameters: Name Type Description Default thetas <function NewType.<locals>.new_type at 0x11d447320> List of library functions, one for every output. required Returns: Type Description <function NewType.<locals>.new_type at 0x11d447320> TensorList: The sparse version of the library function. Source code in deepymod/model/deepmod.py def apply_mask ( self , thetas : TensorList ) -> TensorList : \"\"\" Function that applies the sparsity mask to the library function. Args: thetas (TensorList): List of library functions, one for every output. Returns: TensorList: The sparse version of the library function. \"\"\" sparse_thetas = [ theta [:, sparsity_mask ] for theta , sparsity_mask in zip ( thetas , self . sparsity_masks )] return sparse_thetas forward ( self , input ) Updates the coefficient vector for a given estimation of the library function and time derivatives. Parameters: Name Type Description Default input Tuple[TensorList, TensorList] Tuple of tensors, containing an estimate of the time derivatives and the library function required Source code in deepymod/model/deepmod.py def forward ( self , input : Tuple [ TensorList , TensorList ]) -> Tuple [ TensorList , TensorList ]: \"\"\"Updates the coefficient vector for a given estimation of the library function and time derivatives. Args: input (Tuple[TensorList, TensorList]): Tuple of tensors, containing an estimate of the time derivatives and the library function \"\"\" time_derivs , thetas = input if self . sparsity_masks is None : self . sparsity_masks = [ torch . ones ( theta . shape [ 1 ], dtype = torch . bool ) . to ( theta . device ) for theta in thetas ] sparse_thetas = self . apply_mask ( thetas ) self . coeff_vectors = self . calculate_coeffs ( sparse_thetas , time_derivs ) DeepMoD DeepMoD class integrating the various buiding blocks of the algorithm. It performs a function approximation of the data, calculates the library and time-derivatives thereof, constrains the function approximator to the obtained solution and applies the sparisty pattern of the underlying PDE. Parameters: Name Type Description Default nn PyTorch Class Module of the function approximator, typically a neural network. required forward ( self , input ) [summary] Parameters: Name Type Description Default input Tensor Tensor of shape (n_samples x (n_spatial + 1)) containing the coordinates, first column should be the time coordinate. required Returns: Type Description Tuple[TensorList, TensorList, TensorList] Tuple[TensorList, TensorList, TensorList]: Tuple of tensors containing a tensor of shape (n_samples x n_features) containing the target data, a tensor of the time derivative of the data and the function library. Source code in deepymod/model/deepmod.py def forward ( self , input : torch . Tensor ) -> Tuple [ TensorList , TensorList , TensorList ]: \"\"\"[summary] Args: input (torch.Tensor): Tensor of shape (n_samples x (n_spatial + 1)) containing the coordinates, first column should be the time coordinate. Returns: Tuple[TensorList, TensorList, TensorList]: Tuple of tensors containing a tensor of shape (n_samples x n_features) containing the target data, a tensor of the time derivative of the data and the function library. \"\"\" prediction , coordinates = self . func_approx ( input ) time_derivs , thetas = self . library (( prediction , coordinates )) self . constraint (( time_derivs , thetas )) return prediction , time_derivs , thetas Estimator Abstract class implementing the sparsity estimator set to the function approximator. Parameters: Name Type Description Default nn PyTorch Class Module of the function approximator, typically a neural network. required forward ( self , thetas , time_derivs ) This function nomalized the library and time derivatives and calculates the corresponding sparisity mask. Parameters: Name Type Description Default thetas <function NewType.<locals>.new_type at 0x11d447320> List of library functions, one for every output. required time_derivs <function NewType.<locals>.new_type at 0x11d447320> List of time derivates of the data, one for every output. required Returns: Type Description <function NewType.<locals>.new_type at 0x11d447320> TensorList: A list of sparsity masks, one for every output. Source code in deepymod/model/deepmod.py def forward ( self , thetas : TensorList , time_derivs : TensorList ) -> TensorList : \"\"\"This function nomalized the library and time derivatives and calculates the corresponding sparisity mask. Args: thetas (TensorList): List of library functions, one for every output. time_derivs (TensorList): List of time derivates of the data, one for every output. Returns: TensorList: A list of sparsity masks, one for every output. \"\"\" # we first normalize theta and the time deriv with torch . no_grad (): normed_time_derivs = [( time_deriv / torch . norm ( time_deriv )) . detach () . cpu () . numpy () for time_deriv in time_derivs ] normed_thetas = [( theta / torch . norm ( theta , dim = 0 , keepdim = True )) . detach () . cpu () . numpy () for theta in thetas ] self . coeff_vectors = [ self . fit ( theta , time_deriv . squeeze ())[:, None ] for theta , time_deriv in zip ( normed_thetas , normed_time_derivs )] sparsity_masks = [ torch . tensor ( coeff_vector != 0.0 , dtype = torch . bool ) . squeeze () . to ( thetas [ 0 ] . device ) # move to gpu if required for coeff_vector in self . coeff_vectors ] return sparsity_masks Library Abstract class that calculates the library function and time derivatives. Parameters: Name Type Description Default nn PyTorch Class Module of the function approximator, typically a neural network. required forward ( self , input ) [summary] Parameters: Name Type Description Default input Tuple[TensorList, TensorList] [description] required Returns: Type Description Tuple[TensorList, TensorList] Tuple[TensorList, TensorList]: [description] Source code in deepymod/model/deepmod.py def forward ( self , input : Tuple [ TensorList , TensorList ]) -> Tuple [ TensorList , TensorList ]: \"\"\"[summary] Args: input (torch.Tensor): [description] Returns: Tuple[TensorList, TensorList]: [description] \"\"\" time_derivs , thetas = self . library ( input ) self . norms = [( torch . norm ( time_deriv ) / torch . norm ( theta , dim = 0 , keepdim = True )) . detach () . squeeze () for time_deriv , theta in zip ( time_derivs , thetas )] return time_derivs , thetas","title":"DeepMoD"},{"location":"api/deepmod/#deepymod.model.deepmod","text":"This file contains the four building blocks for the deepmod framework: 1) Function approximator, e.g. a neural network to represent the dataset, XXXX Not present yet 2) Function library on which the model discovery is performed, 3) Constraint function that constrains the neural network with the obtained solution 4) Sparsity selection algorithm. These are all abstract classes and implement the flow logic, rather than the specifics.","title":"deepymod.model.deepmod"},{"location":"api/deepmod/#deepymod.model.deepmod.Constraint","text":"Abstract class implementing the constraint set to the function approximator. Parameters: Name Type Description Default nn PyTorch Class Module of the function approximator, typically a neural network. required","title":"Constraint"},{"location":"api/deepmod/#deepymod.model.deepmod.Constraint.apply_mask","text":"Function that applies the sparsity mask to the library function. Parameters: Name Type Description Default thetas <function NewType.<locals>.new_type at 0x11d447320> List of library functions, one for every output. required Returns: Type Description <function NewType.<locals>.new_type at 0x11d447320> TensorList: The sparse version of the library function. Source code in deepymod/model/deepmod.py def apply_mask ( self , thetas : TensorList ) -> TensorList : \"\"\" Function that applies the sparsity mask to the library function. Args: thetas (TensorList): List of library functions, one for every output. Returns: TensorList: The sparse version of the library function. \"\"\" sparse_thetas = [ theta [:, sparsity_mask ] for theta , sparsity_mask in zip ( thetas , self . sparsity_masks )] return sparse_thetas","title":"apply_mask()"},{"location":"api/deepmod/#deepymod.model.deepmod.Constraint.forward","text":"Updates the coefficient vector for a given estimation of the library function and time derivatives. Parameters: Name Type Description Default input Tuple[TensorList, TensorList] Tuple of tensors, containing an estimate of the time derivatives and the library function required Source code in deepymod/model/deepmod.py def forward ( self , input : Tuple [ TensorList , TensorList ]) -> Tuple [ TensorList , TensorList ]: \"\"\"Updates the coefficient vector for a given estimation of the library function and time derivatives. Args: input (Tuple[TensorList, TensorList]): Tuple of tensors, containing an estimate of the time derivatives and the library function \"\"\" time_derivs , thetas = input if self . sparsity_masks is None : self . sparsity_masks = [ torch . ones ( theta . shape [ 1 ], dtype = torch . bool ) . to ( theta . device ) for theta in thetas ] sparse_thetas = self . apply_mask ( thetas ) self . coeff_vectors = self . calculate_coeffs ( sparse_thetas , time_derivs )","title":"forward()"},{"location":"api/deepmod/#deepymod.model.deepmod.DeepMoD","text":"DeepMoD class integrating the various buiding blocks of the algorithm. It performs a function approximation of the data, calculates the library and time-derivatives thereof, constrains the function approximator to the obtained solution and applies the sparisty pattern of the underlying PDE. Parameters: Name Type Description Default nn PyTorch Class Module of the function approximator, typically a neural network. required","title":"DeepMoD"},{"location":"api/deepmod/#deepymod.model.deepmod.DeepMoD.forward","text":"[summary] Parameters: Name Type Description Default input Tensor Tensor of shape (n_samples x (n_spatial + 1)) containing the coordinates, first column should be the time coordinate. required Returns: Type Description Tuple[TensorList, TensorList, TensorList] Tuple[TensorList, TensorList, TensorList]: Tuple of tensors containing a tensor of shape (n_samples x n_features) containing the target data, a tensor of the time derivative of the data and the function library. Source code in deepymod/model/deepmod.py def forward ( self , input : torch . Tensor ) -> Tuple [ TensorList , TensorList , TensorList ]: \"\"\"[summary] Args: input (torch.Tensor): Tensor of shape (n_samples x (n_spatial + 1)) containing the coordinates, first column should be the time coordinate. Returns: Tuple[TensorList, TensorList, TensorList]: Tuple of tensors containing a tensor of shape (n_samples x n_features) containing the target data, a tensor of the time derivative of the data and the function library. \"\"\" prediction , coordinates = self . func_approx ( input ) time_derivs , thetas = self . library (( prediction , coordinates )) self . constraint (( time_derivs , thetas )) return prediction , time_derivs , thetas","title":"forward()"},{"location":"api/deepmod/#deepymod.model.deepmod.Estimator","text":"Abstract class implementing the sparsity estimator set to the function approximator. Parameters: Name Type Description Default nn PyTorch Class Module of the function approximator, typically a neural network. required","title":"Estimator"},{"location":"api/deepmod/#deepymod.model.deepmod.Estimator.forward","text":"This function nomalized the library and time derivatives and calculates the corresponding sparisity mask. Parameters: Name Type Description Default thetas <function NewType.<locals>.new_type at 0x11d447320> List of library functions, one for every output. required time_derivs <function NewType.<locals>.new_type at 0x11d447320> List of time derivates of the data, one for every output. required Returns: Type Description <function NewType.<locals>.new_type at 0x11d447320> TensorList: A list of sparsity masks, one for every output. Source code in deepymod/model/deepmod.py def forward ( self , thetas : TensorList , time_derivs : TensorList ) -> TensorList : \"\"\"This function nomalized the library and time derivatives and calculates the corresponding sparisity mask. Args: thetas (TensorList): List of library functions, one for every output. time_derivs (TensorList): List of time derivates of the data, one for every output. Returns: TensorList: A list of sparsity masks, one for every output. \"\"\" # we first normalize theta and the time deriv with torch . no_grad (): normed_time_derivs = [( time_deriv / torch . norm ( time_deriv )) . detach () . cpu () . numpy () for time_deriv in time_derivs ] normed_thetas = [( theta / torch . norm ( theta , dim = 0 , keepdim = True )) . detach () . cpu () . numpy () for theta in thetas ] self . coeff_vectors = [ self . fit ( theta , time_deriv . squeeze ())[:, None ] for theta , time_deriv in zip ( normed_thetas , normed_time_derivs )] sparsity_masks = [ torch . tensor ( coeff_vector != 0.0 , dtype = torch . bool ) . squeeze () . to ( thetas [ 0 ] . device ) # move to gpu if required for coeff_vector in self . coeff_vectors ] return sparsity_masks","title":"forward()"},{"location":"api/deepmod/#deepymod.model.deepmod.Library","text":"Abstract class that calculates the library function and time derivatives. Parameters: Name Type Description Default nn PyTorch Class Module of the function approximator, typically a neural network. required","title":"Library"},{"location":"api/deepmod/#deepymod.model.deepmod.Library.forward","text":"[summary] Parameters: Name Type Description Default input Tuple[TensorList, TensorList] [description] required Returns: Type Description Tuple[TensorList, TensorList] Tuple[TensorList, TensorList]: [description] Source code in deepymod/model/deepmod.py def forward ( self , input : Tuple [ TensorList , TensorList ]) -> Tuple [ TensorList , TensorList ]: \"\"\"[summary] Args: input (torch.Tensor): [description] Returns: Tuple[TensorList, TensorList]: [description] \"\"\" time_derivs , thetas = self . library ( input ) self . norms = [( torch . norm ( time_deriv ) / torch . norm ( theta , dim = 0 , keepdim = True )) . detach () . squeeze () for time_deriv , theta in zip ( time_derivs , thetas )] return time_derivs , thetas","title":"forward()"},{"location":"api/func_approx/","text":"NN [summary] Parameters: Name Type Description Default nn [type] [description] required build_network ( self , n_in , n_hidden , n_out ) Constructs a feed-forward neural network. Parameters: Name Type Description Default n_in int Number of input features. required n_hidden List[int] Number of neurons in each layer. required n_out int Number of output features. required Returns: Type Description Sequential torch.Sequential: Pytorch module Source code in deepymod/model/func_approx.py def build_network ( self , n_in : int , n_hidden : List [ int ], n_out : int ) -> torch . nn . Sequential : \"\"\" Constructs a feed-forward neural network. Args: n_in (int): Number of input features. n_hidden (list[int]): Number of neurons in each layer. n_out (int): Number of output features. Returns: torch.Sequential: Pytorch module \"\"\" network = [] architecture = [ n_in ] + n_hidden + [ n_out ] for layer_i , layer_j in zip ( architecture , architecture [ 1 :]): network . append ( nn . Linear ( layer_i , layer_j )) network . append ( nn . Tanh ()) network . pop () # get rid of last activation function return nn . Sequential ( * network ) forward ( self , input ) [summary] Parameters: Name Type Description Default input Tensor [description] required Returns: Type Description Tensor torch.Tensor: [description] Source code in deepymod/model/func_approx.py def forward ( self , input : torch . Tensor ) -> torch . Tensor : \"\"\"[summary] Args: input (torch.Tensor): [description] Returns: torch.Tensor: [description] \"\"\" coordinates = input . clone () . detach () . requires_grad_ ( True ) return self . network ( coordinates ), coordinates SineLayer [summary] Parameters: Name Type Description Default nn [type] [description] required forward ( self , input ) [summary] Parameters: Name Type Description Default input [type] [description] required Returns: Type Description [type] [description] Source code in deepymod/model/func_approx.py def forward ( self , input ): \"\"\"[summary] Args: input ([type]): [description] Returns: [type]: [description] \"\"\" return torch . sin ( self . omega_0 * self . linear ( input )) init_weights ( self ) [summary] Source code in deepymod/model/func_approx.py def init_weights ( self ): \"\"\"[summary] \"\"\" with torch . no_grad (): if self . is_first : self . linear . weight . uniform_ ( - 1 / self . in_features , 1 / self . in_features ) else : self . linear . weight . uniform_ ( - np . sqrt ( 6 / self . in_features ) / self . omega_0 , np . sqrt ( 6 / self . in_features ) / self . omega_0 ) Siren [summary] Parameters: Name Type Description Default nn [type] [description] required build_network ( self , n_in , n_hidden , n_out , first_omega_0 , hidden_omega_0 ) [summary] Parameters: Name Type Description Default n_in int [description] required n_hidden List[int] [description] required n_out int [description] required first_omega_0 float [description] required hidden_omega_0 float [description] required Returns: Type Description [type] [description] Source code in deepymod/model/func_approx.py def build_network ( self , n_in : int , n_hidden : List [ int ], n_out : int , first_omega_0 : float , hidden_omega_0 : float ): \"\"\"[summary] Args: n_in (int): [description] n_hidden (List[int]): [description] n_out (int): [description] first_omega_0 (float): [description] hidden_omega_0 (float): [description] Returns: [type]: [description] \"\"\" network = [] # Input layer network . append ( SineLayer ( n_in , n_hidden [ 0 ], is_first = True , omega_0 = first_omega_0 )) # Hidden layers for layer_i , layer_j in zip ( n_hidden , n_hidden [ 1 :]): network . append ( SineLayer ( layer_i , layer_j , is_first = False , omega_0 = hidden_omega_0 )) # Output layer final_linear = nn . Linear ( n_hidden [ - 1 ], n_out ) with torch . no_grad (): final_linear . weight . uniform_ ( - np . sqrt ( 6 / n_hidden [ - 1 ]) / hidden_omega_0 , np . sqrt ( 6 / n_hidden [ - 1 ]) / hidden_omega_0 ) network . append ( final_linear ) return nn . Sequential ( * network ) forward ( self , input ) [summary] Parameters: Name Type Description Default input Tensor [description] required Returns: Type Description [type] [description] Source code in deepymod/model/func_approx.py def forward ( self , input : torch . Tensor ): \"\"\"[summary] Args: input (torch.Tensor): [description] Returns: [type]: [description] \"\"\" coordinates = input . clone () . detach () . requires_grad_ ( True ) return self . network ( coordinates ), coordinates","title":"Function Approximators"},{"location":"api/func_approx/#deepymod.model.func_approx","text":"","title":"deepymod.model.func_approx"},{"location":"api/func_approx/#deepymod.model.func_approx.NN","text":"[summary] Parameters: Name Type Description Default nn [type] [description] required","title":"NN"},{"location":"api/func_approx/#deepymod.model.func_approx.NN.build_network","text":"Constructs a feed-forward neural network. Parameters: Name Type Description Default n_in int Number of input features. required n_hidden List[int] Number of neurons in each layer. required n_out int Number of output features. required Returns: Type Description Sequential torch.Sequential: Pytorch module Source code in deepymod/model/func_approx.py def build_network ( self , n_in : int , n_hidden : List [ int ], n_out : int ) -> torch . nn . Sequential : \"\"\" Constructs a feed-forward neural network. Args: n_in (int): Number of input features. n_hidden (list[int]): Number of neurons in each layer. n_out (int): Number of output features. Returns: torch.Sequential: Pytorch module \"\"\" network = [] architecture = [ n_in ] + n_hidden + [ n_out ] for layer_i , layer_j in zip ( architecture , architecture [ 1 :]): network . append ( nn . Linear ( layer_i , layer_j )) network . append ( nn . Tanh ()) network . pop () # get rid of last activation function return nn . Sequential ( * network )","title":"build_network()"},{"location":"api/func_approx/#deepymod.model.func_approx.NN.forward","text":"[summary] Parameters: Name Type Description Default input Tensor [description] required Returns: Type Description Tensor torch.Tensor: [description] Source code in deepymod/model/func_approx.py def forward ( self , input : torch . Tensor ) -> torch . Tensor : \"\"\"[summary] Args: input (torch.Tensor): [description] Returns: torch.Tensor: [description] \"\"\" coordinates = input . clone () . detach () . requires_grad_ ( True ) return self . network ( coordinates ), coordinates","title":"forward()"},{"location":"api/func_approx/#deepymod.model.func_approx.SineLayer","text":"[summary] Parameters: Name Type Description Default nn [type] [description] required","title":"SineLayer"},{"location":"api/func_approx/#deepymod.model.func_approx.SineLayer.forward","text":"[summary] Parameters: Name Type Description Default input [type] [description] required Returns: Type Description [type] [description] Source code in deepymod/model/func_approx.py def forward ( self , input ): \"\"\"[summary] Args: input ([type]): [description] Returns: [type]: [description] \"\"\" return torch . sin ( self . omega_0 * self . linear ( input ))","title":"forward()"},{"location":"api/func_approx/#deepymod.model.func_approx.SineLayer.init_weights","text":"[summary] Source code in deepymod/model/func_approx.py def init_weights ( self ): \"\"\"[summary] \"\"\" with torch . no_grad (): if self . is_first : self . linear . weight . uniform_ ( - 1 / self . in_features , 1 / self . in_features ) else : self . linear . weight . uniform_ ( - np . sqrt ( 6 / self . in_features ) / self . omega_0 , np . sqrt ( 6 / self . in_features ) / self . omega_0 )","title":"init_weights()"},{"location":"api/func_approx/#deepymod.model.func_approx.Siren","text":"[summary] Parameters: Name Type Description Default nn [type] [description] required","title":"Siren"},{"location":"api/func_approx/#deepymod.model.func_approx.Siren.build_network","text":"[summary] Parameters: Name Type Description Default n_in int [description] required n_hidden List[int] [description] required n_out int [description] required first_omega_0 float [description] required hidden_omega_0 float [description] required Returns: Type Description [type] [description] Source code in deepymod/model/func_approx.py def build_network ( self , n_in : int , n_hidden : List [ int ], n_out : int , first_omega_0 : float , hidden_omega_0 : float ): \"\"\"[summary] Args: n_in (int): [description] n_hidden (List[int]): [description] n_out (int): [description] first_omega_0 (float): [description] hidden_omega_0 (float): [description] Returns: [type]: [description] \"\"\" network = [] # Input layer network . append ( SineLayer ( n_in , n_hidden [ 0 ], is_first = True , omega_0 = first_omega_0 )) # Hidden layers for layer_i , layer_j in zip ( n_hidden , n_hidden [ 1 :]): network . append ( SineLayer ( layer_i , layer_j , is_first = False , omega_0 = hidden_omega_0 )) # Output layer final_linear = nn . Linear ( n_hidden [ - 1 ], n_out ) with torch . no_grad (): final_linear . weight . uniform_ ( - np . sqrt ( 6 / n_hidden [ - 1 ]) / hidden_omega_0 , np . sqrt ( 6 / n_hidden [ - 1 ]) / hidden_omega_0 ) network . append ( final_linear ) return nn . Sequential ( * network )","title":"build_network()"},{"location":"api/func_approx/#deepymod.model.func_approx.Siren.forward","text":"[summary] Parameters: Name Type Description Default input Tensor [description] required Returns: Type Description [type] [description] Source code in deepymod/model/func_approx.py def forward ( self , input : torch . Tensor ): \"\"\"[summary] Args: input (torch.Tensor): [description] Returns: [type]: [description] \"\"\" coordinates = input . clone () . detach () . requires_grad_ ( True ) return self . network ( coordinates ), coordinates","title":"forward()"},{"location":"api/lib/","text":"Library1D [summary] Parameters: Name Type Description Default Library [type] [description] required library ( self , input ) [summary] Parameters: Name Type Description Default input Tuple[torch.Tensor, torch.Tensor] [description] required Returns: Type Description Tuple[TensorList, TensorList] Tuple[TensorList, TensorList]: [description] Source code in deepymod/model/library.py def library ( self , input : Tuple [ torch . Tensor , torch . Tensor ]) -> Tuple [ TensorList , TensorList ]: \"\"\"[summary] Args: input (Tuple[torch.Tensor, torch.Tensor]): [description] Returns: Tuple[TensorList, TensorList]: [description] \"\"\" prediction , data = input poly_list = [] deriv_list = [] time_deriv_list = [] # Creating lists for all outputs for output in np . arange ( prediction . shape [ 1 ]): time_deriv , du = library_deriv ( data , prediction [:, output : output + 1 ], self . diff_order ) u = library_poly ( prediction [:, output : output + 1 ], self . poly_order ) poly_list . append ( u ) deriv_list . append ( du ) time_deriv_list . append ( time_deriv ) samples = time_deriv_list [ 0 ] . shape [ 0 ] total_terms = poly_list [ 0 ] . shape [ 1 ] * deriv_list [ 0 ] . shape [ 1 ] # Calculating theta if len ( poly_list ) == 1 : # If we have a single output, we simply calculate and flatten matrix product # between polynomials and derivatives to get library theta = torch . matmul ( poly_list [ 0 ][:, :, None ], deriv_list [ 0 ][:, None , :]) . view ( samples , total_terms ) else : theta_uv = reduce (( lambda x , y : ( x [:, :, None ] @ y [:, None , :]) . view ( samples , - 1 )), poly_list ) # calculate all unique combinations of derivatives theta_dudv = torch . cat ([ torch . matmul ( du [:, :, None ], dv [:, None , :]) . view ( samples , - 1 )[:, 1 :] for du , dv in combinations ( deriv_list , 2 )], 1 ) theta = torch . cat ([ theta_uv , theta_dudv ], dim = 1 ) return time_deriv_list , [ theta ] Library2D [summary] Parameters: Name Type Description Default Library [type] [description] required library ( self , input ) [summary] Parameters: Name Type Description Default input Tuple[torch.Tensor, torch.Tensor] [description] required Returns: Type Description Tuple[TensorList, TensorList] Tuple[TensorList, TensorList]: [description] Source code in deepymod/model/library.py def library ( self , input : Tuple [ torch . Tensor , torch . Tensor ]) -> Tuple [ TensorList , TensorList ]: \"\"\"[summary] Args: input (Tuple[torch.Tensor, torch.Tensor]): [description] Returns: Tuple[TensorList, TensorList]: [description] \"\"\" prediction , data = input # Polynomial u = torch . ones_like ( prediction ) for order in np . arange ( 1 , self . poly_order + 1 ): u = torch . cat (( u , u [:, order - 1 : order ] * prediction ), dim = 1 ) # Gradients du = grad ( prediction , data , grad_outputs = torch . ones_like ( prediction ), create_graph = True )[ 0 ] u_t = du [:, 0 : 1 ] u_x = du [:, 1 : 2 ] u_y = du [:, 2 : 3 ] du2 = grad ( u_x , data , grad_outputs = torch . ones_like ( prediction ), create_graph = True )[ 0 ] u_xx = du2 [:, 1 : 2 ] u_xy = du2 [:, 2 : 3 ] u_yy = grad ( u_y , data , grad_outputs = torch . ones_like ( prediction ), create_graph = True )[ 0 ][:, 2 : 3 ] du = torch . cat (( torch . ones_like ( u_x ), u_x , u_y , u_xx , u_yy , u_xy ), dim = 1 ) samples = du . shape [ 0 ] # Bringing it together theta = torch . matmul ( u [:, :, None ], du [:, None , :]) . view ( samples , - 1 ) return [ u_t ], [ theta ] library_deriv ( data , prediction , max_order ) [summary] Parameters: Name Type Description Default data Tensor [description] required prediction Tensor [description] required max_order int [description] required Returns: Type Description Tuple[torch.Tensor, torch.Tensor] Tuple[torch.Tensor, torch.Tensor]: [description] Source code in deepymod/model/library.py def library_deriv ( data : torch . Tensor , prediction : torch . Tensor , max_order : int ) -> Tuple [ torch . Tensor , torch . Tensor ]: \"\"\"[summary] Args: data (torch.Tensor): [description] prediction (torch.Tensor): [description] max_order (int): [description] Returns: Tuple[torch.Tensor, torch.Tensor]: [description] \"\"\" dy = grad ( prediction , data , grad_outputs = torch . ones_like ( prediction ), create_graph = True )[ 0 ] time_deriv = dy [:, 0 : 1 ] if max_order == 0 : du = torch . ones_like ( time_deriv ) else : du = torch . cat (( torch . ones_like ( time_deriv ), dy [:, 1 : 2 ]), dim = 1 ) if max_order > 1 : for order in np . arange ( 1 , max_order ): du = torch . cat (( du , grad ( du [:, order : order + 1 ], data , grad_outputs = torch . ones_like ( prediction ), create_graph = True )[ 0 ][:, 1 : 2 ]), dim = 1 ) return time_deriv , du library_poly ( prediction , max_order ) [summary] Parameters: Name Type Description Default prediction Tensor [description] required max_order int [description] required Returns: Type Description Tensor torch.Tensor: [description] Source code in deepymod/model/library.py def library_poly ( prediction : torch . Tensor , max_order : int ) -> torch . Tensor : \"\"\"[summary] Args: prediction (torch.Tensor): [description] max_order (int): [description] Returns: torch.Tensor: [description] \"\"\" u = torch . ones_like ( prediction ) for order in np . arange ( 1 , max_order + 1 ): u = torch . cat (( u , u [:, order - 1 : order ] * prediction ), dim = 1 ) return u","title":"Libraries"},{"location":"api/lib/#deepymod.model.library","text":"","title":"deepymod.model.library"},{"location":"api/lib/#deepymod.model.library.Library1D","text":"[summary] Parameters: Name Type Description Default Library [type] [description] required","title":"Library1D"},{"location":"api/lib/#deepymod.model.library.Library1D.library","text":"[summary] Parameters: Name Type Description Default input Tuple[torch.Tensor, torch.Tensor] [description] required Returns: Type Description Tuple[TensorList, TensorList] Tuple[TensorList, TensorList]: [description] Source code in deepymod/model/library.py def library ( self , input : Tuple [ torch . Tensor , torch . Tensor ]) -> Tuple [ TensorList , TensorList ]: \"\"\"[summary] Args: input (Tuple[torch.Tensor, torch.Tensor]): [description] Returns: Tuple[TensorList, TensorList]: [description] \"\"\" prediction , data = input poly_list = [] deriv_list = [] time_deriv_list = [] # Creating lists for all outputs for output in np . arange ( prediction . shape [ 1 ]): time_deriv , du = library_deriv ( data , prediction [:, output : output + 1 ], self . diff_order ) u = library_poly ( prediction [:, output : output + 1 ], self . poly_order ) poly_list . append ( u ) deriv_list . append ( du ) time_deriv_list . append ( time_deriv ) samples = time_deriv_list [ 0 ] . shape [ 0 ] total_terms = poly_list [ 0 ] . shape [ 1 ] * deriv_list [ 0 ] . shape [ 1 ] # Calculating theta if len ( poly_list ) == 1 : # If we have a single output, we simply calculate and flatten matrix product # between polynomials and derivatives to get library theta = torch . matmul ( poly_list [ 0 ][:, :, None ], deriv_list [ 0 ][:, None , :]) . view ( samples , total_terms ) else : theta_uv = reduce (( lambda x , y : ( x [:, :, None ] @ y [:, None , :]) . view ( samples , - 1 )), poly_list ) # calculate all unique combinations of derivatives theta_dudv = torch . cat ([ torch . matmul ( du [:, :, None ], dv [:, None , :]) . view ( samples , - 1 )[:, 1 :] for du , dv in combinations ( deriv_list , 2 )], 1 ) theta = torch . cat ([ theta_uv , theta_dudv ], dim = 1 ) return time_deriv_list , [ theta ]","title":"library()"},{"location":"api/lib/#deepymod.model.library.Library2D","text":"[summary] Parameters: Name Type Description Default Library [type] [description] required","title":"Library2D"},{"location":"api/lib/#deepymod.model.library.Library2D.library","text":"[summary] Parameters: Name Type Description Default input Tuple[torch.Tensor, torch.Tensor] [description] required Returns: Type Description Tuple[TensorList, TensorList] Tuple[TensorList, TensorList]: [description] Source code in deepymod/model/library.py def library ( self , input : Tuple [ torch . Tensor , torch . Tensor ]) -> Tuple [ TensorList , TensorList ]: \"\"\"[summary] Args: input (Tuple[torch.Tensor, torch.Tensor]): [description] Returns: Tuple[TensorList, TensorList]: [description] \"\"\" prediction , data = input # Polynomial u = torch . ones_like ( prediction ) for order in np . arange ( 1 , self . poly_order + 1 ): u = torch . cat (( u , u [:, order - 1 : order ] * prediction ), dim = 1 ) # Gradients du = grad ( prediction , data , grad_outputs = torch . ones_like ( prediction ), create_graph = True )[ 0 ] u_t = du [:, 0 : 1 ] u_x = du [:, 1 : 2 ] u_y = du [:, 2 : 3 ] du2 = grad ( u_x , data , grad_outputs = torch . ones_like ( prediction ), create_graph = True )[ 0 ] u_xx = du2 [:, 1 : 2 ] u_xy = du2 [:, 2 : 3 ] u_yy = grad ( u_y , data , grad_outputs = torch . ones_like ( prediction ), create_graph = True )[ 0 ][:, 2 : 3 ] du = torch . cat (( torch . ones_like ( u_x ), u_x , u_y , u_xx , u_yy , u_xy ), dim = 1 ) samples = du . shape [ 0 ] # Bringing it together theta = torch . matmul ( u [:, :, None ], du [:, None , :]) . view ( samples , - 1 ) return [ u_t ], [ theta ]","title":"library()"},{"location":"api/lib/#deepymod.model.library.library_deriv","text":"[summary] Parameters: Name Type Description Default data Tensor [description] required prediction Tensor [description] required max_order int [description] required Returns: Type Description Tuple[torch.Tensor, torch.Tensor] Tuple[torch.Tensor, torch.Tensor]: [description] Source code in deepymod/model/library.py def library_deriv ( data : torch . Tensor , prediction : torch . Tensor , max_order : int ) -> Tuple [ torch . Tensor , torch . Tensor ]: \"\"\"[summary] Args: data (torch.Tensor): [description] prediction (torch.Tensor): [description] max_order (int): [description] Returns: Tuple[torch.Tensor, torch.Tensor]: [description] \"\"\" dy = grad ( prediction , data , grad_outputs = torch . ones_like ( prediction ), create_graph = True )[ 0 ] time_deriv = dy [:, 0 : 1 ] if max_order == 0 : du = torch . ones_like ( time_deriv ) else : du = torch . cat (( torch . ones_like ( time_deriv ), dy [:, 1 : 2 ]), dim = 1 ) if max_order > 1 : for order in np . arange ( 1 , max_order ): du = torch . cat (( du , grad ( du [:, order : order + 1 ], data , grad_outputs = torch . ones_like ( prediction ), create_graph = True )[ 0 ][:, 1 : 2 ]), dim = 1 ) return time_deriv , du","title":"library_deriv()"},{"location":"api/lib/#deepymod.model.library.library_poly","text":"[summary] Parameters: Name Type Description Default prediction Tensor [description] required max_order int [description] required Returns: Type Description Tensor torch.Tensor: [description] Source code in deepymod/model/library.py def library_poly ( prediction : torch . Tensor , max_order : int ) -> torch . Tensor : \"\"\"[summary] Args: prediction (torch.Tensor): [description] max_order (int): [description] Returns: torch.Tensor: [description] \"\"\" u = torch . ones_like ( prediction ) for order in np . arange ( 1 , max_order + 1 ): u = torch . cat (( u , u [:, order - 1 : order ] * prediction ), dim = 1 ) return u","title":"library_poly()"},{"location":"api/spars_sched/","text":"Periodic Controls when to apply sparsity. Initial_epoch is first time of appliance, then every periodicity epochs. __call__ ( self , iteration , l1_norm ) special [summary] Parameters: Name Type Description Default iteration int [description] required l1_norm Tensor [description] required Source code in deepymod/training/sparsity_scheduler.py def __call__ ( self , iteration : int , l1_norm : torch . Tensor ) -> None : \"\"\"[summary] Args: iteration (int): [description] l1_norm (torch.Tensor): [description] \"\"\" if iteration >= self . initial_epoch : if ( iteration - self . initial_epoch ) % self . periodicity == 0 : self . apply_sparsity = True reset ( self ) [summary] Source code in deepymod/training/sparsity_scheduler.py def reset ( self ) -> None : \"\"\"[summary] \"\"\" self . apply_sparsity = False TrainTest Early stops the training if validation loss doesn't improve after a given patience. __init__ ( self , patience = 7 , verbose = False , delta = 0 , path = 'checkpoint.pt' , trace_func =< built - in function print > ) special TO DO: LOAD MODEL AND OPTIMIZER LIKE TRAINTESTPERIODIC Parameters: Name Type Description Default patience int How long to wait after last time validation loss improved. Default: 7 7 verbose bool If True, prints a message for each validation loss improvement. Default: False False delta float Minimum change in the monitored quantity to qualify as an improvement. Default: 0 0 path str Path for the checkpoint to be saved to. Default: 'checkpoint.pt' 'checkpoint.pt' trace_func function trace print function. Default: print <built-in function print> Source code in deepymod/training/sparsity_scheduler.py def __init__ ( self , patience = 7 , verbose = False , delta = 0 , path = 'checkpoint.pt' , trace_func = print ): \"\"\"TO DO: LOAD MODEL AND OPTIMIZER LIKE TRAINTESTPERIODIC Args: patience (int): How long to wait after last time validation loss improved. Default: 7 verbose (bool): If True, prints a message for each validation loss improvement. Default: False delta (float): Minimum change in the monitored quantity to qualify as an improvement. Default: 0 path (str): Path for the checkpoint to be saved to. Default: 'checkpoint.pt' trace_func (function): trace print function. Default: print \"\"\" self . patience = patience self . verbose = verbose self . best_iteration = 0 self . best_score = None self . apply_sparsity = False self . val_loss_min = np . Inf self . delta = delta self . path = path self . trace_func = trace_func reset ( self ) [summary] Source code in deepymod/training/sparsity_scheduler.py def reset ( self ) -> None : \"\"\"[summary] \"\"\" self . best_iteration = 0 self . best_score = None self . apply_sparsity = False self . val_loss_min = np . Inf save_checkpoint ( self , model , optimizer ) Saves model when validation loss decrease. Source code in deepymod/training/sparsity_scheduler.py def save_checkpoint ( self , model , optimizer ): '''Saves model when validation loss decrease.''' checkpoint_path = self . path + 'checkpoint.pt' torch . save ({ 'model_state_dict' : model . state_dict (), 'optimizer_state_dict' : optimizer . state_dict ()}, checkpoint_path ) TrainTestPeriodic Early stops the training if validation loss doesn't improve after a given patience. reset ( self ) [summary] Source code in deepymod/training/sparsity_scheduler.py def reset ( self ) -> None : \"\"\"[summary] \"\"\" self . best_iteration = 0 self . best_score = None self . apply_sparsity = False self . val_loss_min = np . Inf save_checkpoint ( self , model , optimizer ) Saves model when validation loss decrease. Source code in deepymod/training/sparsity_scheduler.py def save_checkpoint ( self , model , optimizer ): '''Saves model when validation loss decrease.''' checkpoint_path = self . path + 'checkpoint.pt' torch . save ({ 'model_state_dict' : model . state_dict (), 'optimizer_state_dict' : optimizer . state_dict (),}, checkpoint_path )","title":"Sparsity Scheduler"},{"location":"api/spars_sched/#deepymod.training.sparsity_scheduler","text":"","title":"deepymod.training.sparsity_scheduler"},{"location":"api/spars_sched/#deepymod.training.sparsity_scheduler.Periodic","text":"Controls when to apply sparsity. Initial_epoch is first time of appliance, then every periodicity epochs.","title":"Periodic"},{"location":"api/spars_sched/#deepymod.training.sparsity_scheduler.Periodic.__call__","text":"[summary] Parameters: Name Type Description Default iteration int [description] required l1_norm Tensor [description] required Source code in deepymod/training/sparsity_scheduler.py def __call__ ( self , iteration : int , l1_norm : torch . Tensor ) -> None : \"\"\"[summary] Args: iteration (int): [description] l1_norm (torch.Tensor): [description] \"\"\" if iteration >= self . initial_epoch : if ( iteration - self . initial_epoch ) % self . periodicity == 0 : self . apply_sparsity = True","title":"__call__()"},{"location":"api/spars_sched/#deepymod.training.sparsity_scheduler.Periodic.reset","text":"[summary] Source code in deepymod/training/sparsity_scheduler.py def reset ( self ) -> None : \"\"\"[summary] \"\"\" self . apply_sparsity = False","title":"reset()"},{"location":"api/spars_sched/#deepymod.training.sparsity_scheduler.TrainTest","text":"Early stops the training if validation loss doesn't improve after a given patience.","title":"TrainTest"},{"location":"api/spars_sched/#deepymod.training.sparsity_scheduler.TrainTest.__init__","text":"TO DO: LOAD MODEL AND OPTIMIZER LIKE TRAINTESTPERIODIC Parameters: Name Type Description Default patience int How long to wait after last time validation loss improved. Default: 7 7 verbose bool If True, prints a message for each validation loss improvement. Default: False False delta float Minimum change in the monitored quantity to qualify as an improvement. Default: 0 0 path str Path for the checkpoint to be saved to. Default: 'checkpoint.pt' 'checkpoint.pt' trace_func function trace print function. Default: print <built-in function print> Source code in deepymod/training/sparsity_scheduler.py def __init__ ( self , patience = 7 , verbose = False , delta = 0 , path = 'checkpoint.pt' , trace_func = print ): \"\"\"TO DO: LOAD MODEL AND OPTIMIZER LIKE TRAINTESTPERIODIC Args: patience (int): How long to wait after last time validation loss improved. Default: 7 verbose (bool): If True, prints a message for each validation loss improvement. Default: False delta (float): Minimum change in the monitored quantity to qualify as an improvement. Default: 0 path (str): Path for the checkpoint to be saved to. Default: 'checkpoint.pt' trace_func (function): trace print function. Default: print \"\"\" self . patience = patience self . verbose = verbose self . best_iteration = 0 self . best_score = None self . apply_sparsity = False self . val_loss_min = np . Inf self . delta = delta self . path = path self . trace_func = trace_func","title":"__init__()"},{"location":"api/spars_sched/#deepymod.training.sparsity_scheduler.TrainTest.reset","text":"[summary] Source code in deepymod/training/sparsity_scheduler.py def reset ( self ) -> None : \"\"\"[summary] \"\"\" self . best_iteration = 0 self . best_score = None self . apply_sparsity = False self . val_loss_min = np . Inf","title":"reset()"},{"location":"api/spars_sched/#deepymod.training.sparsity_scheduler.TrainTest.save_checkpoint","text":"Saves model when validation loss decrease. Source code in deepymod/training/sparsity_scheduler.py def save_checkpoint ( self , model , optimizer ): '''Saves model when validation loss decrease.''' checkpoint_path = self . path + 'checkpoint.pt' torch . save ({ 'model_state_dict' : model . state_dict (), 'optimizer_state_dict' : optimizer . state_dict ()}, checkpoint_path )","title":"save_checkpoint()"},{"location":"api/spars_sched/#deepymod.training.sparsity_scheduler.TrainTestPeriodic","text":"Early stops the training if validation loss doesn't improve after a given patience.","title":"TrainTestPeriodic"},{"location":"api/spars_sched/#deepymod.training.sparsity_scheduler.TrainTestPeriodic.reset","text":"[summary] Source code in deepymod/training/sparsity_scheduler.py def reset ( self ) -> None : \"\"\"[summary] \"\"\" self . best_iteration = 0 self . best_score = None self . apply_sparsity = False self . val_loss_min = np . Inf","title":"reset()"},{"location":"api/spars_sched/#deepymod.training.sparsity_scheduler.TrainTestPeriodic.save_checkpoint","text":"Saves model when validation loss decrease. Source code in deepymod/training/sparsity_scheduler.py def save_checkpoint ( self , model , optimizer ): '''Saves model when validation loss decrease.''' checkpoint_path = self . path + 'checkpoint.pt' torch . save ({ 'model_state_dict' : model . state_dict (), 'optimizer_state_dict' : optimizer . state_dict (),}, checkpoint_path )","title":"save_checkpoint()"},{"location":"api/sparse/","text":"Periodic Controls when to apply sparsity. Initial_epoch is first time of appliance, then every periodicity epochs. __call__ ( self , iteration , l1_norm ) special [summary] Parameters: Name Type Description Default iteration int [description] required l1_norm Tensor [description] required Source code in deepymod/training/sparsity_scheduler.py def __call__ ( self , iteration : int , l1_norm : torch . Tensor ) -> None : \"\"\"[summary] Args: iteration (int): [description] l1_norm (torch.Tensor): [description] \"\"\" if iteration >= self . initial_epoch : if ( iteration - self . initial_epoch ) % self . periodicity == 0 : self . apply_sparsity = True reset ( self ) [summary] Source code in deepymod/training/sparsity_scheduler.py def reset ( self ) -> None : \"\"\"[summary] \"\"\" self . apply_sparsity = False TrainTest Early stops the training if validation loss doesn't improve after a given patience. __init__ ( self , patience = 7 , verbose = False , delta = 0 , path = 'checkpoint.pt' , trace_func =< built - in function print > ) special TO DO: LOAD MODEL AND OPTIMIZER LIKE TRAINTESTPERIODIC Parameters: Name Type Description Default patience int How long to wait after last time validation loss improved. Default: 7 7 verbose bool If True, prints a message for each validation loss improvement. Default: False False delta float Minimum change in the monitored quantity to qualify as an improvement. Default: 0 0 path str Path for the checkpoint to be saved to. Default: 'checkpoint.pt' 'checkpoint.pt' trace_func function trace print function. Default: print <built-in function print> Source code in deepymod/training/sparsity_scheduler.py def __init__ ( self , patience = 7 , verbose = False , delta = 0 , path = 'checkpoint.pt' , trace_func = print ): \"\"\"TO DO: LOAD MODEL AND OPTIMIZER LIKE TRAINTESTPERIODIC Args: patience (int): How long to wait after last time validation loss improved. Default: 7 verbose (bool): If True, prints a message for each validation loss improvement. Default: False delta (float): Minimum change in the monitored quantity to qualify as an improvement. Default: 0 path (str): Path for the checkpoint to be saved to. Default: 'checkpoint.pt' trace_func (function): trace print function. Default: print \"\"\" self . patience = patience self . verbose = verbose self . best_iteration = 0 self . best_score = None self . apply_sparsity = False self . val_loss_min = np . Inf self . delta = delta self . path = path self . trace_func = trace_func reset ( self ) [summary] Source code in deepymod/training/sparsity_scheduler.py def reset ( self ) -> None : \"\"\"[summary] \"\"\" self . best_iteration = 0 self . best_score = None self . apply_sparsity = False self . val_loss_min = np . Inf save_checkpoint ( self , model , optimizer ) Saves model when validation loss decrease. Source code in deepymod/training/sparsity_scheduler.py def save_checkpoint ( self , model , optimizer ): '''Saves model when validation loss decrease.''' checkpoint_path = self . path + 'checkpoint.pt' torch . save ({ 'model_state_dict' : model . state_dict (), 'optimizer_state_dict' : optimizer . state_dict ()}, checkpoint_path ) TrainTestPeriodic Early stops the training if validation loss doesn't improve after a given patience. reset ( self ) [summary] Source code in deepymod/training/sparsity_scheduler.py def reset ( self ) -> None : \"\"\"[summary] \"\"\" self . best_iteration = 0 self . best_score = None self . apply_sparsity = False self . val_loss_min = np . Inf save_checkpoint ( self , model , optimizer ) Saves model when validation loss decrease. Source code in deepymod/training/sparsity_scheduler.py def save_checkpoint ( self , model , optimizer ): '''Saves model when validation loss decrease.''' checkpoint_path = self . path + 'checkpoint.pt' torch . save ({ 'model_state_dict' : model . state_dict (), 'optimizer_state_dict' : optimizer . state_dict (),}, checkpoint_path )","title":"Sparsity"},{"location":"api/sparse/#deepymod.training.sparsity_scheduler","text":"","title":"deepymod.training.sparsity_scheduler"},{"location":"api/sparse/#deepymod.training.sparsity_scheduler.Periodic","text":"Controls when to apply sparsity. Initial_epoch is first time of appliance, then every periodicity epochs.","title":"Periodic"},{"location":"api/sparse/#deepymod.training.sparsity_scheduler.Periodic.__call__","text":"[summary] Parameters: Name Type Description Default iteration int [description] required l1_norm Tensor [description] required Source code in deepymod/training/sparsity_scheduler.py def __call__ ( self , iteration : int , l1_norm : torch . Tensor ) -> None : \"\"\"[summary] Args: iteration (int): [description] l1_norm (torch.Tensor): [description] \"\"\" if iteration >= self . initial_epoch : if ( iteration - self . initial_epoch ) % self . periodicity == 0 : self . apply_sparsity = True","title":"__call__()"},{"location":"api/sparse/#deepymod.training.sparsity_scheduler.Periodic.reset","text":"[summary] Source code in deepymod/training/sparsity_scheduler.py def reset ( self ) -> None : \"\"\"[summary] \"\"\" self . apply_sparsity = False","title":"reset()"},{"location":"api/sparse/#deepymod.training.sparsity_scheduler.TrainTest","text":"Early stops the training if validation loss doesn't improve after a given patience.","title":"TrainTest"},{"location":"api/sparse/#deepymod.training.sparsity_scheduler.TrainTest.__init__","text":"TO DO: LOAD MODEL AND OPTIMIZER LIKE TRAINTESTPERIODIC Parameters: Name Type Description Default patience int How long to wait after last time validation loss improved. Default: 7 7 verbose bool If True, prints a message for each validation loss improvement. Default: False False delta float Minimum change in the monitored quantity to qualify as an improvement. Default: 0 0 path str Path for the checkpoint to be saved to. Default: 'checkpoint.pt' 'checkpoint.pt' trace_func function trace print function. Default: print <built-in function print> Source code in deepymod/training/sparsity_scheduler.py def __init__ ( self , patience = 7 , verbose = False , delta = 0 , path = 'checkpoint.pt' , trace_func = print ): \"\"\"TO DO: LOAD MODEL AND OPTIMIZER LIKE TRAINTESTPERIODIC Args: patience (int): How long to wait after last time validation loss improved. Default: 7 verbose (bool): If True, prints a message for each validation loss improvement. Default: False delta (float): Minimum change in the monitored quantity to qualify as an improvement. Default: 0 path (str): Path for the checkpoint to be saved to. Default: 'checkpoint.pt' trace_func (function): trace print function. Default: print \"\"\" self . patience = patience self . verbose = verbose self . best_iteration = 0 self . best_score = None self . apply_sparsity = False self . val_loss_min = np . Inf self . delta = delta self . path = path self . trace_func = trace_func","title":"__init__()"},{"location":"api/sparse/#deepymod.training.sparsity_scheduler.TrainTest.reset","text":"[summary] Source code in deepymod/training/sparsity_scheduler.py def reset ( self ) -> None : \"\"\"[summary] \"\"\" self . best_iteration = 0 self . best_score = None self . apply_sparsity = False self . val_loss_min = np . Inf","title":"reset()"},{"location":"api/sparse/#deepymod.training.sparsity_scheduler.TrainTest.save_checkpoint","text":"Saves model when validation loss decrease. Source code in deepymod/training/sparsity_scheduler.py def save_checkpoint ( self , model , optimizer ): '''Saves model when validation loss decrease.''' checkpoint_path = self . path + 'checkpoint.pt' torch . save ({ 'model_state_dict' : model . state_dict (), 'optimizer_state_dict' : optimizer . state_dict ()}, checkpoint_path )","title":"save_checkpoint()"},{"location":"api/sparse/#deepymod.training.sparsity_scheduler.TrainTestPeriodic","text":"Early stops the training if validation loss doesn't improve after a given patience.","title":"TrainTestPeriodic"},{"location":"api/sparse/#deepymod.training.sparsity_scheduler.TrainTestPeriodic.reset","text":"[summary] Source code in deepymod/training/sparsity_scheduler.py def reset ( self ) -> None : \"\"\"[summary] \"\"\" self . best_iteration = 0 self . best_score = None self . apply_sparsity = False self . val_loss_min = np . Inf","title":"reset()"},{"location":"api/sparse/#deepymod.training.sparsity_scheduler.TrainTestPeriodic.save_checkpoint","text":"Saves model when validation loss decrease. Source code in deepymod/training/sparsity_scheduler.py def save_checkpoint ( self , model , optimizer ): '''Saves model when validation loss decrease.''' checkpoint_path = self . path + 'checkpoint.pt' torch . save ({ 'model_state_dict' : model . state_dict (), 'optimizer_state_dict' : optimizer . state_dict (),}, checkpoint_path )","title":"save_checkpoint()"},{"location":"api/training/","text":"train ( model , data , target , optimizer , sparsity_scheduler , test = 'mse' , split = 0.8 , log_dir = None , max_iterations = 10000 , write_iterations = 25 , ** convergence_kwargs ) Trains the DeepMoD model. This function automatically splits the data set in a train and test set. Parameters: Name Type Description Default model DeepMoD A DeepMoD object. required data Tensor Tensor of shape (n_samples x (n_spatial + 1)) containing the coordinates, first column should be the time coordinate. required target Tensor Tensor of shape (n_samples x n_features) containing the target data. required optimizer [type] Pytorch optimizer. required sparsity_scheduler [type] Decides when to update the sparsity mask. required test str Sets what to use for the test loss, by default 'mse' 'mse' split float Fraction of the train set, by default 0.8. 0.8 log_dir Optional[str] Directory where tensorboard file is written, by default None. None max_iterations int [description]. Max number of epochs , by default 10000. 10000 write_iterations int [description]. Sets how often data is written to tensorboard and checks train loss , by default 25. 25 Source code in deepymod/training/training.py def train ( model : DeepMoD , data : torch . Tensor , target : torch . Tensor , optimizer , sparsity_scheduler , test = 'mse' , split : float = 0.8 , log_dir : Optional [ str ] = None , max_iterations : int = 10000 , write_iterations : int = 25 , ** convergence_kwargs ) -> None : \"\"\"Trains the DeepMoD model. This function automatically splits the data set in a train and test set. Args: model (DeepMoD): A DeepMoD object. data (torch.Tensor): Tensor of shape (n_samples x (n_spatial + 1)) containing the coordinates, first column should be the time coordinate. target (torch.Tensor): Tensor of shape (n_samples x n_features) containing the target data. optimizer ([type]): Pytorch optimizer. sparsity_scheduler ([type]): Decides when to update the sparsity mask. test (str, optional): Sets what to use for the test loss, by default 'mse' split (float, optional): Fraction of the train set, by default 0.8. log_dir (Optional[str], optional): Directory where tensorboard file is written, by default None. max_iterations (int, optional): [description]. Max number of epochs , by default 10000. write_iterations (int, optional): [description]. Sets how often data is written to tensorboard and checks train loss , by default 25. \"\"\" logger = Logger ( log_dir ) sparsity_scheduler . path = logger . log_dir # write checkpoint to same folder as tb output. # Splitting data, assumes data is already randomized n_train = int ( split * data . shape [ 0 ]) n_test = data . shape [ 0 ] - n_train data_train , data_test = torch . split ( data , [ n_train , n_test ], dim = 0 ) target_train , target_test = torch . split ( target , [ n_train , n_test ], dim = 0 ) # Training convergence = Convergence ( ** convergence_kwargs ) for iteration in np . arange ( 0 , max_iterations + 1 ): # ================== Training Model ============================ prediction , time_derivs , thetas = model ( data_train ) MSE = torch . mean (( prediction - target_train ) ** 2 , dim = 0 ) # loss per output Reg = torch . stack ([ torch . mean (( dt - theta @ coeff_vector ) ** 2 ) for dt , theta , coeff_vector in zip ( time_derivs , thetas , model . constraint_coeffs ( scaled = False , sparse = True ))]) loss = torch . sum ( MSE + Reg ) # Optimizer step optimizer . zero_grad () loss . backward () optimizer . step () if iteration % write_iterations == 0 : # ================== Validation costs ================ prediction_test , coordinates = model . func_approx ( data_test ) time_derivs_test , thetas_test = model . library (( prediction_test , coordinates )) with torch . no_grad (): MSE_test = torch . mean (( prediction_test - target_test ) ** 2 , dim = 0 ) # loss per output Reg_test = torch . stack ([ torch . mean (( dt - theta @ coeff_vector ) ** 2 ) for dt , theta , coeff_vector in zip ( time_derivs_test , thetas_test , model . constraint_coeffs ( scaled = False , sparse = True ))]) loss_test = torch . sum ( MSE_test + Reg_test ) # ====================== Logging ======================= _ = model . sparse_estimator ( thetas , time_derivs ) # calculating l1 adjusted coeffs but not setting mask logger ( iteration , loss , MSE , Reg , model . constraint_coeffs ( sparse = True , scaled = True ), model . constraint_coeffs ( sparse = True , scaled = False ), model . estimator_coeffs (), MSE_test = MSE_test ) # ================== Sparsity update ============= # Updating sparsity and or convergence if iteration % write_iterations == 0 : if test == 'mse' : sparsity_scheduler ( iteration , torch . sum ( MSE_test ), model , optimizer ) else : sparsity_scheduler ( iteration , loss_test , model , optimizer ) if sparsity_scheduler . apply_sparsity is True : with torch . no_grad (): model . constraint . sparsity_masks = model . sparse_estimator ( thetas , time_derivs ) sparsity_scheduler . reset () # ================= Checking convergence l1_norm = torch . sum ( torch . abs ( torch . cat ( model . constraint_coeffs ( sparse = True , scaled = True ), dim = 1 ))) convergence ( iteration , l1_norm ) if convergence . converged is True : print ( 'Algorithm converged. Stopping training.' ) break logger . close ( model )","title":"Training"},{"location":"api/training/#deepymod.training.training","text":"","title":"deepymod.training.training"},{"location":"api/training/#deepymod.training.training.train","text":"Trains the DeepMoD model. This function automatically splits the data set in a train and test set. Parameters: Name Type Description Default model DeepMoD A DeepMoD object. required data Tensor Tensor of shape (n_samples x (n_spatial + 1)) containing the coordinates, first column should be the time coordinate. required target Tensor Tensor of shape (n_samples x n_features) containing the target data. required optimizer [type] Pytorch optimizer. required sparsity_scheduler [type] Decides when to update the sparsity mask. required test str Sets what to use for the test loss, by default 'mse' 'mse' split float Fraction of the train set, by default 0.8. 0.8 log_dir Optional[str] Directory where tensorboard file is written, by default None. None max_iterations int [description]. Max number of epochs , by default 10000. 10000 write_iterations int [description]. Sets how often data is written to tensorboard and checks train loss , by default 25. 25 Source code in deepymod/training/training.py def train ( model : DeepMoD , data : torch . Tensor , target : torch . Tensor , optimizer , sparsity_scheduler , test = 'mse' , split : float = 0.8 , log_dir : Optional [ str ] = None , max_iterations : int = 10000 , write_iterations : int = 25 , ** convergence_kwargs ) -> None : \"\"\"Trains the DeepMoD model. This function automatically splits the data set in a train and test set. Args: model (DeepMoD): A DeepMoD object. data (torch.Tensor): Tensor of shape (n_samples x (n_spatial + 1)) containing the coordinates, first column should be the time coordinate. target (torch.Tensor): Tensor of shape (n_samples x n_features) containing the target data. optimizer ([type]): Pytorch optimizer. sparsity_scheduler ([type]): Decides when to update the sparsity mask. test (str, optional): Sets what to use for the test loss, by default 'mse' split (float, optional): Fraction of the train set, by default 0.8. log_dir (Optional[str], optional): Directory where tensorboard file is written, by default None. max_iterations (int, optional): [description]. Max number of epochs , by default 10000. write_iterations (int, optional): [description]. Sets how often data is written to tensorboard and checks train loss , by default 25. \"\"\" logger = Logger ( log_dir ) sparsity_scheduler . path = logger . log_dir # write checkpoint to same folder as tb output. # Splitting data, assumes data is already randomized n_train = int ( split * data . shape [ 0 ]) n_test = data . shape [ 0 ] - n_train data_train , data_test = torch . split ( data , [ n_train , n_test ], dim = 0 ) target_train , target_test = torch . split ( target , [ n_train , n_test ], dim = 0 ) # Training convergence = Convergence ( ** convergence_kwargs ) for iteration in np . arange ( 0 , max_iterations + 1 ): # ================== Training Model ============================ prediction , time_derivs , thetas = model ( data_train ) MSE = torch . mean (( prediction - target_train ) ** 2 , dim = 0 ) # loss per output Reg = torch . stack ([ torch . mean (( dt - theta @ coeff_vector ) ** 2 ) for dt , theta , coeff_vector in zip ( time_derivs , thetas , model . constraint_coeffs ( scaled = False , sparse = True ))]) loss = torch . sum ( MSE + Reg ) # Optimizer step optimizer . zero_grad () loss . backward () optimizer . step () if iteration % write_iterations == 0 : # ================== Validation costs ================ prediction_test , coordinates = model . func_approx ( data_test ) time_derivs_test , thetas_test = model . library (( prediction_test , coordinates )) with torch . no_grad (): MSE_test = torch . mean (( prediction_test - target_test ) ** 2 , dim = 0 ) # loss per output Reg_test = torch . stack ([ torch . mean (( dt - theta @ coeff_vector ) ** 2 ) for dt , theta , coeff_vector in zip ( time_derivs_test , thetas_test , model . constraint_coeffs ( scaled = False , sparse = True ))]) loss_test = torch . sum ( MSE_test + Reg_test ) # ====================== Logging ======================= _ = model . sparse_estimator ( thetas , time_derivs ) # calculating l1 adjusted coeffs but not setting mask logger ( iteration , loss , MSE , Reg , model . constraint_coeffs ( sparse = True , scaled = True ), model . constraint_coeffs ( sparse = True , scaled = False ), model . estimator_coeffs (), MSE_test = MSE_test ) # ================== Sparsity update ============= # Updating sparsity and or convergence if iteration % write_iterations == 0 : if test == 'mse' : sparsity_scheduler ( iteration , torch . sum ( MSE_test ), model , optimizer ) else : sparsity_scheduler ( iteration , loss_test , model , optimizer ) if sparsity_scheduler . apply_sparsity is True : with torch . no_grad (): model . constraint . sparsity_masks = model . sparse_estimator ( thetas , time_derivs ) sparsity_scheduler . reset () # ================= Checking convergence l1_norm = torch . sum ( torch . abs ( torch . cat ( model . constraint_coeffs ( sparse = True , scaled = True ), dim = 1 ))) convergence ( iteration , l1_norm ) if convergence . converged is True : print ( 'Algorithm converged. Stopping training.' ) break logger . close ( model )","title":"train()"},{"location":"api/data/base/","text":"Dataset This class automatically generates all the necessary proporties of a predifined data set with a single spatial dimension as input. In particular it calculates the solution, the time derivative and the library. Note that all the pytorch opperations such as automatic differentiation can be used on the results. create_dataset ( self , x , t , n_samples , noise , random = True , normalize = True , return_idx = False , random_state = 42 ) Function creates the data set in the precise format used by DeepMoD Parameters: Name Type Description Default x Tensor Input vector of spatial coordinates required t Tensor Input vector of temporal coordinates required n_samples int Number of samples, set n_samples=0 for all. required noise float Noise level in percentage of std. required random bool When true, data set is randomised. Defaults to True. True normalize bool When true, data set is normalized. Defaults to True. True return_idx bool When true, the id of the data, before randomizing is returned. Defaults to False. False random_state int Seed of the randomisation. Defaults to 42. 42 Returns: Type Description [type] Tensor containing the input and output and optionally the randomisation. Source code in deepymod/data/base.py def create_dataset ( self , x , t , n_samples , noise , random = True , normalize = True , return_idx = False , random_state = 42 ): \"\"\"Function creates the data set in the precise format used by DeepMoD Args: x (Tensor): Input vector of spatial coordinates t (Tensor): Input vector of temporal coordinates n_samples (int): Number of samples, set n_samples=0 for all. noise (float): Noise level in percentage of std. random (bool, optional): When true, data set is randomised. Defaults to True. normalize (bool, optional): When true, data set is normalized. Defaults to True. return_idx (bool, optional): When true, the id of the data, before randomizing is returned. Defaults to False. random_state (int, optional): Seed of the randomisation. Defaults to 42. Returns: [type]: Tensor containing the input and output and optionally the randomisation. \"\"\" assert (( x . shape [ 1 ] == 1 ) & ( t . shape [ 1 ] == 1 )), 'x and t should have shape (n_samples x 1)' u = self . generate_solution ( x , t ) X = np . concatenate ([ t , x ], axis = 1 ) if random_state is None : y = u + noise * np . std ( u , axis = 0 ) * np . random . normal ( size = u . shape ) else : y = u + noise * np . std ( u , axis = 0 ) * np . random . RandomState ( seed = random_state ) . normal ( size = u . shape ) # creating random idx for samples N = y . shape [ 0 ] if n_samples == 0 else n_samples if random is True : if random_state is None : rand_idx = np . random . permutation ( y . shape [ 0 ])[: N ] else : rand_idx = np . random . RandomState ( seed = random_state ) . permutation ( y . shape [ 0 ])[: N ] else : rand_idx = np . arange ( y . shape [ 0 ])[: N ] # Normalizing if normalize : if ( self . scaling_factor is None ): self . scaling_factor = ( - ( np . max ( X , axis = 0 ) + np . min ( X , axis = 0 )) / 2 , ( np . max ( X , axis = 0 ) - np . min ( X , axis = 0 )) / 2 ) # only calculate the first time X = ( X + self . scaling_factor [ 0 ]) / self . scaling_factor [ 1 ] # Building dataset X_train = torch . tensor ( X [ rand_idx , :], dtype = torch . float32 ) y_train = torch . tensor ( y [ rand_idx , :], dtype = torch . float32 ) if return_idx is False : return X_train , y_train else : return X_train , y_train , rand_idx Dataset_2D This class automatically generates all the necessary proporties of a predifined data set with two spatial dimension as input. In particular it calculates the solution, the time derivative and the library. Note that all the pytorch opperations such as automatic differentiation can be used on the results. create_dataset ( self , x , t , n_samples , noise , random = True , return_idx = False , random_state = 42 ) Function creates the data set in the precise format used by DeepMoD Parameters: Name Type Description Default x Tensor Input vector of spatial coordinates required t Tensor Input vector of temporal coordinates required n_samples int Number of samples, set n_samples=0 for all. required noise float Noise level in percentage of std. required random bool When true, data set is randomised. Defaults to True. True normalize bool When true, data set is normalized. Defaults to True. required return_idx bool When true, the id of the data, before randomizing is returned. Defaults to False. False random_state int Seed of the randomisation. Defaults to 42. 42 Returns: Type Description [type] Tensor containing the input and output and optionally the randomisation. Source code in deepymod/data/base.py def create_dataset ( self , x , t , n_samples , noise , random = True , return_idx = False , random_state = 42 ): \"\"\"Function creates the data set in the precise format used by DeepMoD Args: x (Tensor): Input vector of spatial coordinates t (Tensor): Input vector of temporal coordinates n_samples (int): Number of samples, set n_samples=0 for all. noise (float): Noise level in percentage of std. random (bool, optional): When true, data set is randomised. Defaults to True. normalize (bool, optional): When true, data set is normalized. Defaults to True. return_idx (bool, optional): When true, the id of the data, before randomizing is returned. Defaults to False. random_state (int, optional): Seed of the randomisation. Defaults to 42. Returns: [type]: Tensor containing the input and output and optionally the randomisation. \"\"\" assert (( x . shape [ 1 ] == 2 ) & ( t . shape [ 1 ] == 1 )), 'x and t should have shape (n_samples x 1)' u = self . generate_solution ( x , t ) X = np . concatenate ([ t , x ], axis = 1 ) y = u + noise * np . std ( u , axis = 0 ) * np . random . normal ( size = u . shape ) # creating random idx for samples N = y . shape [ 0 ] if n_samples == 0 else n_samples if random is True : rand_idx = np . random . RandomState ( seed = random_state ) . permutation ( y . shape [ 0 ])[: N ] # so we can get similar splits for different noise levels else : rand_idx = np . arange ( y . shape [ 0 ])[: N ] # Building dataset X_train = torch . tensor ( X [ rand_idx , :], requires_grad = True , dtype = torch . float32 ) y_train = torch . tensor ( y [ rand_idx , :], requires_grad = True , dtype = torch . float32 ) if return_idx is False : return X_train , y_train else : return X_train , y_train , rand_idx pytorch_func ( function ) Decorator to automatically transform arrays to tensors and back Parameters: Name Type Description Default function Tensor Pytorch tensor required Returns: Type Description Numpy array Source code in deepymod/data/base.py def pytorch_func ( function ): \"\"\"Decorator to automatically transform arrays to tensors and back Args: function (Tensor): Pytorch tensor Returns: Numpy array \"\"\" def wrapper ( self , * args , ** kwargs ): torch_args = [ torch . tensor ( arg , requires_grad = True , dtype = torch . float64 ) if type ( arg ) is ndarray else arg for arg in args ] torch_kwargs = { key : torch . tensor ( kwarg , requires_grad = True , dtype = torch . float64 ) if type ( kwarg ) is ndarray else kwarg for key , kwarg in kwargs . items ()} result = function ( self , * torch_args , ** torch_kwargs ) return result . cpu () . detach () . numpy () return wrapper","title":"Base"},{"location":"api/data/base/#deepymod.data.base","text":"","title":"deepymod.data.base"},{"location":"api/data/base/#deepymod.data.base.Dataset","text":"This class automatically generates all the necessary proporties of a predifined data set with a single spatial dimension as input. In particular it calculates the solution, the time derivative and the library. Note that all the pytorch opperations such as automatic differentiation can be used on the results.","title":"Dataset"},{"location":"api/data/base/#deepymod.data.base.Dataset.create_dataset","text":"Function creates the data set in the precise format used by DeepMoD Parameters: Name Type Description Default x Tensor Input vector of spatial coordinates required t Tensor Input vector of temporal coordinates required n_samples int Number of samples, set n_samples=0 for all. required noise float Noise level in percentage of std. required random bool When true, data set is randomised. Defaults to True. True normalize bool When true, data set is normalized. Defaults to True. True return_idx bool When true, the id of the data, before randomizing is returned. Defaults to False. False random_state int Seed of the randomisation. Defaults to 42. 42 Returns: Type Description [type] Tensor containing the input and output and optionally the randomisation. Source code in deepymod/data/base.py def create_dataset ( self , x , t , n_samples , noise , random = True , normalize = True , return_idx = False , random_state = 42 ): \"\"\"Function creates the data set in the precise format used by DeepMoD Args: x (Tensor): Input vector of spatial coordinates t (Tensor): Input vector of temporal coordinates n_samples (int): Number of samples, set n_samples=0 for all. noise (float): Noise level in percentage of std. random (bool, optional): When true, data set is randomised. Defaults to True. normalize (bool, optional): When true, data set is normalized. Defaults to True. return_idx (bool, optional): When true, the id of the data, before randomizing is returned. Defaults to False. random_state (int, optional): Seed of the randomisation. Defaults to 42. Returns: [type]: Tensor containing the input and output and optionally the randomisation. \"\"\" assert (( x . shape [ 1 ] == 1 ) & ( t . shape [ 1 ] == 1 )), 'x and t should have shape (n_samples x 1)' u = self . generate_solution ( x , t ) X = np . concatenate ([ t , x ], axis = 1 ) if random_state is None : y = u + noise * np . std ( u , axis = 0 ) * np . random . normal ( size = u . shape ) else : y = u + noise * np . std ( u , axis = 0 ) * np . random . RandomState ( seed = random_state ) . normal ( size = u . shape ) # creating random idx for samples N = y . shape [ 0 ] if n_samples == 0 else n_samples if random is True : if random_state is None : rand_idx = np . random . permutation ( y . shape [ 0 ])[: N ] else : rand_idx = np . random . RandomState ( seed = random_state ) . permutation ( y . shape [ 0 ])[: N ] else : rand_idx = np . arange ( y . shape [ 0 ])[: N ] # Normalizing if normalize : if ( self . scaling_factor is None ): self . scaling_factor = ( - ( np . max ( X , axis = 0 ) + np . min ( X , axis = 0 )) / 2 , ( np . max ( X , axis = 0 ) - np . min ( X , axis = 0 )) / 2 ) # only calculate the first time X = ( X + self . scaling_factor [ 0 ]) / self . scaling_factor [ 1 ] # Building dataset X_train = torch . tensor ( X [ rand_idx , :], dtype = torch . float32 ) y_train = torch . tensor ( y [ rand_idx , :], dtype = torch . float32 ) if return_idx is False : return X_train , y_train else : return X_train , y_train , rand_idx","title":"create_dataset()"},{"location":"api/data/base/#deepymod.data.base.Dataset_2D","text":"This class automatically generates all the necessary proporties of a predifined data set with two spatial dimension as input. In particular it calculates the solution, the time derivative and the library. Note that all the pytorch opperations such as automatic differentiation can be used on the results.","title":"Dataset_2D"},{"location":"api/data/base/#deepymod.data.base.Dataset_2D.create_dataset","text":"Function creates the data set in the precise format used by DeepMoD Parameters: Name Type Description Default x Tensor Input vector of spatial coordinates required t Tensor Input vector of temporal coordinates required n_samples int Number of samples, set n_samples=0 for all. required noise float Noise level in percentage of std. required random bool When true, data set is randomised. Defaults to True. True normalize bool When true, data set is normalized. Defaults to True. required return_idx bool When true, the id of the data, before randomizing is returned. Defaults to False. False random_state int Seed of the randomisation. Defaults to 42. 42 Returns: Type Description [type] Tensor containing the input and output and optionally the randomisation. Source code in deepymod/data/base.py def create_dataset ( self , x , t , n_samples , noise , random = True , return_idx = False , random_state = 42 ): \"\"\"Function creates the data set in the precise format used by DeepMoD Args: x (Tensor): Input vector of spatial coordinates t (Tensor): Input vector of temporal coordinates n_samples (int): Number of samples, set n_samples=0 for all. noise (float): Noise level in percentage of std. random (bool, optional): When true, data set is randomised. Defaults to True. normalize (bool, optional): When true, data set is normalized. Defaults to True. return_idx (bool, optional): When true, the id of the data, before randomizing is returned. Defaults to False. random_state (int, optional): Seed of the randomisation. Defaults to 42. Returns: [type]: Tensor containing the input and output and optionally the randomisation. \"\"\" assert (( x . shape [ 1 ] == 2 ) & ( t . shape [ 1 ] == 1 )), 'x and t should have shape (n_samples x 1)' u = self . generate_solution ( x , t ) X = np . concatenate ([ t , x ], axis = 1 ) y = u + noise * np . std ( u , axis = 0 ) * np . random . normal ( size = u . shape ) # creating random idx for samples N = y . shape [ 0 ] if n_samples == 0 else n_samples if random is True : rand_idx = np . random . RandomState ( seed = random_state ) . permutation ( y . shape [ 0 ])[: N ] # so we can get similar splits for different noise levels else : rand_idx = np . arange ( y . shape [ 0 ])[: N ] # Building dataset X_train = torch . tensor ( X [ rand_idx , :], requires_grad = True , dtype = torch . float32 ) y_train = torch . tensor ( y [ rand_idx , :], requires_grad = True , dtype = torch . float32 ) if return_idx is False : return X_train , y_train else : return X_train , y_train , rand_idx","title":"create_dataset()"},{"location":"api/data/base/#deepymod.data.base.pytorch_func","text":"Decorator to automatically transform arrays to tensors and back Parameters: Name Type Description Default function Tensor Pytorch tensor required Returns: Type Description Numpy array Source code in deepymod/data/base.py def pytorch_func ( function ): \"\"\"Decorator to automatically transform arrays to tensors and back Args: function (Tensor): Pytorch tensor Returns: Numpy array \"\"\" def wrapper ( self , * args , ** kwargs ): torch_args = [ torch . tensor ( arg , requires_grad = True , dtype = torch . float64 ) if type ( arg ) is ndarray else arg for arg in args ] torch_kwargs = { key : torch . tensor ( kwarg , requires_grad = True , dtype = torch . float64 ) if type ( kwarg ) is ndarray else kwarg for key , kwarg in kwargs . items ()} result = function ( self , * torch_args , ** torch_kwargs ) return result . cpu () . detach () . numpy () return wrapper","title":"pytorch_func()"},{"location":"api/data/burgers/","text":"BurgersCos ( x , t , v , a , b , k ) Function to generate analytical solutions of Burgers equation with cosine initial condition: u(x, 0) = b + a \\cos(kx) u(x, 0) = b + a \\cos(kx) Source: https://www.iist.ac.in/sites/default/files/people/IN08026/Burgers_equation_viscous.pdf Parameters: Name Type Description Default x [Tensor] Input vector of spatial coordinates. required t [Tensor] Input vector of temporal coordinates. required v Float Velocity. required a [type] Amplitude of the initial periodic condition. required b [type] Offset of the initial condition. required k [type] Wavenumber of the initial condition. required Returns: Type Description [Tensor] solution. Source code in deepymod/data/burgers/burgers.py def BurgersCos ( x , t , v , a , b , k ): \"\"\"Function to generate analytical solutions of Burgers equation with cosine initial condition: $u(x, 0) = b + a \\cos(kx)$ Source: https://www.iist.ac.in/sites/default/files/people/IN08026/Burgers_equation_viscous.pdf Args: x ([Tensor]): Input vector of spatial coordinates. t ([Tensor]): Input vector of temporal coordinates. v (Float): Velocity. a ([type]): Amplitude of the initial periodic condition. b ([type]): Offset of the initial condition. k ([type]): Wavenumber of the initial condition. Returns: [Tensor]: solution. \"\"\" z = v * k ** 2 * t u = ( 2 * v * a * k * torch . exp ( - z ) * torch . sin ( k * x )) / ( b + a * torch . exp ( - z ) * torch . cos ( k * x )) return u BurgersDelta ( x , t , v , A ) Function to generate analytical solutions of Burgers equation with delta peak initial condition: u(x, 0) = A delta(x) Source: https://www.iist.ac.in/sites/default/files/people/IN08026/Burgers_equation_viscous.pdf Note that this source has an error in the erfc prefactor, should be sqrt(pi)/2, not sqrt(pi/2). Parameters: Name Type Description Default x [Tensor] Input vector of spatial coordinates. required t [Tensor] Input vector of temporal coordinates. required v Float Velocity. required A Float Amplitude of the initial condition. required Returns: Type Description [Tensor] solution. Source code in deepymod/data/burgers/burgers.py def BurgersDelta ( x , t , v , A ): \"\"\" Function to generate analytical solutions of Burgers equation with delta peak initial condition: u(x, 0) = A delta(x) Source: https://www.iist.ac.in/sites/default/files/people/IN08026/Burgers_equation_viscous.pdf Note that this source has an error in the erfc prefactor, should be sqrt(pi)/2, not sqrt(pi/2). Args: x ([Tensor]): Input vector of spatial coordinates. t ([Tensor]): Input vector of temporal coordinates. v (Float): Velocity. A (Float): Amplitude of the initial condition. Returns: [Tensor]: solution. \"\"\" R = torch . tensor ( A / ( 2 * v )) # otherwise throws error z = x / torch . sqrt ( 4 * v * t ) u = torch . sqrt ( v / ( pi * t )) * (( torch . exp ( R ) - 1 ) * torch . exp ( - z ** 2 )) / ( 1 + ( torch . exp ( R ) - 1 ) / 2 * torch . erfc ( z )) return u BurgersSawtooth ( x , t , v ) Function to generate analytical solutions of Burgers equation with sawtooth initial condition (see soruce for exact expression). Solution only valid between for x in [0, 2pi] and t in [0, 0.5] http://www.thevisualroom.com/02_barba_projects/burgers_equation.html Parameters: Name Type Description Default x [Tensor] Input vector of spatial coordinates. required t [Tensor] Input vector of temporal coordinates. required v Float Velocity. required Returns: Type Description [Tensor] solution. Source code in deepymod/data/burgers/burgers.py def BurgersSawtooth ( x , t , v ): \"\"\"Function to generate analytical solutions of Burgers equation with sawtooth initial condition (see soruce for exact expression). Solution only valid between for x in [0, 2pi] and t in [0, 0.5] http://www.thevisualroom.com/02_barba_projects/burgers_equation.html Args: x ([Tensor]): Input vector of spatial coordinates. t ([Tensor]): Input vector of temporal coordinates. v (Float): Velocity. Returns: [Tensor]: solution. \"\"\" z_left = ( x - 4 * t ) z_right = ( x - 4 * t - 2 * pi ) l = 4 * v * ( t + 1 ) phi = torch . exp ( - z_left ** 2 / l ) + torch . exp ( - z_right ** 2 / l ) dphi_x = - 2 * z_left / l * torch . exp ( - z_left ** 2 / l ) - 2 * z_right / l * torch . exp ( - z_right ** 2 / l ) u = - 2 * v * dphi_x / phi + 4 return u","title":"Burgers"},{"location":"api/data/burgers/#deepymod.data.burgers.burgers","text":"","title":"deepymod.data.burgers.burgers"},{"location":"api/data/burgers/#deepymod.data.burgers.burgers.BurgersCos","text":"Function to generate analytical solutions of Burgers equation with cosine initial condition: u(x, 0) = b + a \\cos(kx) u(x, 0) = b + a \\cos(kx) Source: https://www.iist.ac.in/sites/default/files/people/IN08026/Burgers_equation_viscous.pdf Parameters: Name Type Description Default x [Tensor] Input vector of spatial coordinates. required t [Tensor] Input vector of temporal coordinates. required v Float Velocity. required a [type] Amplitude of the initial periodic condition. required b [type] Offset of the initial condition. required k [type] Wavenumber of the initial condition. required Returns: Type Description [Tensor] solution. Source code in deepymod/data/burgers/burgers.py def BurgersCos ( x , t , v , a , b , k ): \"\"\"Function to generate analytical solutions of Burgers equation with cosine initial condition: $u(x, 0) = b + a \\cos(kx)$ Source: https://www.iist.ac.in/sites/default/files/people/IN08026/Burgers_equation_viscous.pdf Args: x ([Tensor]): Input vector of spatial coordinates. t ([Tensor]): Input vector of temporal coordinates. v (Float): Velocity. a ([type]): Amplitude of the initial periodic condition. b ([type]): Offset of the initial condition. k ([type]): Wavenumber of the initial condition. Returns: [Tensor]: solution. \"\"\" z = v * k ** 2 * t u = ( 2 * v * a * k * torch . exp ( - z ) * torch . sin ( k * x )) / ( b + a * torch . exp ( - z ) * torch . cos ( k * x )) return u","title":"BurgersCos()"},{"location":"api/data/burgers/#deepymod.data.burgers.burgers.BurgersDelta","text":"Function to generate analytical solutions of Burgers equation with delta peak initial condition: u(x, 0) = A delta(x) Source: https://www.iist.ac.in/sites/default/files/people/IN08026/Burgers_equation_viscous.pdf Note that this source has an error in the erfc prefactor, should be sqrt(pi)/2, not sqrt(pi/2). Parameters: Name Type Description Default x [Tensor] Input vector of spatial coordinates. required t [Tensor] Input vector of temporal coordinates. required v Float Velocity. required A Float Amplitude of the initial condition. required Returns: Type Description [Tensor] solution. Source code in deepymod/data/burgers/burgers.py def BurgersDelta ( x , t , v , A ): \"\"\" Function to generate analytical solutions of Burgers equation with delta peak initial condition: u(x, 0) = A delta(x) Source: https://www.iist.ac.in/sites/default/files/people/IN08026/Burgers_equation_viscous.pdf Note that this source has an error in the erfc prefactor, should be sqrt(pi)/2, not sqrt(pi/2). Args: x ([Tensor]): Input vector of spatial coordinates. t ([Tensor]): Input vector of temporal coordinates. v (Float): Velocity. A (Float): Amplitude of the initial condition. Returns: [Tensor]: solution. \"\"\" R = torch . tensor ( A / ( 2 * v )) # otherwise throws error z = x / torch . sqrt ( 4 * v * t ) u = torch . sqrt ( v / ( pi * t )) * (( torch . exp ( R ) - 1 ) * torch . exp ( - z ** 2 )) / ( 1 + ( torch . exp ( R ) - 1 ) / 2 * torch . erfc ( z )) return u","title":"BurgersDelta()"},{"location":"api/data/burgers/#deepymod.data.burgers.burgers.BurgersSawtooth","text":"Function to generate analytical solutions of Burgers equation with sawtooth initial condition (see soruce for exact expression). Solution only valid between for x in [0, 2pi] and t in [0, 0.5] http://www.thevisualroom.com/02_barba_projects/burgers_equation.html Parameters: Name Type Description Default x [Tensor] Input vector of spatial coordinates. required t [Tensor] Input vector of temporal coordinates. required v Float Velocity. required Returns: Type Description [Tensor] solution. Source code in deepymod/data/burgers/burgers.py def BurgersSawtooth ( x , t , v ): \"\"\"Function to generate analytical solutions of Burgers equation with sawtooth initial condition (see soruce for exact expression). Solution only valid between for x in [0, 2pi] and t in [0, 0.5] http://www.thevisualroom.com/02_barba_projects/burgers_equation.html Args: x ([Tensor]): Input vector of spatial coordinates. t ([Tensor]): Input vector of temporal coordinates. v (Float): Velocity. Returns: [Tensor]: solution. \"\"\" z_left = ( x - 4 * t ) z_right = ( x - 4 * t - 2 * pi ) l = 4 * v * ( t + 1 ) phi = torch . exp ( - z_left ** 2 / l ) + torch . exp ( - z_right ** 2 / l ) dphi_x = - 2 * z_left / l * torch . exp ( - z_left ** 2 / l ) - 2 * z_right / l * torch . exp ( - z_right ** 2 / l ) u = - 2 * v * dphi_x / phi + 4 return u","title":"BurgersSawtooth()"},{"location":"api/data/kdv/","text":"DoubleSoliton ( x , t , c , x0 ) Single soliton solution of the KdV equation (u_t + u_{xxx} - 6 u u_x = 0) source: http://lie.math.brocku.ca/~sanco/solitons/kdv_solitons.php Parameters: Name Type Description Default x [Tensor] Input vector of spatial coordinates. required t [Tensor] Input vector of temporal coordinates. required c [Array] Array containing the velocities of the two solitons, note that c[0] > c[1]. required x0 [Array] Array containing the offsets of the two solitons. required Returns: Type Description [Tensor] Solution. Source code in deepymod/data/kdv/kdv.py def DoubleSoliton ( x , t , c , x0 ): \"\"\" Single soliton solution of the KdV equation (u_t + u_{xxx} - 6 u u_x = 0) source: http://lie.math.brocku.ca/~sanco/solitons/kdv_solitons.php Args: x ([Tensor]): Input vector of spatial coordinates. t ([Tensor]): Input vector of temporal coordinates. c ([Array]): Array containing the velocities of the two solitons, note that c[0] > c[1]. x0 ([Array]): Array containing the offsets of the two solitons. Returns: [Tensor]: Solution. \"\"\" assert c [ 0 ] > c [ 1 ], 'c1 has to be bigger than c[2]' xi0 = np . sqrt ( c [ 0 ]) / 2 * ( x - c [ 0 ] * t - x0 [ 0 ]) # switch to moving coordinate frame xi1 = np . sqrt ( c [ 1 ]) / 2 * ( x - c [ 1 ] * t - x0 [ 1 ]) part_1 = 2 * ( c [ 0 ] - c [ 1 ]) numerator = c [ 0 ] * torch . cosh ( xi1 ) ** 2 + c [ 1 ] * torch . sinh ( xi0 ) ** 2 denominator_1 = ( np . sqrt ( c [ 0 ]) - np . sqrt ( c [ 1 ])) * torch . cosh ( xi0 + xi1 ) denominator_2 = ( np . sqrt ( c [ 0 ]) + np . sqrt ( c [ 1 ])) * torch . cosh ( xi0 - xi1 ) u = part_1 * numerator / ( denominator_1 + denominator_2 ) ** 2 return u SingleSoliton ( x , t , c , x0 ) Single soliton solution of the KdV equation (u_t + u_{xxx} - 6 u u_x = 0) Parameters: Name Type Description Default x [Tensor] Input vector of spatial coordinates. required t [Tensor] Input vector of temporal coordinates. required c [Float] Velocity. required x0 [Float] Offset. required Returns: Type Description [Tensor] Solution. Source code in deepymod/data/kdv/kdv.py def SingleSoliton ( x , t , c , x0 ): \"\"\"Single soliton solution of the KdV equation (u_t + u_{xxx} - 6 u u_x = 0) Args: x ([Tensor]): Input vector of spatial coordinates. t ([Tensor]): Input vector of temporal coordinates. c ([Float]): Velocity. x0 ([Float]): Offset. Returns: [Tensor]: Solution. \"\"\" xi = np . sqrt ( c ) / 2 * ( x - c * t - x0 ) # switch to moving coordinate frame u = c / 2 * 1 / torch . cosh ( xi ) ** 2 return u","title":"Korteweg-de Vries"},{"location":"api/data/kdv/#deepymod.data.kdv.kdv","text":"","title":"deepymod.data.kdv.kdv"},{"location":"api/data/kdv/#deepymod.data.kdv.kdv.DoubleSoliton","text":"Single soliton solution of the KdV equation (u_t + u_{xxx} - 6 u u_x = 0) source: http://lie.math.brocku.ca/~sanco/solitons/kdv_solitons.php Parameters: Name Type Description Default x [Tensor] Input vector of spatial coordinates. required t [Tensor] Input vector of temporal coordinates. required c [Array] Array containing the velocities of the two solitons, note that c[0] > c[1]. required x0 [Array] Array containing the offsets of the two solitons. required Returns: Type Description [Tensor] Solution. Source code in deepymod/data/kdv/kdv.py def DoubleSoliton ( x , t , c , x0 ): \"\"\" Single soliton solution of the KdV equation (u_t + u_{xxx} - 6 u u_x = 0) source: http://lie.math.brocku.ca/~sanco/solitons/kdv_solitons.php Args: x ([Tensor]): Input vector of spatial coordinates. t ([Tensor]): Input vector of temporal coordinates. c ([Array]): Array containing the velocities of the two solitons, note that c[0] > c[1]. x0 ([Array]): Array containing the offsets of the two solitons. Returns: [Tensor]: Solution. \"\"\" assert c [ 0 ] > c [ 1 ], 'c1 has to be bigger than c[2]' xi0 = np . sqrt ( c [ 0 ]) / 2 * ( x - c [ 0 ] * t - x0 [ 0 ]) # switch to moving coordinate frame xi1 = np . sqrt ( c [ 1 ]) / 2 * ( x - c [ 1 ] * t - x0 [ 1 ]) part_1 = 2 * ( c [ 0 ] - c [ 1 ]) numerator = c [ 0 ] * torch . cosh ( xi1 ) ** 2 + c [ 1 ] * torch . sinh ( xi0 ) ** 2 denominator_1 = ( np . sqrt ( c [ 0 ]) - np . sqrt ( c [ 1 ])) * torch . cosh ( xi0 + xi1 ) denominator_2 = ( np . sqrt ( c [ 0 ]) + np . sqrt ( c [ 1 ])) * torch . cosh ( xi0 - xi1 ) u = part_1 * numerator / ( denominator_1 + denominator_2 ) ** 2 return u","title":"DoubleSoliton()"},{"location":"api/data/kdv/#deepymod.data.kdv.kdv.SingleSoliton","text":"Single soliton solution of the KdV equation (u_t + u_{xxx} - 6 u u_x = 0) Parameters: Name Type Description Default x [Tensor] Input vector of spatial coordinates. required t [Tensor] Input vector of temporal coordinates. required c [Float] Velocity. required x0 [Float] Offset. required Returns: Type Description [Tensor] Solution. Source code in deepymod/data/kdv/kdv.py def SingleSoliton ( x , t , c , x0 ): \"\"\"Single soliton solution of the KdV equation (u_t + u_{xxx} - 6 u u_x = 0) Args: x ([Tensor]): Input vector of spatial coordinates. t ([Tensor]): Input vector of temporal coordinates. c ([Float]): Velocity. x0 ([Float]): Offset. Returns: [Tensor]: Solution. \"\"\" xi = np . sqrt ( c ) / 2 * ( x - c * t - x0 ) # switch to moving coordinate frame u = c / 2 * 1 / torch . cosh ( xi ) ** 2 return u","title":"SingleSoliton()"},{"location":"background/background/","text":"Background","title":"Background"},{"location":"background/background/#background","text":"","title":"Background"},{"location":"examples/2DAD/2DAD/","text":"2D Advection-Diffusion equation in this notebook we provide a simple example of the DeepMoD algorithm and apply it on the 2D advection-diffusion equation. # General imports import numpy as np import torch import matplotlib.pylab as plt # DeepMoD functions from deepymod import DeepMoD from deepymod.model.func_approx import NN from deepymod.model.library import Library2D from deepymod.model.constraint import LeastSquares from deepymod.model.sparse_estimators import Threshold , PDEFIND from deepymod.training import train from deepymod.training.sparsity_scheduler import TrainTestPeriodic from scipy.io import loadmat # Settings for reproducibility np . random . seed ( 42 ) torch . manual_seed ( 0 ) % load_ext autoreload % autoreload 2 Prepare the data Next, we prepare the dataset. data = loadmat ( '../data/Advection_diffusion.mat' ) usol = np . real ( data [ 'Expression1' ]) usol = usol . reshape (( 51 , 51 , 61 , 4 )) x_v = usol [:,:,:, 0 ] y_v = usol [:,:,:, 1 ] t_v = usol [:,:,:, 2 ] u_v = usol [:,:,:, 3 ] Next we plot the dataset for three different time-points fig , axes = plt . subplots ( ncols = 3 , figsize = ( 15 , 4 )) im0 = axes [ 0 ] . contourf ( x_v [:,:, 0 ], y_v [:,:, 0 ], u_v [:,:, 0 ], cmap = 'coolwarm' ) axes [ 0 ] . set_xlabel ( 'x' ) axes [ 0 ] . set_ylabel ( 'y' ) axes [ 0 ] . set_title ( 't = 0' ) im1 = axes [ 1 ] . contourf ( x_v [:,:, 10 ], y_v [:,:, 10 ], u_v [:,:, 10 ], cmap = 'coolwarm' ) axes [ 1 ] . set_xlabel ( 'x' ) axes [ 1 ] . set_title ( 't = 10' ) im2 = axes [ 2 ] . contourf ( x_v [:,:, 20 ], y_v [:,:, 20 ], u_v [:,:, 20 ], cmap = 'coolwarm' ) axes [ 2 ] . set_xlabel ( 'x' ) axes [ 2 ] . set_title ( 't= 20' ) fig . colorbar ( im1 , ax = axes . ravel () . tolist ()) plt . show () We flatten it to give it the right dimensions for feeding it to the network: X = np . transpose (( t_v . flatten (), x_v . flatten (), y_v . flatten ())) y = np . float32 ( u_v . reshape (( u_v . size , 1 ))) We select the noise level we add to the data-set noise_level = 0.01 y_noisy = y + noise_level * np . std ( y ) * np . random . randn ( y . size , 1 ) Select the number of samples: number_of_samples = 1000 idx = np . random . permutation ( y . shape [ 0 ]) X_train = torch . tensor ( X [ idx , :][: number_of_samples ], dtype = torch . float32 , requires_grad = True ) y_train = torch . tensor ( y [ idx , :][: number_of_samples ], dtype = torch . float32 ) Configuration of DeepMoD Configuration of the function approximator: Here the first argument is the number of input and the last argument the number of output layers. network = NN ( 3 , [ 50 , 50 , 50 , 50 ], 1 ) Configuration of the library function: We select athe library with a 2D spatial input. Note that that the max differential order has been pre-determined here out of convinience. So, for poly_order 1 the library contains the following 12 terms: * [ 1, u_x, u_y, u_{xx}, u_{yy}, u_{xy}, u, u u_x, u u_y, u u_{xx}, u u_{yy}, u u_{xy} 1, u_x, u_y, u_{xx}, u_{yy}, u_{xy}, u, u u_x, u u_y, u u_{xx}, u u_{yy}, u u_{xy} ] library = Library2D ( poly_order = 1 ) Configuration of the sparsity estimator and sparsity scheduler used. In this case we use the most basic threshold-based Lasso estimator and a scheduler that asseses the validation loss after a given patience. If that value is smaller than 1e-5, the algorithm is converged. estimator = Threshold ( 0.1 ) sparsity_scheduler = TrainTestPeriodic ( periodicity = 50 , patience = 10 , delta = 1e-5 ) Configuration of the sparsity estimator constraint = LeastSquares () # Configuration of the sparsity scheduler Now we instantiate the model and select the optimizer model = DeepMoD ( network , library , estimator , constraint ) # Defining optimizer optimizer = torch . optim . Adam ( model . parameters (), betas = ( 0.99 , 0.99 ), amsgrad = True , lr = 1e-3 ) Run DeepMoD We can now run DeepMoD using all the options we have set and the training data: * The directory where the tensorboard file is written (log_dir) * The ratio of train/test set used (split) * The maximum number of iterations performed (max_iterations) * The absolute change in L1 norm considered converged (delta) * The amount of epochs over which the absolute change in L1 norm is calculated (patience) train ( model , X_train , y_train , optimizer , sparsity_scheduler , log_dir = 'runs/2DAD/' , split = 0.8 , max_iterations = 100000 , delta = 1e-4 , patience = 8 ) | Iteration | Progress | Time remaining | Loss | MSE | Reg | L1 norm | 7000 7.00% 3733s 1.07e-04 3.60e-05 7.08e-05 1.87e+00 Algorithm converged. Stopping training. Sparsity masks provide the active and non-active terms in the PDE: model . sparsity_masks [tensor([False, True, True, True, True, False, False, False, False, False, False, False])] estimatior_coeffs gives the magnitude of the active terms: print ( model . estimator_coeffs ()) [array([[0. ], [0.3770935 ], [0.7139108 ], [0.389949 ], [0.32122847], [0. ], [0. ], [0. ], [0. ], [0. ], [0. ], [0. ]], dtype=float32)]","title":"2D Advection Diffusion"},{"location":"examples/2DAD/2DAD/#2d-advection-diffusion-equation","text":"in this notebook we provide a simple example of the DeepMoD algorithm and apply it on the 2D advection-diffusion equation. # General imports import numpy as np import torch import matplotlib.pylab as plt # DeepMoD functions from deepymod import DeepMoD from deepymod.model.func_approx import NN from deepymod.model.library import Library2D from deepymod.model.constraint import LeastSquares from deepymod.model.sparse_estimators import Threshold , PDEFIND from deepymod.training import train from deepymod.training.sparsity_scheduler import TrainTestPeriodic from scipy.io import loadmat # Settings for reproducibility np . random . seed ( 42 ) torch . manual_seed ( 0 ) % load_ext autoreload % autoreload 2","title":"2D Advection-Diffusion equation"},{"location":"examples/2DAD/2DAD/#prepare-the-data","text":"Next, we prepare the dataset. data = loadmat ( '../data/Advection_diffusion.mat' ) usol = np . real ( data [ 'Expression1' ]) usol = usol . reshape (( 51 , 51 , 61 , 4 )) x_v = usol [:,:,:, 0 ] y_v = usol [:,:,:, 1 ] t_v = usol [:,:,:, 2 ] u_v = usol [:,:,:, 3 ] Next we plot the dataset for three different time-points fig , axes = plt . subplots ( ncols = 3 , figsize = ( 15 , 4 )) im0 = axes [ 0 ] . contourf ( x_v [:,:, 0 ], y_v [:,:, 0 ], u_v [:,:, 0 ], cmap = 'coolwarm' ) axes [ 0 ] . set_xlabel ( 'x' ) axes [ 0 ] . set_ylabel ( 'y' ) axes [ 0 ] . set_title ( 't = 0' ) im1 = axes [ 1 ] . contourf ( x_v [:,:, 10 ], y_v [:,:, 10 ], u_v [:,:, 10 ], cmap = 'coolwarm' ) axes [ 1 ] . set_xlabel ( 'x' ) axes [ 1 ] . set_title ( 't = 10' ) im2 = axes [ 2 ] . contourf ( x_v [:,:, 20 ], y_v [:,:, 20 ], u_v [:,:, 20 ], cmap = 'coolwarm' ) axes [ 2 ] . set_xlabel ( 'x' ) axes [ 2 ] . set_title ( 't= 20' ) fig . colorbar ( im1 , ax = axes . ravel () . tolist ()) plt . show () We flatten it to give it the right dimensions for feeding it to the network: X = np . transpose (( t_v . flatten (), x_v . flatten (), y_v . flatten ())) y = np . float32 ( u_v . reshape (( u_v . size , 1 ))) We select the noise level we add to the data-set noise_level = 0.01 y_noisy = y + noise_level * np . std ( y ) * np . random . randn ( y . size , 1 ) Select the number of samples: number_of_samples = 1000 idx = np . random . permutation ( y . shape [ 0 ]) X_train = torch . tensor ( X [ idx , :][: number_of_samples ], dtype = torch . float32 , requires_grad = True ) y_train = torch . tensor ( y [ idx , :][: number_of_samples ], dtype = torch . float32 )","title":"Prepare the data"},{"location":"examples/2DAD/2DAD/#configuration-of-deepmod","text":"Configuration of the function approximator: Here the first argument is the number of input and the last argument the number of output layers. network = NN ( 3 , [ 50 , 50 , 50 , 50 ], 1 ) Configuration of the library function: We select athe library with a 2D spatial input. Note that that the max differential order has been pre-determined here out of convinience. So, for poly_order 1 the library contains the following 12 terms: * [ 1, u_x, u_y, u_{xx}, u_{yy}, u_{xy}, u, u u_x, u u_y, u u_{xx}, u u_{yy}, u u_{xy} 1, u_x, u_y, u_{xx}, u_{yy}, u_{xy}, u, u u_x, u u_y, u u_{xx}, u u_{yy}, u u_{xy} ] library = Library2D ( poly_order = 1 ) Configuration of the sparsity estimator and sparsity scheduler used. In this case we use the most basic threshold-based Lasso estimator and a scheduler that asseses the validation loss after a given patience. If that value is smaller than 1e-5, the algorithm is converged. estimator = Threshold ( 0.1 ) sparsity_scheduler = TrainTestPeriodic ( periodicity = 50 , patience = 10 , delta = 1e-5 ) Configuration of the sparsity estimator constraint = LeastSquares () # Configuration of the sparsity scheduler Now we instantiate the model and select the optimizer model = DeepMoD ( network , library , estimator , constraint ) # Defining optimizer optimizer = torch . optim . Adam ( model . parameters (), betas = ( 0.99 , 0.99 ), amsgrad = True , lr = 1e-3 )","title":"Configuration of DeepMoD"},{"location":"examples/2DAD/2DAD/#run-deepmod","text":"We can now run DeepMoD using all the options we have set and the training data: * The directory where the tensorboard file is written (log_dir) * The ratio of train/test set used (split) * The maximum number of iterations performed (max_iterations) * The absolute change in L1 norm considered converged (delta) * The amount of epochs over which the absolute change in L1 norm is calculated (patience) train ( model , X_train , y_train , optimizer , sparsity_scheduler , log_dir = 'runs/2DAD/' , split = 0.8 , max_iterations = 100000 , delta = 1e-4 , patience = 8 ) | Iteration | Progress | Time remaining | Loss | MSE | Reg | L1 norm | 7000 7.00% 3733s 1.07e-04 3.60e-05 7.08e-05 1.87e+00 Algorithm converged. Stopping training. Sparsity masks provide the active and non-active terms in the PDE: model . sparsity_masks [tensor([False, True, True, True, True, False, False, False, False, False, False, False])] estimatior_coeffs gives the magnitude of the active terms: print ( model . estimator_coeffs ()) [array([[0. ], [0.3770935 ], [0.7139108 ], [0.389949 ], [0.32122847], [0. ], [0. ], [0. ], [0. ], [0. ], [0. ], [0. ]], dtype=float32)]","title":"Run DeepMoD"},{"location":"examples/Burgers/PDE_Burgers/","text":"Example Burgers' equation In this notebook we provide a simple example of the DeepMoD algorithm by applying it on the Burgers' equation. We start by importing the required libraries and setting the plotting style: # General imports import numpy as np import torch import matplotlib.pylab as plt # DeepMoD stuff from deepymod_torch.DeepMod import DeepMod from deepymod_torch.library_functions import library_1D_in from deepymod_torch.training import train_deepmod , train_mse # Settings for reproducibility np . random . seed ( 42 ) torch . manual_seed ( 0 ) % load_ext autoreload % autoreload 2 The autoreload extension is already loaded . To reload it , use : %reload_ext autoreload Next, we prepare the dataset. data = np . load ( 'data/burgers.npy' , allow_pickle = True ) . item () print ( 'Shape of grid:' , data [ 'x' ] . shape ) Shape of grid: (256, 101) Let's plot it to get an idea of the data: fig , ax = plt . subplots () im = ax . contourf ( data [ 'x' ], data [ 't' ], np . real ( data [ 'u' ])) ax . set_xlabel ( 'x' ) ax . set_ylabel ( 't' ) fig . colorbar ( mappable = im ) plt . show () X = np . transpose (( data [ 't' ] . flatten (), data [ 'x' ] . flatten ())) y = np . real ( data [ 'u' ]) . reshape (( data [ 'u' ] . size , 1 )) print ( X . shape , y . shape ) (25856, 2) (25856, 1) As we can see, X X has 2 dimensions, \\{x, t\\} \\{x, t\\} , while y y has only one, \\{u\\} \\{u\\} . Always explicity set the shape (i.e. N\\times 1 N\\times 1 , not N N ) or you'll get errors. This dataset is noiseless, so let's add 5\\% 5\\% noise: noise_level = 0.05 y_noisy = y + noise_level * np . std ( y ) * np . random . randn ( y [:, 0 ] . size , 1 ) The dataset is also much larger than needed, so let's hussle it and pick out a 1000 samples: number_of_samples = 1000 idx = np . random . permutation ( y . shape [ 0 ]) X_train = torch . tensor ( X [ idx , :][: number_of_samples ], dtype = torch . float32 , requires_grad = True ) y_train = torch . tensor ( y_noisy [ idx , :][: number_of_samples ], dtype = torch . float32 ) print ( X_train . shape , y_train . shape ) torch.Size([1000, 2]) torch.Size([1000, 1]) We now have a dataset which we can use. Let's plot, for a final time, the original dataset, the noisy set and the samples points: fig , axes = plt . subplots ( ncols = 3 , figsize = ( 15 , 4 )) im0 = axes [ 0 ] . contourf ( data [ 'x' ], data [ 't' ], np . real ( data [ 'u' ]), cmap = 'coolwarm' ) axes [ 0 ] . set_xlabel ( 'x' ) axes [ 0 ] . set_ylabel ( 't' ) axes [ 0 ] . set_title ( 'Ground truth' ) im1 = axes [ 1 ] . contourf ( data [ 'x' ], data [ 't' ], y_noisy . reshape ( data [ 'x' ] . shape ), cmap = 'coolwarm' ) axes [ 1 ] . set_xlabel ( 'x' ) axes [ 1 ] . set_title ( 'Noisy' ) sampled = np . array ([ y_noisy [ index , 0 ] if index in idx [: number_of_samples ] else np . nan for index in np . arange ( data [ 'x' ] . size )]) sampled = np . rot90 ( sampled . reshape ( data [ 'x' ] . shape )) #array needs to be rotated because of imshow im2 = axes [ 2 ] . imshow ( sampled , aspect = 'auto' , cmap = 'coolwarm' ) axes [ 2 ] . set_xlabel ( 'x' ) axes [ 2 ] . set_title ( 'Sampled' ) fig . colorbar ( im1 , ax = axes . ravel () . tolist ()) plt . show () Configuring DeepMoD We now setup the options for DeepMoD. The setup requires the dimensions of the neural network, a library function and some args for the library function: ## Running DeepMoD config = { 'n_in' : 2 , 'hidden_dims' : [ 20 , 20 , 20 , 20 , 20 , 20 ], 'n_out' : 1 , 'library_function' : library_1D_in , 'library_args' :{ 'poly_order' : 1 , 'diff_order' : 2 }} Now we instantiate the model: model = DeepMod ( ** config ) optimizer = torch . optim . Adam ([{ 'params' : model . network_parameters (), 'lr' : 0.001 }, { 'params' : model . coeff_vector (), 'lr' : 0.005 }]) Run DeepMoD We can now run DeepMoD using all the options we have set and the training data. We need to slightly preprocess the input data for the derivatives: train_deepmod ( model , X_train , y_train , optimizer , 25000 , { 'l1' : 1e-5 }) | Iteration | Progress | Time remaining | Cost | MSE | Reg | L1 | 100 0 . 40 % 371 s 2 .29e-02 1 .64e-02 6 .41e-03 1 .25e-04 ------------------------------------------------------------------------- -- KeyboardInterrupt Traceback (most recent call last) < ipython - input - 21 - 790 b1a88b6d5 > in < module > ----> 1 train_deepmod ( model , X_train , y_train , optimizer , 25000 , { 'l1' : 1 e - 5 } ) ~/ Documents / GitHub / New_DeepMod_Simple / DeePyMoD_torch / src / deepymod_torch / training . py in train_deepmod ( model , data , target , optimizer , max_iterations , loss_func_args ) 67 '''Performs full deepmod cycle: trains model, thresholds and trains again for unbiased estimate. Updates model in-place.''' 68 # Train first cycle and get prediction ---> 69 train ( model , data , target , optimizer , max_iterations , loss_func_args ) 70 prediction , time_deriv_list , sparse_theta_list , coeff_vector_list = model ( data ) 71 ~/ Documents / GitHub / New_DeepMod_Simple / DeePyMoD_torch / src / deepymod_torch / training . py in train ( model , data , target , optimizer , max_iterations , loss_func_args ) 32 # Optimizer step 33 optimizer . zero_grad () ---> 34 loss . backward () 35 optimizer . step () 36 board . close () ~/ opt / anaconda3 / lib / python3 . 7 / site - packages / torch / tensor . py in backward ( self , gradient , retain_graph , create_graph ) 193 products . Defaults to `` False `` . 194 \"\" \" --> 195 torch . autograd . backward ( self , gradient , retain_graph , create_graph ) 196 197 def register_hook ( self , hook ): ~/ opt / anaconda3 / lib / python3 . 7 / site - packages / torch / autograd / __init__ . py in backward ( tensors , grad_tensors , retain_graph , create_graph , grad_variables ) 97 Variable . _execution_engine . run_backward ( 98 tensors , grad_tensors , retain_graph , create_graph , ---> 99 allow_unreachable = True ) # allow_unreachable flag 100 101 KeyboardInterrupt : Now that DeepMoD has converged, it has found the following numbers: print ( model . fit . sparsity_mask ) [tensor([2, 4])] print ( model . fit . coeff_vector [ 0 ]) Parameter containing: tensor([[ 0.0974], [-0.9905]], requires_grad=True)","title":"Example Burgers' equation"},{"location":"examples/Burgers/PDE_Burgers/#example-burgers-equation","text":"In this notebook we provide a simple example of the DeepMoD algorithm by applying it on the Burgers' equation. We start by importing the required libraries and setting the plotting style: # General imports import numpy as np import torch import matplotlib.pylab as plt # DeepMoD stuff from deepymod_torch.DeepMod import DeepMod from deepymod_torch.library_functions import library_1D_in from deepymod_torch.training import train_deepmod , train_mse # Settings for reproducibility np . random . seed ( 42 ) torch . manual_seed ( 0 ) % load_ext autoreload % autoreload 2 The autoreload extension is already loaded . To reload it , use : %reload_ext autoreload Next, we prepare the dataset. data = np . load ( 'data/burgers.npy' , allow_pickle = True ) . item () print ( 'Shape of grid:' , data [ 'x' ] . shape ) Shape of grid: (256, 101) Let's plot it to get an idea of the data: fig , ax = plt . subplots () im = ax . contourf ( data [ 'x' ], data [ 't' ], np . real ( data [ 'u' ])) ax . set_xlabel ( 'x' ) ax . set_ylabel ( 't' ) fig . colorbar ( mappable = im ) plt . show () X = np . transpose (( data [ 't' ] . flatten (), data [ 'x' ] . flatten ())) y = np . real ( data [ 'u' ]) . reshape (( data [ 'u' ] . size , 1 )) print ( X . shape , y . shape ) (25856, 2) (25856, 1) As we can see, X X has 2 dimensions, \\{x, t\\} \\{x, t\\} , while y y has only one, \\{u\\} \\{u\\} . Always explicity set the shape (i.e. N\\times 1 N\\times 1 , not N N ) or you'll get errors. This dataset is noiseless, so let's add 5\\% 5\\% noise: noise_level = 0.05 y_noisy = y + noise_level * np . std ( y ) * np . random . randn ( y [:, 0 ] . size , 1 ) The dataset is also much larger than needed, so let's hussle it and pick out a 1000 samples: number_of_samples = 1000 idx = np . random . permutation ( y . shape [ 0 ]) X_train = torch . tensor ( X [ idx , :][: number_of_samples ], dtype = torch . float32 , requires_grad = True ) y_train = torch . tensor ( y_noisy [ idx , :][: number_of_samples ], dtype = torch . float32 ) print ( X_train . shape , y_train . shape ) torch.Size([1000, 2]) torch.Size([1000, 1]) We now have a dataset which we can use. Let's plot, for a final time, the original dataset, the noisy set and the samples points: fig , axes = plt . subplots ( ncols = 3 , figsize = ( 15 , 4 )) im0 = axes [ 0 ] . contourf ( data [ 'x' ], data [ 't' ], np . real ( data [ 'u' ]), cmap = 'coolwarm' ) axes [ 0 ] . set_xlabel ( 'x' ) axes [ 0 ] . set_ylabel ( 't' ) axes [ 0 ] . set_title ( 'Ground truth' ) im1 = axes [ 1 ] . contourf ( data [ 'x' ], data [ 't' ], y_noisy . reshape ( data [ 'x' ] . shape ), cmap = 'coolwarm' ) axes [ 1 ] . set_xlabel ( 'x' ) axes [ 1 ] . set_title ( 'Noisy' ) sampled = np . array ([ y_noisy [ index , 0 ] if index in idx [: number_of_samples ] else np . nan for index in np . arange ( data [ 'x' ] . size )]) sampled = np . rot90 ( sampled . reshape ( data [ 'x' ] . shape )) #array needs to be rotated because of imshow im2 = axes [ 2 ] . imshow ( sampled , aspect = 'auto' , cmap = 'coolwarm' ) axes [ 2 ] . set_xlabel ( 'x' ) axes [ 2 ] . set_title ( 'Sampled' ) fig . colorbar ( im1 , ax = axes . ravel () . tolist ()) plt . show ()","title":"Example Burgers' equation"},{"location":"examples/Burgers/PDE_Burgers/#configuring-deepmod","text":"We now setup the options for DeepMoD. The setup requires the dimensions of the neural network, a library function and some args for the library function: ## Running DeepMoD config = { 'n_in' : 2 , 'hidden_dims' : [ 20 , 20 , 20 , 20 , 20 , 20 ], 'n_out' : 1 , 'library_function' : library_1D_in , 'library_args' :{ 'poly_order' : 1 , 'diff_order' : 2 }} Now we instantiate the model: model = DeepMod ( ** config ) optimizer = torch . optim . Adam ([{ 'params' : model . network_parameters (), 'lr' : 0.001 }, { 'params' : model . coeff_vector (), 'lr' : 0.005 }])","title":"Configuring DeepMoD"},{"location":"examples/Burgers/PDE_Burgers/#run-deepmod","text":"We can now run DeepMoD using all the options we have set and the training data. We need to slightly preprocess the input data for the derivatives: train_deepmod ( model , X_train , y_train , optimizer , 25000 , { 'l1' : 1e-5 }) | Iteration | Progress | Time remaining | Cost | MSE | Reg | L1 | 100 0 . 40 % 371 s 2 .29e-02 1 .64e-02 6 .41e-03 1 .25e-04 ------------------------------------------------------------------------- -- KeyboardInterrupt Traceback (most recent call last) < ipython - input - 21 - 790 b1a88b6d5 > in < module > ----> 1 train_deepmod ( model , X_train , y_train , optimizer , 25000 , { 'l1' : 1 e - 5 } ) ~/ Documents / GitHub / New_DeepMod_Simple / DeePyMoD_torch / src / deepymod_torch / training . py in train_deepmod ( model , data , target , optimizer , max_iterations , loss_func_args ) 67 '''Performs full deepmod cycle: trains model, thresholds and trains again for unbiased estimate. Updates model in-place.''' 68 # Train first cycle and get prediction ---> 69 train ( model , data , target , optimizer , max_iterations , loss_func_args ) 70 prediction , time_deriv_list , sparse_theta_list , coeff_vector_list = model ( data ) 71 ~/ Documents / GitHub / New_DeepMod_Simple / DeePyMoD_torch / src / deepymod_torch / training . py in train ( model , data , target , optimizer , max_iterations , loss_func_args ) 32 # Optimizer step 33 optimizer . zero_grad () ---> 34 loss . backward () 35 optimizer . step () 36 board . close () ~/ opt / anaconda3 / lib / python3 . 7 / site - packages / torch / tensor . py in backward ( self , gradient , retain_graph , create_graph ) 193 products . Defaults to `` False `` . 194 \"\" \" --> 195 torch . autograd . backward ( self , gradient , retain_graph , create_graph ) 196 197 def register_hook ( self , hook ): ~/ opt / anaconda3 / lib / python3 . 7 / site - packages / torch / autograd / __init__ . py in backward ( tensors , grad_tensors , retain_graph , create_graph , grad_variables ) 97 Variable . _execution_engine . run_backward ( 98 tensors , grad_tensors , retain_graph , create_graph , ---> 99 allow_unreachable = True ) # allow_unreachable flag 100 101 KeyboardInterrupt : Now that DeepMoD has converged, it has found the following numbers: print ( model . fit . sparsity_mask ) [tensor([2, 4])] print ( model . fit . coeff_vector [ 0 ]) Parameter containing: tensor([[ 0.0974], [-0.9905]], requires_grad=True)","title":"Run DeepMoD"},{"location":"examples/PDE_KdV/PDE_KdV/","text":"Example Korteweg de Vries equation In this notebook we provide a simple example of the DeepMoD algorithm by applying it on the KdV equation. # General imports import numpy as np import torch import matplotlib.pylab as plt # DeepMoD functions from deepymod import DeepMoD from deepymod.model.func_approx import NN from deepymod.model.library import Library1D from deepymod.model.constraint import LeastSquares from deepymod.model.sparse_estimators import Threshold , PDEFIND from deepymod.training import train from deepymod.training.sparsity_scheduler import TrainTestPeriodic from scipy.io import loadmat # Settings for reproducibility np . random . seed ( 42 ) torch . manual_seed ( 0 ) % load_ext autoreload % autoreload 2 Next, we prepare the dataset. data = np . load ( '../data/kdv.npy' , allow_pickle = True ) . item () print ( 'Shape of grid:' , data [ 'x' ] . shape ) Shape of grid: (512, 201) Let's plot it to get an idea of the data: fig , ax = plt . subplots () im = ax . contourf ( data [ 'x' ], data [ 't' ], np . real ( data [ 'u' ])) ax . set_xlabel ( 'x' ) ax . set_ylabel ( 't' ) fig . colorbar ( mappable = im ) plt . show () X = np . transpose (( data [ 't' ] . flatten (), data [ 'x' ] . flatten ())) y = np . real ( data [ 'u' ]) . reshape (( data [ 'u' ] . size , 1 )) print ( X . shape , y . shape ) (102912, 2) (102912, 1) As we can see, X X has 2 dimensions, \\{x, t\\} \\{x, t\\} , while y y has only one, \\{u\\} \\{u\\} . Always explicity set the shape (i.e. N\\times 1 N\\times 1 , not N N ) or you'll get errors. This dataset is noiseless, so let's add 2.5\\% 2.5\\% noise: noise_level = 0.025 y_noisy = y + noise_level * np . std ( y ) * np . random . randn ( y [:, 0 ] . size , 1 ) The dataset is also much larger than needed, so let's hussle it and pick out a 1000 samples: number_of_samples = 1000 idx = np . random . permutation ( y . shape [ 0 ]) X_train = torch . tensor ( X [ idx , :][: number_of_samples ], dtype = torch . float32 , requires_grad = True ) y_train = torch . tensor ( y_noisy [ idx , :][: number_of_samples ], dtype = torch . float32 ) print ( X_train . shape , y_train . shape ) torch.Size([1000, 2]) torch.Size([1000, 1]) We now have a dataset which we can use. Let's plot, for a final time, the original dataset, the noisy set and the samples points: fig , axes = plt . subplots ( ncols = 3 , figsize = ( 15 , 4 )) im0 = axes [ 0 ] . contourf ( data [ 'x' ], data [ 't' ], np . real ( data [ 'u' ]), cmap = 'coolwarm' ) axes [ 0 ] . set_xlabel ( 'x' ) axes [ 0 ] . set_ylabel ( 't' ) axes [ 0 ] . set_title ( 'Ground truth' ) im1 = axes [ 1 ] . contourf ( data [ 'x' ], data [ 't' ], y_noisy . reshape ( data [ 'x' ] . shape ), cmap = 'coolwarm' ) axes [ 1 ] . set_xlabel ( 'x' ) axes [ 1 ] . set_title ( 'Noisy' ) sampled = np . array ([ y_noisy [ index , 0 ] if index in idx [: number_of_samples ] else np . nan for index in np . arange ( data [ 'x' ] . size )]) sampled = np . rot90 ( sampled . reshape ( data [ 'x' ] . shape )) #array needs to be rotated because of imshow im2 = axes [ 2 ] . imshow ( sampled , aspect = 'auto' , cmap = 'coolwarm' ) axes [ 2 ] . set_xlabel ( 'x' ) axes [ 2 ] . set_title ( 'Sampled' ) fig . colorbar ( im1 , ax = axes . ravel () . tolist ()) plt . show () Configuring DeepMoD Configuration of the function approximator: Here the first argument is the number of input and the last argument the number of output layers. network = NN ( 2 , [ 50 , 50 , 50 , 50 ], 1 ) Configuration of the library function: We select athe library with a 2D spatial input. Note that that the max differential order has been pre-determined here out of convinience. So, for poly_order 1 the library contains the following 12 terms: * [ 1, u_x, u_{xx}, u_{xxx}, u, u u_{x}, u u_{xx}, u u_{xxx}, u^2, u^2 u_{x}, u^2 u_{xx}, u^2 u_{xxx} 1, u_x, u_{xx}, u_{xxx}, u, u u_{x}, u u_{xx}, u u_{xxx}, u^2, u^2 u_{x}, u^2 u_{xx}, u^2 u_{xxx} ] library = Library1D ( poly_order = 2 , diff_order = 3 ) Configuration of the sparsity estimator and sparsity scheduler used. In this case we use the most basic threshold-based Lasso estimator and a scheduler that asseses the validation loss after a given patience. If that value is smaller than 1e-5, the algorithm is converged. estimator = Threshold ( 0.1 ) sparsity_scheduler = TrainTestPeriodic ( periodicity = 50 , patience = 10 , delta = 1e-5 ) Configuration of the sparsity estimator constraint = LeastSquares () # Configuration of the sparsity scheduler Now we instantiate the model and select the optimizer model = DeepMoD ( network , library , estimator , constraint ) # Defining optimizer optimizer = torch . optim . Adam ( model . parameters (), betas = ( 0.99 , 0.99 ), amsgrad = True , lr = 1e-3 ) Run DeepMoD We can now run DeepMoD using all the options we have set and the training data: * The directory where the tensorboard file is written (log_dir) * The ratio of train/test set used (split) * The maximum number of iterations performed (max_iterations) * The absolute change in L1 norm considered converged (delta) * The amount of epochs over which the absolute change in L1 norm is calculated (patience) train ( model , X_train , y_train , optimizer , sparsity_scheduler , log_dir = 'runs/KDV/' , split = 0.8 , max_iterations = 100000 , delta = 1e-4 , patience = 8 ) | Iteration | Progress | Time remaining | Loss | MSE | Reg | L1 norm | 25775 25.77% 3510s 7.15e-06 6.44e-06 7.10e-07 2.88e+00 Algorithm converged. Stopping training. Sparsity masks provide the active and non-active terms in the PDE: model . sparsity_masks [tensor([False, False, False, True, False, True, False, False, False, False, False, False])] estimatior_coeffs gives the magnitude of the active terms: print ( model . estimator_coeffs ()) [array([[ 0. ], [ 0. ], [ 0. ], [-0.9088099], [ 0. ], [-1.7334794], [ 0. ], [ 0. ], [ 0. ], [ 0. ], [ 0. ], [ 0. ]], dtype=float32)]","title":"KdV"},{"location":"examples/PDE_KdV/PDE_KdV/#example-korteweg-de-vries-equation","text":"In this notebook we provide a simple example of the DeepMoD algorithm by applying it on the KdV equation. # General imports import numpy as np import torch import matplotlib.pylab as plt # DeepMoD functions from deepymod import DeepMoD from deepymod.model.func_approx import NN from deepymod.model.library import Library1D from deepymod.model.constraint import LeastSquares from deepymod.model.sparse_estimators import Threshold , PDEFIND from deepymod.training import train from deepymod.training.sparsity_scheduler import TrainTestPeriodic from scipy.io import loadmat # Settings for reproducibility np . random . seed ( 42 ) torch . manual_seed ( 0 ) % load_ext autoreload % autoreload 2 Next, we prepare the dataset. data = np . load ( '../data/kdv.npy' , allow_pickle = True ) . item () print ( 'Shape of grid:' , data [ 'x' ] . shape ) Shape of grid: (512, 201) Let's plot it to get an idea of the data: fig , ax = plt . subplots () im = ax . contourf ( data [ 'x' ], data [ 't' ], np . real ( data [ 'u' ])) ax . set_xlabel ( 'x' ) ax . set_ylabel ( 't' ) fig . colorbar ( mappable = im ) plt . show () X = np . transpose (( data [ 't' ] . flatten (), data [ 'x' ] . flatten ())) y = np . real ( data [ 'u' ]) . reshape (( data [ 'u' ] . size , 1 )) print ( X . shape , y . shape ) (102912, 2) (102912, 1) As we can see, X X has 2 dimensions, \\{x, t\\} \\{x, t\\} , while y y has only one, \\{u\\} \\{u\\} . Always explicity set the shape (i.e. N\\times 1 N\\times 1 , not N N ) or you'll get errors. This dataset is noiseless, so let's add 2.5\\% 2.5\\% noise: noise_level = 0.025 y_noisy = y + noise_level * np . std ( y ) * np . random . randn ( y [:, 0 ] . size , 1 ) The dataset is also much larger than needed, so let's hussle it and pick out a 1000 samples: number_of_samples = 1000 idx = np . random . permutation ( y . shape [ 0 ]) X_train = torch . tensor ( X [ idx , :][: number_of_samples ], dtype = torch . float32 , requires_grad = True ) y_train = torch . tensor ( y_noisy [ idx , :][: number_of_samples ], dtype = torch . float32 ) print ( X_train . shape , y_train . shape ) torch.Size([1000, 2]) torch.Size([1000, 1]) We now have a dataset which we can use. Let's plot, for a final time, the original dataset, the noisy set and the samples points: fig , axes = plt . subplots ( ncols = 3 , figsize = ( 15 , 4 )) im0 = axes [ 0 ] . contourf ( data [ 'x' ], data [ 't' ], np . real ( data [ 'u' ]), cmap = 'coolwarm' ) axes [ 0 ] . set_xlabel ( 'x' ) axes [ 0 ] . set_ylabel ( 't' ) axes [ 0 ] . set_title ( 'Ground truth' ) im1 = axes [ 1 ] . contourf ( data [ 'x' ], data [ 't' ], y_noisy . reshape ( data [ 'x' ] . shape ), cmap = 'coolwarm' ) axes [ 1 ] . set_xlabel ( 'x' ) axes [ 1 ] . set_title ( 'Noisy' ) sampled = np . array ([ y_noisy [ index , 0 ] if index in idx [: number_of_samples ] else np . nan for index in np . arange ( data [ 'x' ] . size )]) sampled = np . rot90 ( sampled . reshape ( data [ 'x' ] . shape )) #array needs to be rotated because of imshow im2 = axes [ 2 ] . imshow ( sampled , aspect = 'auto' , cmap = 'coolwarm' ) axes [ 2 ] . set_xlabel ( 'x' ) axes [ 2 ] . set_title ( 'Sampled' ) fig . colorbar ( im1 , ax = axes . ravel () . tolist ()) plt . show ()","title":"Example Korteweg de Vries equation"},{"location":"examples/PDE_KdV/PDE_KdV/#configuring-deepmod","text":"Configuration of the function approximator: Here the first argument is the number of input and the last argument the number of output layers. network = NN ( 2 , [ 50 , 50 , 50 , 50 ], 1 ) Configuration of the library function: We select athe library with a 2D spatial input. Note that that the max differential order has been pre-determined here out of convinience. So, for poly_order 1 the library contains the following 12 terms: * [ 1, u_x, u_{xx}, u_{xxx}, u, u u_{x}, u u_{xx}, u u_{xxx}, u^2, u^2 u_{x}, u^2 u_{xx}, u^2 u_{xxx} 1, u_x, u_{xx}, u_{xxx}, u, u u_{x}, u u_{xx}, u u_{xxx}, u^2, u^2 u_{x}, u^2 u_{xx}, u^2 u_{xxx} ] library = Library1D ( poly_order = 2 , diff_order = 3 ) Configuration of the sparsity estimator and sparsity scheduler used. In this case we use the most basic threshold-based Lasso estimator and a scheduler that asseses the validation loss after a given patience. If that value is smaller than 1e-5, the algorithm is converged. estimator = Threshold ( 0.1 ) sparsity_scheduler = TrainTestPeriodic ( periodicity = 50 , patience = 10 , delta = 1e-5 ) Configuration of the sparsity estimator constraint = LeastSquares () # Configuration of the sparsity scheduler Now we instantiate the model and select the optimizer model = DeepMoD ( network , library , estimator , constraint ) # Defining optimizer optimizer = torch . optim . Adam ( model . parameters (), betas = ( 0.99 , 0.99 ), amsgrad = True , lr = 1e-3 )","title":"Configuring DeepMoD"},{"location":"examples/PDE_KdV/PDE_KdV/#run-deepmod","text":"We can now run DeepMoD using all the options we have set and the training data: * The directory where the tensorboard file is written (log_dir) * The ratio of train/test set used (split) * The maximum number of iterations performed (max_iterations) * The absolute change in L1 norm considered converged (delta) * The amount of epochs over which the absolute change in L1 norm is calculated (patience) train ( model , X_train , y_train , optimizer , sparsity_scheduler , log_dir = 'runs/KDV/' , split = 0.8 , max_iterations = 100000 , delta = 1e-4 , patience = 8 ) | Iteration | Progress | Time remaining | Loss | MSE | Reg | L1 norm | 25775 25.77% 3510s 7.15e-06 6.44e-06 7.10e-07 2.88e+00 Algorithm converged. Stopping training. Sparsity masks provide the active and non-active terms in the PDE: model . sparsity_masks [tensor([False, False, False, True, False, True, False, False, False, False, False, False])] estimatior_coeffs gives the magnitude of the active terms: print ( model . estimator_coeffs ()) [array([[ 0. ], [ 0. ], [ 0. ], [-0.9088099], [ 0. ], [-1.7334794], [ 0. ], [ 0. ], [ 0. ], [ 0. ], [ 0. ], [ 0. ]], dtype=float32)]","title":"Run DeepMoD"},{"location":"examples/kdv/PDE_KdV/","text":"Example Korteweg de Vries equation In this notebook we provide a simple example of the DeepMoD algorithm by applying it on the Burgers' equation. We start by importing the required libraries and setting the plotting style: # General imports import numpy as np import torch import matplotlib.pylab as plt # DeepMoD stuff from deepymod_torch.DeepMod import DeepMod from deepymod_torch.library_functions import library_1D_in from deepymod_torch.training import train_deepmod , train_mse # Settings for reproducibility np . random . seed ( 42 ) torch . manual_seed ( 0 ) % load_ext autoreload % autoreload 2 Next, we prepare the dataset. data = np . load ( 'data/kdv.npy' , allow_pickle = True ) . item () print ( 'Shape of grid:' , data [ 'x' ] . shape ) Shape of grid: (512, 201) Let's plot it to get an idea of the data: fig , ax = plt . subplots () im = ax . contourf ( data [ 'x' ], data [ 't' ], np . real ( data [ 'u' ])) ax . set_xlabel ( 'x' ) ax . set_ylabel ( 't' ) fig . colorbar ( mappable = im ) plt . show () X = np . transpose (( data [ 't' ] . flatten (), data [ 'x' ] . flatten ())) y = np . real ( data [ 'u' ]) . reshape (( data [ 'u' ] . size , 1 )) print ( X . shape , y . shape ) (102912, 2) (102912, 1) As we can see, X X has 2 dimensions, \\{x, t\\} \\{x, t\\} , while y y has only one, \\{u\\} \\{u\\} . Always explicity set the shape (i.e. N\\times 1 N\\times 1 , not N N ) or you'll get errors. This dataset is noiseless, so let's add 5\\% 5\\% noise: noise_level = 0.05 y_noisy = y + noise_level * np . std ( y ) * np . random . randn ( y [:, 0 ] . size , 1 ) The dataset is also much larger than needed, so let's hussle it and pick out a 1000 samples: number_of_samples = 1000 idx = np . random . permutation ( y . shape [ 0 ]) X_train = torch . tensor ( X [ idx , :][: number_of_samples ], dtype = torch . float32 , requires_grad = True ) y_train = torch . tensor ( y_noisy [ idx , :][: number_of_samples ], dtype = torch . float32 ) print ( X_train . shape , y_train . shape ) torch.Size([1000, 2]) torch.Size([1000, 1]) We now have a dataset which we can use. Let's plot, for a final time, the original dataset, the noisy set and the samples points: fig , axes = plt . subplots ( ncols = 3 , figsize = ( 15 , 4 )) im0 = axes [ 0 ] . contourf ( data [ 'x' ], data [ 't' ], np . real ( data [ 'u' ]), cmap = 'coolwarm' ) axes [ 0 ] . set_xlabel ( 'x' ) axes [ 0 ] . set_ylabel ( 't' ) axes [ 0 ] . set_title ( 'Ground truth' ) im1 = axes [ 1 ] . contourf ( data [ 'x' ], data [ 't' ], y_noisy . reshape ( data [ 'x' ] . shape ), cmap = 'coolwarm' ) axes [ 1 ] . set_xlabel ( 'x' ) axes [ 1 ] . set_title ( 'Noisy' ) sampled = np . array ([ y_noisy [ index , 0 ] if index in idx [: number_of_samples ] else np . nan for index in np . arange ( data [ 'x' ] . size )]) sampled = np . rot90 ( sampled . reshape ( data [ 'x' ] . shape )) #array needs to be rotated because of imshow im2 = axes [ 2 ] . imshow ( sampled , aspect = 'auto' , cmap = 'coolwarm' ) axes [ 2 ] . set_xlabel ( 'x' ) axes [ 2 ] . set_title ( 'Sampled' ) fig . colorbar ( im1 , ax = axes . ravel () . tolist ()) plt . show () Configuring DeepMoD We now setup the options for DeepMoD. The setup requires the dimensions of the neural network, a library function and some args for the library function: ## Running DeepMoD config = { 'n_in' : 2 , 'hidden_dims' : [ 20 , 20 , 20 , 20 , 20 , 20 ], 'n_out' : 1 , 'library_function' : library_1D_in , 'library_args' :{ 'poly_order' : 1 , 'diff_order' : 3 }} Now we instantiate the model: model = DeepMod ( ** config ) optimizer = torch . optim . Adam ([{ 'params' : model . network_parameters (), 'lr' : 0.001 }, { 'params' : model . coeff_vector (), 'lr' : 0.0025 }], betas = ( 0.99 , 0.99 )) Run DeepMoD We can now run DeepMoD using all the options we have set and the training data. We need to slightly preprocess the input data for the derivatives: train_deepmod ( model , X_train , y_train , optimizer , 25000 , { 'l1' : 1e-5 }) | Iteration | Progress | Time remaining | Cost | MSE | Reg | L1 | 25000 100.00 % 0 s 5.83e-05 3.13e-05 4.58e-06 2.24e-05 [ Parameter containing : tensor ([[ - 0.7742 ], [ - 4.7608 ]], requires_grad = True )] [ tensor ([ 3 , 5 ])] | Iteration | Progress | Time remaining | Cost | MSE | Reg | L1 | 25000 100.00 % 0 s 9.18e-05 2.87e-05 6.32e-05 0.00e+00 Now that DeepMoD has converged, it has found the following numbers: print ( model . fit . sparsity_mask ) [tensor([3, 5])] print ( model . fit . coeff_vector [ 0 ]) Parameter containing: tensor([[-0.7742], [-4.7608]], requires_grad=True)","title":"Example Korteweg de Vries equation"},{"location":"examples/kdv/PDE_KdV/#example-korteweg-de-vries-equation","text":"In this notebook we provide a simple example of the DeepMoD algorithm by applying it on the Burgers' equation. We start by importing the required libraries and setting the plotting style: # General imports import numpy as np import torch import matplotlib.pylab as plt # DeepMoD stuff from deepymod_torch.DeepMod import DeepMod from deepymod_torch.library_functions import library_1D_in from deepymod_torch.training import train_deepmod , train_mse # Settings for reproducibility np . random . seed ( 42 ) torch . manual_seed ( 0 ) % load_ext autoreload % autoreload 2 Next, we prepare the dataset. data = np . load ( 'data/kdv.npy' , allow_pickle = True ) . item () print ( 'Shape of grid:' , data [ 'x' ] . shape ) Shape of grid: (512, 201) Let's plot it to get an idea of the data: fig , ax = plt . subplots () im = ax . contourf ( data [ 'x' ], data [ 't' ], np . real ( data [ 'u' ])) ax . set_xlabel ( 'x' ) ax . set_ylabel ( 't' ) fig . colorbar ( mappable = im ) plt . show () X = np . transpose (( data [ 't' ] . flatten (), data [ 'x' ] . flatten ())) y = np . real ( data [ 'u' ]) . reshape (( data [ 'u' ] . size , 1 )) print ( X . shape , y . shape ) (102912, 2) (102912, 1) As we can see, X X has 2 dimensions, \\{x, t\\} \\{x, t\\} , while y y has only one, \\{u\\} \\{u\\} . Always explicity set the shape (i.e. N\\times 1 N\\times 1 , not N N ) or you'll get errors. This dataset is noiseless, so let's add 5\\% 5\\% noise: noise_level = 0.05 y_noisy = y + noise_level * np . std ( y ) * np . random . randn ( y [:, 0 ] . size , 1 ) The dataset is also much larger than needed, so let's hussle it and pick out a 1000 samples: number_of_samples = 1000 idx = np . random . permutation ( y . shape [ 0 ]) X_train = torch . tensor ( X [ idx , :][: number_of_samples ], dtype = torch . float32 , requires_grad = True ) y_train = torch . tensor ( y_noisy [ idx , :][: number_of_samples ], dtype = torch . float32 ) print ( X_train . shape , y_train . shape ) torch.Size([1000, 2]) torch.Size([1000, 1]) We now have a dataset which we can use. Let's plot, for a final time, the original dataset, the noisy set and the samples points: fig , axes = plt . subplots ( ncols = 3 , figsize = ( 15 , 4 )) im0 = axes [ 0 ] . contourf ( data [ 'x' ], data [ 't' ], np . real ( data [ 'u' ]), cmap = 'coolwarm' ) axes [ 0 ] . set_xlabel ( 'x' ) axes [ 0 ] . set_ylabel ( 't' ) axes [ 0 ] . set_title ( 'Ground truth' ) im1 = axes [ 1 ] . contourf ( data [ 'x' ], data [ 't' ], y_noisy . reshape ( data [ 'x' ] . shape ), cmap = 'coolwarm' ) axes [ 1 ] . set_xlabel ( 'x' ) axes [ 1 ] . set_title ( 'Noisy' ) sampled = np . array ([ y_noisy [ index , 0 ] if index in idx [: number_of_samples ] else np . nan for index in np . arange ( data [ 'x' ] . size )]) sampled = np . rot90 ( sampled . reshape ( data [ 'x' ] . shape )) #array needs to be rotated because of imshow im2 = axes [ 2 ] . imshow ( sampled , aspect = 'auto' , cmap = 'coolwarm' ) axes [ 2 ] . set_xlabel ( 'x' ) axes [ 2 ] . set_title ( 'Sampled' ) fig . colorbar ( im1 , ax = axes . ravel () . tolist ()) plt . show ()","title":"Example Korteweg de Vries equation"},{"location":"examples/kdv/PDE_KdV/#configuring-deepmod","text":"We now setup the options for DeepMoD. The setup requires the dimensions of the neural network, a library function and some args for the library function: ## Running DeepMoD config = { 'n_in' : 2 , 'hidden_dims' : [ 20 , 20 , 20 , 20 , 20 , 20 ], 'n_out' : 1 , 'library_function' : library_1D_in , 'library_args' :{ 'poly_order' : 1 , 'diff_order' : 3 }} Now we instantiate the model: model = DeepMod ( ** config ) optimizer = torch . optim . Adam ([{ 'params' : model . network_parameters (), 'lr' : 0.001 }, { 'params' : model . coeff_vector (), 'lr' : 0.0025 }], betas = ( 0.99 , 0.99 ))","title":"Configuring DeepMoD"},{"location":"examples/kdv/PDE_KdV/#run-deepmod","text":"We can now run DeepMoD using all the options we have set and the training data. We need to slightly preprocess the input data for the derivatives: train_deepmod ( model , X_train , y_train , optimizer , 25000 , { 'l1' : 1e-5 }) | Iteration | Progress | Time remaining | Cost | MSE | Reg | L1 | 25000 100.00 % 0 s 5.83e-05 3.13e-05 4.58e-06 2.24e-05 [ Parameter containing : tensor ([[ - 0.7742 ], [ - 4.7608 ]], requires_grad = True )] [ tensor ([ 3 , 5 ])] | Iteration | Progress | Time remaining | Cost | MSE | Reg | L1 | 25000 100.00 % 0 s 9.18e-05 2.87e-05 6.32e-05 0.00e+00 Now that DeepMoD has converged, it has found the following numbers: print ( model . fit . sparsity_mask ) [tensor([3, 5])] print ( model . fit . coeff_vector [ 0 ]) Parameter containing: tensor([[-0.7742], [-4.7608]], requires_grad=True)","title":"Run DeepMoD"}]}