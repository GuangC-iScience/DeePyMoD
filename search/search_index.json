{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Documentation page for the Deep learning based Model Discovery package DeepMoD. DeePyMoD is a PyTorch-based implementation of the DeepMoD algorithm for model discovery of PDEs and ODEs. github.com/PhIMaL/DeePyMoD . This work is based on two papers: The original DeepMoD paper arXiv:1904.09406 , presenting the foundation of this neural network driven model discovery and a follow-up paper [xxx] describing a modular plug and play framework. Summary DeepMoD is a modular model discovery framewrok aimed at discovering the ODE/PDE underlying a spatio-temporal dataset. Essentially the framework is comprised of four components: Function approximator, e.g. a neural network to represent the dataset, Function library on which the model discovery is performed, Constraint function that constrains the neural network with the obtained solution Sparsity selection algorithm.","title":"Home"},{"location":"#summary","text":"DeepMoD is a modular model discovery framewrok aimed at discovering the ODE/PDE underlying a spatio-temporal dataset. Essentially the framework is comprised of four components: Function approximator, e.g. a neural network to represent the dataset, Function library on which the model discovery is performed, Constraint function that constrains the neural network with the obtained solution Sparsity selection algorithm.","title":"Summary"},{"location":"api/analysis/","text":"load_tensorboard ( path ) Loads tensorboard files into a pandas dataframe. Assumes one run per folder! Parameters: Name Type Description Default path str path of folder with tensorboard files. required Returns: Type Description DataFrame DataFrame: Pandas dataframe with all run data. Source code in deepymod/analysis/load_tensorboard.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 def load_tensorboard ( path : str ) -> pd . DataFrame : \"\"\" Loads tensorboard files into a pandas dataframe. Assumes one run per folder! Args: path (string): path of folder with tensorboard files. Returns: DataFrame: Pandas dataframe with all run data. \"\"\" event_paths = [ file for file in os . walk ( path , topdown = True ) if file [ 2 ][ 0 ][: len ( 'events' )] == 'events' ] df = pd . DataFrame () steps = None # steps are the same for all files for event_idx , path in enumerate ( event_paths ): summary_iterator = EventAccumulator ( os . path . join ( path [ 0 ], path [ 2 ][ 0 ])) . Reload () tags = summary_iterator . Tags ()[ 'scalars' ] data = [[ event . value for event in summary_iterator . Scalars ( tag )] for tag in tags ] if steps is None : steps = [ event . step for event in summary_iterator . Scalars ( tags [ 0 ])] # Adding to dataframe tags = [ tag . replace ( '/' , '_' ) for tag in tags ] # for name consistency if event_idx > 0 : # We have one file in the top level, so after we need to use folder name tags = [ path [ 0 ] . split ( '/' )[ - 1 ]] for idx , tag in enumerate ( tags ): df [ tag ] = data [ idx ] df . index = steps return df","title":"Analysis"},{"location":"api/analysis/#deepymod.analysis.load_tensorboard","text":"","title":"deepymod.analysis.load_tensorboard"},{"location":"api/analysis/#deepymod.analysis.load_tensorboard.load_tensorboard","text":"Loads tensorboard files into a pandas dataframe. Assumes one run per folder! Parameters: Name Type Description Default path str path of folder with tensorboard files. required Returns: Type Description DataFrame DataFrame: Pandas dataframe with all run data. Source code in deepymod/analysis/load_tensorboard.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 def load_tensorboard ( path : str ) -> pd . DataFrame : \"\"\" Loads tensorboard files into a pandas dataframe. Assumes one run per folder! Args: path (string): path of folder with tensorboard files. Returns: DataFrame: Pandas dataframe with all run data. \"\"\" event_paths = [ file for file in os . walk ( path , topdown = True ) if file [ 2 ][ 0 ][: len ( 'events' )] == 'events' ] df = pd . DataFrame () steps = None # steps are the same for all files for event_idx , path in enumerate ( event_paths ): summary_iterator = EventAccumulator ( os . path . join ( path [ 0 ], path [ 2 ][ 0 ])) . Reload () tags = summary_iterator . Tags ()[ 'scalars' ] data = [[ event . value for event in summary_iterator . Scalars ( tag )] for tag in tags ] if steps is None : steps = [ event . step for event in summary_iterator . Scalars ( tags [ 0 ])] # Adding to dataframe tags = [ tag . replace ( '/' , '_' ) for tag in tags ] # for name consistency if event_idx > 0 : # We have one file in the top level, so after we need to use folder name tags = [ path [ 0 ] . split ( '/' )[ - 1 ]] for idx , tag in enumerate ( tags ): df [ tag ] = data [ idx ] df . index = steps return df","title":"load_tensorboard()"},{"location":"api/constraint/","text":"This module contains concrete implementations of the constraint component. GradParams [Summary] calculate_coeffs ( self , sparse_thetas , time_derivs ) [Summary] Parameters: Name Type Description Default sparse_thetas TensorList List containing the sparse feature tensors. required time_derivs TensorList List containing the time derivatives. required Returns: Type Description (TensorList) Calculated coefficients. Source code in deepymod/model/constraint.py 45 46 47 48 49 50 51 52 53 54 def calculate_coeffs ( self , sparse_thetas , time_derivs ): \"\"\"[Summary] Args: sparse_thetas (TensorList): List containing the sparse feature tensors. time_derivs (TensorList): List containing the time derivatives. Returns: (TensorList): Calculated coefficients. \"\"\" return self . coeff_vectors LeastSquares Implements the constraint as a least squares problem solved by QR decomposition. calculate_coeffs ( self , sparse_thetas , time_derivs ) Calculates the coefficients of the constraint using the QR decomposition for every pair of sparse feature matrix and time derivative. Parameters: Name Type Description Default sparse_thetas List[torch.Tensor] List containing the sparse feature tensors. required time_derivs List[torch.Tensor] List containing the time derivatives. required Returns: Type Description List[torch.Tensor] (TensorList): Calculated coefficients. Source code in deepymod/model/constraint.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 def calculate_coeffs ( self , sparse_thetas : TensorList , time_derivs : TensorList ) -> TensorList : \"\"\"Calculates the coefficients of the constraint using the QR decomposition for every pair of sparse feature matrix and time derivative. Args: sparse_thetas (TensorList): List containing the sparse feature tensors. time_derivs (TensorList): List containing the time derivatives. Returns: (TensorList): Calculated coefficients. \"\"\" opt_coeff = [] for theta , dt in zip ( sparse_thetas , time_derivs ): Q , R = torch . qr ( theta ) # solution of lst. sq. by QR decomp. opt_coeff . append ( torch . inverse ( R ) @ Q . T @ dt ) # Putting them in the right spot coeff_vectors = [ torch . zeros (( mask . shape [ 0 ], 1 )) . to ( coeff_vector . device ) . masked_scatter_ ( mask [:, None ], coeff_vector ) for mask , coeff_vector in zip ( self . sparsity_masks , opt_coeff )] return coeff_vectors","title":"Constraints"},{"location":"api/constraint/#deepymod.model.constraint","text":"This module contains concrete implementations of the constraint component.","title":"deepymod.model.constraint"},{"location":"api/constraint/#deepymod.model.constraint.GradParams","text":"[Summary]","title":"GradParams"},{"location":"api/constraint/#deepymod.model.constraint.GradParams.calculate_coeffs","text":"[Summary] Parameters: Name Type Description Default sparse_thetas TensorList List containing the sparse feature tensors. required time_derivs TensorList List containing the time derivatives. required Returns: Type Description (TensorList) Calculated coefficients. Source code in deepymod/model/constraint.py 45 46 47 48 49 50 51 52 53 54 def calculate_coeffs ( self , sparse_thetas , time_derivs ): \"\"\"[Summary] Args: sparse_thetas (TensorList): List containing the sparse feature tensors. time_derivs (TensorList): List containing the time derivatives. Returns: (TensorList): Calculated coefficients. \"\"\" return self . coeff_vectors","title":"calculate_coeffs()"},{"location":"api/constraint/#deepymod.model.constraint.LeastSquares","text":"Implements the constraint as a least squares problem solved by QR decomposition.","title":"LeastSquares"},{"location":"api/constraint/#deepymod.model.constraint.LeastSquares.calculate_coeffs","text":"Calculates the coefficients of the constraint using the QR decomposition for every pair of sparse feature matrix and time derivative. Parameters: Name Type Description Default sparse_thetas List[torch.Tensor] List containing the sparse feature tensors. required time_derivs List[torch.Tensor] List containing the time derivatives. required Returns: Type Description List[torch.Tensor] (TensorList): Calculated coefficients. Source code in deepymod/model/constraint.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 def calculate_coeffs ( self , sparse_thetas : TensorList , time_derivs : TensorList ) -> TensorList : \"\"\"Calculates the coefficients of the constraint using the QR decomposition for every pair of sparse feature matrix and time derivative. Args: sparse_thetas (TensorList): List containing the sparse feature tensors. time_derivs (TensorList): List containing the time derivatives. Returns: (TensorList): Calculated coefficients. \"\"\" opt_coeff = [] for theta , dt in zip ( sparse_thetas , time_derivs ): Q , R = torch . qr ( theta ) # solution of lst. sq. by QR decomp. opt_coeff . append ( torch . inverse ( R ) @ Q . T @ dt ) # Putting them in the right spot coeff_vectors = [ torch . zeros (( mask . shape [ 0 ], 1 )) . to ( coeff_vector . device ) . masked_scatter_ ( mask [:, None ], coeff_vector ) for mask , coeff_vector in zip ( self . sparsity_masks , opt_coeff )] return coeff_vectors","title":"calculate_coeffs()"},{"location":"api/convergence/","text":"Convergence Implements convergence criterium. Convergence is when change in patience epochs is smaller than delta. __call__ ( self , iteration , l1_norm ) special Parameters: Name Type Description Default epoch int Current epoch of the optimization required l1_norm Tensor Value of the L1 norm required Source code in deepymod/training/convergence.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 def __call__ ( self , iteration : int , l1_norm : torch . Tensor ) -> bool : \"\"\" Args: epoch (int): Current epoch of the optimization l1_norm (torch.Tensor): Value of the L1 norm \"\"\" converged = False # overwrite later # Initialize if doesn't exist if self . start_l1 is None : self . start_l1 = l1_norm self . start_iteration = iteration # Check if change is smaller than delta and if we've exceeded patience elif torch . abs ( self . start_l1 - l1_norm ) . item () < self . delta : if ( iteration - self . start_iteration ) >= self . patience : converged = True # If not, reset and keep going else : self . start_l1 = l1_norm self . start_iteration = iteration return converged","title":"Convergence"},{"location":"api/convergence/#deepymod.training.convergence","text":"","title":"deepymod.training.convergence"},{"location":"api/convergence/#deepymod.training.convergence.Convergence","text":"Implements convergence criterium. Convergence is when change in patience epochs is smaller than delta.","title":"Convergence"},{"location":"api/convergence/#deepymod.training.convergence.Convergence.__call__","text":"Parameters: Name Type Description Default epoch int Current epoch of the optimization required l1_norm Tensor Value of the L1 norm required Source code in deepymod/training/convergence.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 def __call__ ( self , iteration : int , l1_norm : torch . Tensor ) -> bool : \"\"\" Args: epoch (int): Current epoch of the optimization l1_norm (torch.Tensor): Value of the L1 norm \"\"\" converged = False # overwrite later # Initialize if doesn't exist if self . start_l1 is None : self . start_l1 = l1_norm self . start_iteration = iteration # Check if change is smaller than delta and if we've exceeded patience elif torch . abs ( self . start_l1 - l1_norm ) . item () < self . delta : if ( iteration - self . start_iteration ) >= self . patience : converged = True # If not, reset and keep going else : self . start_l1 = l1_norm self . start_iteration = iteration return converged","title":"__call__()"},{"location":"api/deepmod/","text":"This file contains the four building blocks for the deepmod framework: 1) Function approximator, e.g. a neural network to represent the dataset, XXXX Not present yet 2) Function library on which the model discovery is performed, 3) Constraint function that constrains the neural network with the obtained solution 4) Sparsity selection algorithm. These are all abstract classes and implement the flow logic, rather than the specifics. Constraint Abstract class implementing the constraint set to the function approximator. !!! args nn (PyTorch Class): Module of the function approximator, typically a neural network. apply_mask ( self , thetas ) Function that applies the sparsity mask to the library function. Parameters: Name Type Description Default thetas <function NewType.<locals>.new_type at 0x7fa552e0bd40> List of library functions, one for every output. required Returns: Type Description <function NewType.<locals>.new_type at 0x7fa552e0bd40> TensorList: The sparse version of the library function. Source code in deepymod/model/deepmod.py 43 44 45 46 47 48 49 50 51 52 53 def apply_mask ( self , thetas : TensorList ) -> TensorList : \"\"\" Function that applies the sparsity mask to the library function. Args: thetas (TensorList): List of library functions, one for every output. Returns: TensorList: The sparse version of the library function. \"\"\" sparse_thetas = [ theta [:, sparsity_mask ] for theta , sparsity_mask in zip ( thetas , self . sparsity_masks )] return sparse_thetas calculate_coeffs ( self , sparse_thetas , time_derivs ) Abstract method to compute the coefficients for the library. Parameters: Name Type Description Default sparse_thetas <function NewType.<locals>.new_type at 0x7fa552e0bd40> List of sparsified library functions, one for every output. required time_derivs <function NewType.<locals>.new_type at 0x7fa552e0bd40> List of time derivatives required Returns: Type Description <function NewType.<locals>.new_type at 0x7fa552e0bd40> Coefficients (TensorList): Return the coefficients. Source code in deepymod/model/deepmod.py 55 56 57 58 59 60 61 62 63 64 65 @abstractmethod def calculate_coeffs ( self , sparse_thetas : TensorList , time_derivs : TensorList ) -> TensorList : \"\"\"Abstract method to compute the coefficients for the library. Args: sparse_thetas (TensorList): List of sparsified library functions, one for every output. time_derivs (TensorList): List of time derivatives Returns: Coefficients (TensorList): Return the coefficients. \"\"\" pass forward ( self , input ) Updates the coefficient vector for a given estimation of the library function and time derivatives. Parameters: Name Type Description Default input Tuple[TensorList, TensorList] Tuple of tensors, containing an estimate of the time derivatives and the library function required Source code in deepymod/model/deepmod.py 28 29 30 31 32 33 34 35 36 37 38 39 40 def forward ( self , input : Tuple [ TensorList , TensorList ]) -> Tuple [ TensorList , TensorList ]: \"\"\"Updates the coefficient vector for a given estimation of the library function and time derivatives. Args: input (Tuple[TensorList, TensorList]): Tuple of tensors, containing an estimate of the time derivatives and the library function \"\"\" time_derivs , thetas = input if self . sparsity_masks is None : self . sparsity_masks = [ torch . ones ( theta . shape [ 1 ], dtype = torch . bool ) . to ( theta . device ) for theta in thetas ] sparse_thetas = self . apply_mask ( thetas ) self . coeff_vectors = self . calculate_coeffs ( sparse_thetas , time_derivs ) DeepMoD DeepMoD class integrating the various buiding blocks of the algorithm. It performs a function approximation of the data, calculates the library and time-derivatives thereof, constrains the function approximator to the obtained solution and applies the sparisty pattern of the underlying PDE. !!! args nn (PyTorch Class): Module of the function approximator, typically a neural network. sparsity_masks property readonly The sparsity mask defines which library terms are used and masked constraint_coeffs ( self , scaled = False , sparse = False ) Get the coefficients of the constraint Parameters: Name Type Description Default scaled bool Determine whether or not the coefficients should be normalized False sparse bool Apply the sparsity mask to the coefficients or not False Returns: coeff_vectors (TensorList): list with the coefficients Source code in deepymod/model/deepmod.py 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 def constraint_coeffs ( self , scaled = False , sparse = False ): \"\"\" Get the coefficients of the constraint Args: scaled (bool): Determine whether or not the coefficients should be normalized sparse (bool): Apply the sparsity mask to the coefficients or not Returns: coeff_vectors (TensorList): list with the coefficients \"\"\" coeff_vectors = self . constraint . coeff_vectors if scaled : coeff_vectors = [ coeff / norm [:, None ] for coeff , norm , mask in zip ( coeff_vectors , self . library . norms , self . sparsity_masks )] if sparse : coeff_vectors = [ sparsity_mask [:, None ] * coeff for sparsity_mask , coeff in zip ( self . sparsity_masks , coeff_vectors )] return coeff_vectors estimator_coeffs ( self ) Get the coefficients of the sparsity estimator Returns: Type Description Coefficient vector (TensorList) The sparse coefficient estimator Source code in deepymod/model/deepmod.py 190 191 192 193 194 195 196 197 def estimator_coeffs ( self ): \"\"\" Get the coefficients of the sparsity estimator Returns: Coefficient vector (TensorList): The sparse coefficient estimator \"\"\" coeff_vectors = self . sparse_estimator . coeff_vectors return coeff_vectors forward ( self , input ) [summary] Parameters: Name Type Description Default input Tensor Tensor of shape (n_samples x (n_spatial + 1)) containing the coordinates, first column should be the time coordinate. required Returns: Type Description Tuple[TensorList, TensorList, TensorList] Tuple[TensorList, TensorList, TensorList]: Tuple of tensors containing a tensor of shape (n_samples x n_features) containing the target data, a tensor of the time derivative of the data and the function library. Source code in deepymod/model/deepmod.py 171 172 173 174 175 176 177 178 179 180 181 182 183 def forward ( self , input : torch . Tensor ) -> Tuple [ TensorList , TensorList , TensorList ]: \"\"\"[summary] Args: input (torch.Tensor): Tensor of shape (n_samples x (n_spatial + 1)) containing the coordinates, first column should be the time coordinate. Returns: Tuple[TensorList, TensorList, TensorList]: Tuple of tensors containing a tensor of shape (n_samples x n_features) containing the target data, a tensor of the time derivative of the data and the function library. \"\"\" prediction , coordinates = self . func_approx ( input ) time_derivs , thetas = self . library (( prediction , coordinates )) self . constraint (( time_derivs , thetas )) return prediction , time_derivs , thetas Estimator Abstract class implementing the sparsity estimator set to the function approximator. !!! args nn (PyTorch Class): Module of the function approximator, typically a neural network. fit ( self , X , y ) Abstract method to compute the result of the network based on data X and labels y Args: x (np.ndarray): input data y (np.ndarray): output labels Returns: Type Description ndarray coefficients (np.ndarray) Source code in deepymod/model/deepmod.py 101 102 103 104 105 106 107 108 109 110 111 @abstractmethod def fit ( self , X : np . ndarray , y : np . ndarray ) -> np . ndarray : \"\"\" Abstract method to compute the result of the network based on data X and labels y Args: x (np.ndarray): input data y (np.ndarray): output labels Returns: coefficients (np.ndarray) \"\"\" pass forward ( self , thetas , time_derivs ) This function nomalized the library and time derivatives and calculates the corresponding sparisity mask. Parameters: Name Type Description Default thetas <function NewType.<locals>.new_type at 0x7fa552e0bd40> List of library functions, one for every output. required time_derivs <function NewType.<locals>.new_type at 0x7fa552e0bd40> List of time derivates of the data, one for every output. required Returns: Type Description <function NewType.<locals>.new_type at 0x7fa552e0bd40> sparsity_mask (TensorList): A list of sparsity masks, one for every output. Source code in deepymod/model/deepmod.py 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 def forward ( self , thetas : TensorList , time_derivs : TensorList ) -> TensorList : \"\"\"This function nomalized the library and time derivatives and calculates the corresponding sparisity mask. Args: thetas (TensorList): List of library functions, one for every output. time_derivs (TensorList): List of time derivates of the data, one for every output. Returns: sparsity_mask (TensorList): A list of sparsity masks, one for every output. \"\"\" # we first normalize theta and the time deriv with torch . no_grad (): normed_time_derivs = [( time_deriv / torch . norm ( time_deriv )) . detach () . cpu () . numpy () for time_deriv in time_derivs ] normed_thetas = [( theta / torch . norm ( theta , dim = 0 , keepdim = True )) . detach () . cpu () . numpy () for theta in thetas ] self . coeff_vectors = [ self . fit ( theta , time_deriv . squeeze ())[:, None ] for theta , time_deriv in zip ( normed_thetas , normed_time_derivs )] sparsity_masks = [ torch . tensor ( coeff_vector != 0.0 , dtype = torch . bool ) . squeeze () . to ( thetas [ 0 ] . device ) # move to gpu if required for coeff_vector in self . coeff_vectors ] return sparsity_masks Library Abstract class that calculates the library function and time derivatives. !!! args nn (PyTorch Class): Module of the function approximator, typically a neural network. forward ( self , input ) Compute the library (time derivatives and thetas) given a dataset Parameters: Name Type Description Default input Tuple[TensorList, TensorList] tuple with firstly the predicted values and required Returns: Type Description Tuple[TensorList, TensorList] Tuple[TensorList, TensorList]: The time derivatives and thetas Source code in deepymod/model/deepmod.py 124 125 126 127 128 129 130 131 132 133 134 135 136 def forward ( self , input : Tuple [ TensorList , TensorList ]) -> Tuple [ TensorList , TensorList ]: \"\"\"Compute the library (time derivatives and thetas) given a dataset Args: input (Tuple[TensorList, TensorList]): tuple with firstly the predicted values and secondly the respective coordinates for the prediction Returns: Tuple[TensorList, TensorList]: The time derivatives and thetas \"\"\" time_derivs , thetas = self . library ( input ) self . norms = [( torch . norm ( time_deriv ) / torch . norm ( theta , dim = 0 , keepdim = True )) . detach () . squeeze () for time_deriv , theta in zip ( time_derivs , thetas )] return time_derivs , thetas library ( self , input ) Abstract method that should compute the library: Parameters: Name Type Description Default input Tuple[torch.Tensor, torch.Tensor] predictions and coordinates for which to evaluate required Returns: Type Description Tuple[TensorList, TensorList] (Tuple[TensorList, TensorList]): The time derivatives and thetas Source code in deepymod/model/deepmod.py 138 139 140 141 142 143 144 145 146 147 148 149 @abstractmethod def library ( self , input : Tuple [ torch . Tensor , torch . Tensor ]) -> Tuple [ TensorList , TensorList ]: \"\"\"Abstract method that should compute the library: Args: input (Tuple[TensorList, TensorList]): predictions and coordinates for which to evaluate the library. Returns: (Tuple[TensorList, TensorList]): The time derivatives and thetas \"\"\" pass","title":"DeepMoD"},{"location":"api/deepmod/#deepymod.model.deepmod","text":"This file contains the four building blocks for the deepmod framework: 1) Function approximator, e.g. a neural network to represent the dataset, XXXX Not present yet 2) Function library on which the model discovery is performed, 3) Constraint function that constrains the neural network with the obtained solution 4) Sparsity selection algorithm. These are all abstract classes and implement the flow logic, rather than the specifics.","title":"deepymod.model.deepmod"},{"location":"api/deepmod/#deepymod.model.deepmod.Constraint","text":"Abstract class implementing the constraint set to the function approximator. !!! args nn (PyTorch Class): Module of the function approximator, typically a neural network.","title":"Constraint"},{"location":"api/deepmod/#deepymod.model.deepmod.Constraint.apply_mask","text":"Function that applies the sparsity mask to the library function. Parameters: Name Type Description Default thetas <function NewType.<locals>.new_type at 0x7fa552e0bd40> List of library functions, one for every output. required Returns: Type Description <function NewType.<locals>.new_type at 0x7fa552e0bd40> TensorList: The sparse version of the library function. Source code in deepymod/model/deepmod.py 43 44 45 46 47 48 49 50 51 52 53 def apply_mask ( self , thetas : TensorList ) -> TensorList : \"\"\" Function that applies the sparsity mask to the library function. Args: thetas (TensorList): List of library functions, one for every output. Returns: TensorList: The sparse version of the library function. \"\"\" sparse_thetas = [ theta [:, sparsity_mask ] for theta , sparsity_mask in zip ( thetas , self . sparsity_masks )] return sparse_thetas","title":"apply_mask()"},{"location":"api/deepmod/#deepymod.model.deepmod.Constraint.calculate_coeffs","text":"Abstract method to compute the coefficients for the library. Parameters: Name Type Description Default sparse_thetas <function NewType.<locals>.new_type at 0x7fa552e0bd40> List of sparsified library functions, one for every output. required time_derivs <function NewType.<locals>.new_type at 0x7fa552e0bd40> List of time derivatives required Returns: Type Description <function NewType.<locals>.new_type at 0x7fa552e0bd40> Coefficients (TensorList): Return the coefficients. Source code in deepymod/model/deepmod.py 55 56 57 58 59 60 61 62 63 64 65 @abstractmethod def calculate_coeffs ( self , sparse_thetas : TensorList , time_derivs : TensorList ) -> TensorList : \"\"\"Abstract method to compute the coefficients for the library. Args: sparse_thetas (TensorList): List of sparsified library functions, one for every output. time_derivs (TensorList): List of time derivatives Returns: Coefficients (TensorList): Return the coefficients. \"\"\" pass","title":"calculate_coeffs()"},{"location":"api/deepmod/#deepymod.model.deepmod.Constraint.forward","text":"Updates the coefficient vector for a given estimation of the library function and time derivatives. Parameters: Name Type Description Default input Tuple[TensorList, TensorList] Tuple of tensors, containing an estimate of the time derivatives and the library function required Source code in deepymod/model/deepmod.py 28 29 30 31 32 33 34 35 36 37 38 39 40 def forward ( self , input : Tuple [ TensorList , TensorList ]) -> Tuple [ TensorList , TensorList ]: \"\"\"Updates the coefficient vector for a given estimation of the library function and time derivatives. Args: input (Tuple[TensorList, TensorList]): Tuple of tensors, containing an estimate of the time derivatives and the library function \"\"\" time_derivs , thetas = input if self . sparsity_masks is None : self . sparsity_masks = [ torch . ones ( theta . shape [ 1 ], dtype = torch . bool ) . to ( theta . device ) for theta in thetas ] sparse_thetas = self . apply_mask ( thetas ) self . coeff_vectors = self . calculate_coeffs ( sparse_thetas , time_derivs )","title":"forward()"},{"location":"api/deepmod/#deepymod.model.deepmod.DeepMoD","text":"DeepMoD class integrating the various buiding blocks of the algorithm. It performs a function approximation of the data, calculates the library and time-derivatives thereof, constrains the function approximator to the obtained solution and applies the sparisty pattern of the underlying PDE. !!! args nn (PyTorch Class): Module of the function approximator, typically a neural network.","title":"DeepMoD"},{"location":"api/deepmod/#deepymod.model.deepmod.DeepMoD.sparsity_masks","text":"The sparsity mask defines which library terms are used and masked","title":"sparsity_masks"},{"location":"api/deepmod/#deepymod.model.deepmod.DeepMoD.constraint_coeffs","text":"Get the coefficients of the constraint Parameters: Name Type Description Default scaled bool Determine whether or not the coefficients should be normalized False sparse bool Apply the sparsity mask to the coefficients or not False Returns: coeff_vectors (TensorList): list with the coefficients Source code in deepymod/model/deepmod.py 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 def constraint_coeffs ( self , scaled = False , sparse = False ): \"\"\" Get the coefficients of the constraint Args: scaled (bool): Determine whether or not the coefficients should be normalized sparse (bool): Apply the sparsity mask to the coefficients or not Returns: coeff_vectors (TensorList): list with the coefficients \"\"\" coeff_vectors = self . constraint . coeff_vectors if scaled : coeff_vectors = [ coeff / norm [:, None ] for coeff , norm , mask in zip ( coeff_vectors , self . library . norms , self . sparsity_masks )] if sparse : coeff_vectors = [ sparsity_mask [:, None ] * coeff for sparsity_mask , coeff in zip ( self . sparsity_masks , coeff_vectors )] return coeff_vectors","title":"constraint_coeffs()"},{"location":"api/deepmod/#deepymod.model.deepmod.DeepMoD.estimator_coeffs","text":"Get the coefficients of the sparsity estimator Returns: Type Description Coefficient vector (TensorList) The sparse coefficient estimator Source code in deepymod/model/deepmod.py 190 191 192 193 194 195 196 197 def estimator_coeffs ( self ): \"\"\" Get the coefficients of the sparsity estimator Returns: Coefficient vector (TensorList): The sparse coefficient estimator \"\"\" coeff_vectors = self . sparse_estimator . coeff_vectors return coeff_vectors","title":"estimator_coeffs()"},{"location":"api/deepmod/#deepymod.model.deepmod.DeepMoD.forward","text":"[summary] Parameters: Name Type Description Default input Tensor Tensor of shape (n_samples x (n_spatial + 1)) containing the coordinates, first column should be the time coordinate. required Returns: Type Description Tuple[TensorList, TensorList, TensorList] Tuple[TensorList, TensorList, TensorList]: Tuple of tensors containing a tensor of shape (n_samples x n_features) containing the target data, a tensor of the time derivative of the data and the function library. Source code in deepymod/model/deepmod.py 171 172 173 174 175 176 177 178 179 180 181 182 183 def forward ( self , input : torch . Tensor ) -> Tuple [ TensorList , TensorList , TensorList ]: \"\"\"[summary] Args: input (torch.Tensor): Tensor of shape (n_samples x (n_spatial + 1)) containing the coordinates, first column should be the time coordinate. Returns: Tuple[TensorList, TensorList, TensorList]: Tuple of tensors containing a tensor of shape (n_samples x n_features) containing the target data, a tensor of the time derivative of the data and the function library. \"\"\" prediction , coordinates = self . func_approx ( input ) time_derivs , thetas = self . library (( prediction , coordinates )) self . constraint (( time_derivs , thetas )) return prediction , time_derivs , thetas","title":"forward()"},{"location":"api/deepmod/#deepymod.model.deepmod.Estimator","text":"Abstract class implementing the sparsity estimator set to the function approximator. !!! args nn (PyTorch Class): Module of the function approximator, typically a neural network.","title":"Estimator"},{"location":"api/deepmod/#deepymod.model.deepmod.Estimator.fit","text":"Abstract method to compute the result of the network based on data X and labels y Args: x (np.ndarray): input data y (np.ndarray): output labels Returns: Type Description ndarray coefficients (np.ndarray) Source code in deepymod/model/deepmod.py 101 102 103 104 105 106 107 108 109 110 111 @abstractmethod def fit ( self , X : np . ndarray , y : np . ndarray ) -> np . ndarray : \"\"\" Abstract method to compute the result of the network based on data X and labels y Args: x (np.ndarray): input data y (np.ndarray): output labels Returns: coefficients (np.ndarray) \"\"\" pass","title":"fit()"},{"location":"api/deepmod/#deepymod.model.deepmod.Estimator.forward","text":"This function nomalized the library and time derivatives and calculates the corresponding sparisity mask. Parameters: Name Type Description Default thetas <function NewType.<locals>.new_type at 0x7fa552e0bd40> List of library functions, one for every output. required time_derivs <function NewType.<locals>.new_type at 0x7fa552e0bd40> List of time derivates of the data, one for every output. required Returns: Type Description <function NewType.<locals>.new_type at 0x7fa552e0bd40> sparsity_mask (TensorList): A list of sparsity masks, one for every output. Source code in deepymod/model/deepmod.py 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 def forward ( self , thetas : TensorList , time_derivs : TensorList ) -> TensorList : \"\"\"This function nomalized the library and time derivatives and calculates the corresponding sparisity mask. Args: thetas (TensorList): List of library functions, one for every output. time_derivs (TensorList): List of time derivates of the data, one for every output. Returns: sparsity_mask (TensorList): A list of sparsity masks, one for every output. \"\"\" # we first normalize theta and the time deriv with torch . no_grad (): normed_time_derivs = [( time_deriv / torch . norm ( time_deriv )) . detach () . cpu () . numpy () for time_deriv in time_derivs ] normed_thetas = [( theta / torch . norm ( theta , dim = 0 , keepdim = True )) . detach () . cpu () . numpy () for theta in thetas ] self . coeff_vectors = [ self . fit ( theta , time_deriv . squeeze ())[:, None ] for theta , time_deriv in zip ( normed_thetas , normed_time_derivs )] sparsity_masks = [ torch . tensor ( coeff_vector != 0.0 , dtype = torch . bool ) . squeeze () . to ( thetas [ 0 ] . device ) # move to gpu if required for coeff_vector in self . coeff_vectors ] return sparsity_masks","title":"forward()"},{"location":"api/deepmod/#deepymod.model.deepmod.Library","text":"Abstract class that calculates the library function and time derivatives. !!! args nn (PyTorch Class): Module of the function approximator, typically a neural network.","title":"Library"},{"location":"api/deepmod/#deepymod.model.deepmod.Library.forward","text":"Compute the library (time derivatives and thetas) given a dataset Parameters: Name Type Description Default input Tuple[TensorList, TensorList] tuple with firstly the predicted values and required Returns: Type Description Tuple[TensorList, TensorList] Tuple[TensorList, TensorList]: The time derivatives and thetas Source code in deepymod/model/deepmod.py 124 125 126 127 128 129 130 131 132 133 134 135 136 def forward ( self , input : Tuple [ TensorList , TensorList ]) -> Tuple [ TensorList , TensorList ]: \"\"\"Compute the library (time derivatives and thetas) given a dataset Args: input (Tuple[TensorList, TensorList]): tuple with firstly the predicted values and secondly the respective coordinates for the prediction Returns: Tuple[TensorList, TensorList]: The time derivatives and thetas \"\"\" time_derivs , thetas = self . library ( input ) self . norms = [( torch . norm ( time_deriv ) / torch . norm ( theta , dim = 0 , keepdim = True )) . detach () . squeeze () for time_deriv , theta in zip ( time_derivs , thetas )] return time_derivs , thetas","title":"forward()"},{"location":"api/deepmod/#deepymod.model.deepmod.Library.library","text":"Abstract method that should compute the library: Parameters: Name Type Description Default input Tuple[torch.Tensor, torch.Tensor] predictions and coordinates for which to evaluate required Returns: Type Description Tuple[TensorList, TensorList] (Tuple[TensorList, TensorList]): The time derivatives and thetas Source code in deepymod/model/deepmod.py 138 139 140 141 142 143 144 145 146 147 148 149 @abstractmethod def library ( self , input : Tuple [ torch . Tensor , torch . Tensor ]) -> Tuple [ TensorList , TensorList ]: \"\"\"Abstract method that should compute the library: Args: input (Tuple[TensorList, TensorList]): predictions and coordinates for which to evaluate the library. Returns: (Tuple[TensorList, TensorList]): The time derivatives and thetas \"\"\" pass","title":"library()"},{"location":"api/func_approx/","text":"NN [summary] !!! args nn ( [ type ] ) : [ description ] build_network ( self , n_in , n_hidden , n_out ) Constructs a feed-forward neural network. Parameters: Name Type Description Default n_in int Number of input features. required n_hidden List[int] Number of neurons in each layer. required n_out int Number of output features. required Returns: Type Description Sequential torch.Sequential: Pytorch module Source code in deepymod/model/func_approx.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 def build_network ( self , n_in : int , n_hidden : List [ int ], n_out : int ) -> torch . nn . Sequential : \"\"\" Constructs a feed-forward neural network. Args: n_in (int): Number of input features. n_hidden (list[int]): Number of neurons in each layer. n_out (int): Number of output features. Returns: torch.Sequential: Pytorch module \"\"\" network = [] architecture = [ n_in ] + n_hidden + [ n_out ] for layer_i , layer_j in zip ( architecture , architecture [ 1 :]): network . append ( nn . Linear ( layer_i , layer_j )) network . append ( nn . Tanh ()) network . pop () # get rid of last activation function return nn . Sequential ( * network ) forward ( self , input ) [summary] Parameters: Name Type Description Default input Tensor [description] required Returns: Type Description Tensor torch.Tensor: [description] Source code in deepymod/model/func_approx.py 17 18 19 20 21 22 23 24 25 26 27 def forward ( self , input : torch . Tensor ) -> torch . Tensor : \"\"\"[summary] Args: input (torch.Tensor): [description] Returns: torch.Tensor: [description] \"\"\" coordinates = input . clone () . detach () . requires_grad_ ( True ) return self . network ( coordinates ), coordinates SineLayer [summary] !!! args nn ( [ type ] ) : [ description ] forward ( self , input ) [summary] Parameters: Name Type Description Default input [type] [description] required Returns: Type Description [type] [description] Source code in deepymod/model/func_approx.py 76 77 78 79 80 81 82 83 84 85 def forward ( self , input ): \"\"\"[summary] Args: input ([type]): [description] Returns: [type]: [description] \"\"\" return torch . sin ( self . omega_0 * self . linear ( input )) init_weights ( self ) [summary] Source code in deepymod/model/func_approx.py 66 67 68 69 70 71 72 73 74 def init_weights ( self ): \"\"\"[summary] \"\"\" with torch . no_grad (): if self . is_first : self . linear . weight . uniform_ ( - 1 / self . in_features , 1 / self . in_features ) else : self . linear . weight . uniform_ ( - np . sqrt ( 6 / self . in_features ) / self . omega_0 , np . sqrt ( 6 / self . in_features ) / self . omega_0 ) Siren [summary] !!! args nn ( [ type ] ) : [ description ] build_network ( self , n_in , n_hidden , n_out , first_omega_0 , hidden_omega_0 ) [summary] Parameters: Name Type Description Default n_in int [description] required n_hidden List[int] [description] required n_out int [description] required first_omega_0 float [description] required hidden_omega_0 float [description] required Returns: Type Description [type] [description] Source code in deepymod/model/func_approx.py 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 def build_network ( self , n_in : int , n_hidden : List [ int ], n_out : int , first_omega_0 : float , hidden_omega_0 : float ): \"\"\"[summary] Args: n_in (int): [description] n_hidden (List[int]): [description] n_out (int): [description] first_omega_0 (float): [description] hidden_omega_0 (float): [description] Returns: [type]: [description] \"\"\" network = [] # Input layer network . append ( SineLayer ( n_in , n_hidden [ 0 ], is_first = True , omega_0 = first_omega_0 )) # Hidden layers for layer_i , layer_j in zip ( n_hidden , n_hidden [ 1 :]): network . append ( SineLayer ( layer_i , layer_j , is_first = False , omega_0 = hidden_omega_0 )) # Output layer final_linear = nn . Linear ( n_hidden [ - 1 ], n_out ) with torch . no_grad (): final_linear . weight . uniform_ ( - np . sqrt ( 6 / n_hidden [ - 1 ]) / hidden_omega_0 , np . sqrt ( 6 / n_hidden [ - 1 ]) / hidden_omega_0 ) network . append ( final_linear ) return nn . Sequential ( * network ) forward ( self , input ) [summary] Parameters: Name Type Description Default input Tensor [description] required Returns: Type Description [type] [description] Source code in deepymod/model/func_approx.py 98 99 100 101 102 103 104 105 106 107 108 def forward ( self , input : torch . Tensor ): \"\"\"[summary] Args: input (torch.Tensor): [description] Returns: [type]: [description] \"\"\" coordinates = input . clone () . detach () . requires_grad_ ( True ) return self . network ( coordinates ), coordinates","title":"Function Approximators"},{"location":"api/func_approx/#deepymod.model.func_approx","text":"","title":"deepymod.model.func_approx"},{"location":"api/func_approx/#deepymod.model.func_approx.NN","text":"[summary] !!! args nn ( [ type ] ) : [ description ]","title":"NN"},{"location":"api/func_approx/#deepymod.model.func_approx.NN.build_network","text":"Constructs a feed-forward neural network. Parameters: Name Type Description Default n_in int Number of input features. required n_hidden List[int] Number of neurons in each layer. required n_out int Number of output features. required Returns: Type Description Sequential torch.Sequential: Pytorch module Source code in deepymod/model/func_approx.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 def build_network ( self , n_in : int , n_hidden : List [ int ], n_out : int ) -> torch . nn . Sequential : \"\"\" Constructs a feed-forward neural network. Args: n_in (int): Number of input features. n_hidden (list[int]): Number of neurons in each layer. n_out (int): Number of output features. Returns: torch.Sequential: Pytorch module \"\"\" network = [] architecture = [ n_in ] + n_hidden + [ n_out ] for layer_i , layer_j in zip ( architecture , architecture [ 1 :]): network . append ( nn . Linear ( layer_i , layer_j )) network . append ( nn . Tanh ()) network . pop () # get rid of last activation function return nn . Sequential ( * network )","title":"build_network()"},{"location":"api/func_approx/#deepymod.model.func_approx.NN.forward","text":"[summary] Parameters: Name Type Description Default input Tensor [description] required Returns: Type Description Tensor torch.Tensor: [description] Source code in deepymod/model/func_approx.py 17 18 19 20 21 22 23 24 25 26 27 def forward ( self , input : torch . Tensor ) -> torch . Tensor : \"\"\"[summary] Args: input (torch.Tensor): [description] Returns: torch.Tensor: [description] \"\"\" coordinates = input . clone () . detach () . requires_grad_ ( True ) return self . network ( coordinates ), coordinates","title":"forward()"},{"location":"api/func_approx/#deepymod.model.func_approx.SineLayer","text":"[summary] !!! args nn ( [ type ] ) : [ description ]","title":"SineLayer"},{"location":"api/func_approx/#deepymod.model.func_approx.SineLayer.forward","text":"[summary] Parameters: Name Type Description Default input [type] [description] required Returns: Type Description [type] [description] Source code in deepymod/model/func_approx.py 76 77 78 79 80 81 82 83 84 85 def forward ( self , input ): \"\"\"[summary] Args: input ([type]): [description] Returns: [type]: [description] \"\"\" return torch . sin ( self . omega_0 * self . linear ( input ))","title":"forward()"},{"location":"api/func_approx/#deepymod.model.func_approx.SineLayer.init_weights","text":"[summary] Source code in deepymod/model/func_approx.py 66 67 68 69 70 71 72 73 74 def init_weights ( self ): \"\"\"[summary] \"\"\" with torch . no_grad (): if self . is_first : self . linear . weight . uniform_ ( - 1 / self . in_features , 1 / self . in_features ) else : self . linear . weight . uniform_ ( - np . sqrt ( 6 / self . in_features ) / self . omega_0 , np . sqrt ( 6 / self . in_features ) / self . omega_0 )","title":"init_weights()"},{"location":"api/func_approx/#deepymod.model.func_approx.Siren","text":"[summary] !!! args nn ( [ type ] ) : [ description ]","title":"Siren"},{"location":"api/func_approx/#deepymod.model.func_approx.Siren.build_network","text":"[summary] Parameters: Name Type Description Default n_in int [description] required n_hidden List[int] [description] required n_out int [description] required first_omega_0 float [description] required hidden_omega_0 float [description] required Returns: Type Description [type] [description] Source code in deepymod/model/func_approx.py 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 def build_network ( self , n_in : int , n_hidden : List [ int ], n_out : int , first_omega_0 : float , hidden_omega_0 : float ): \"\"\"[summary] Args: n_in (int): [description] n_hidden (List[int]): [description] n_out (int): [description] first_omega_0 (float): [description] hidden_omega_0 (float): [description] Returns: [type]: [description] \"\"\" network = [] # Input layer network . append ( SineLayer ( n_in , n_hidden [ 0 ], is_first = True , omega_0 = first_omega_0 )) # Hidden layers for layer_i , layer_j in zip ( n_hidden , n_hidden [ 1 :]): network . append ( SineLayer ( layer_i , layer_j , is_first = False , omega_0 = hidden_omega_0 )) # Output layer final_linear = nn . Linear ( n_hidden [ - 1 ], n_out ) with torch . no_grad (): final_linear . weight . uniform_ ( - np . sqrt ( 6 / n_hidden [ - 1 ]) / hidden_omega_0 , np . sqrt ( 6 / n_hidden [ - 1 ]) / hidden_omega_0 ) network . append ( final_linear ) return nn . Sequential ( * network )","title":"build_network()"},{"location":"api/func_approx/#deepymod.model.func_approx.Siren.forward","text":"[summary] Parameters: Name Type Description Default input Tensor [description] required Returns: Type Description [type] [description] Source code in deepymod/model/func_approx.py 98 99 100 101 102 103 104 105 106 107 108 def forward ( self , input : torch . Tensor ): \"\"\"[summary] Args: input (torch.Tensor): [description] Returns: [type]: [description] \"\"\" coordinates = input . clone () . detach () . requires_grad_ ( True ) return self . network ( coordinates ), coordinates","title":"forward()"},{"location":"api/lib/","text":"Library1D [summary] !!! args Library ( [ type ] ) : [ description ] library ( self , input ) [summary] Parameters: Name Type Description Default input Tuple[torch.Tensor, torch.Tensor] [description] required Returns: Type Description Tuple[TensorList, TensorList] Tuple[TensorList, TensorList]: [description] Source code in deepymod/model/library.py 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 def library ( self , input : Tuple [ torch . Tensor , torch . Tensor ]) -> Tuple [ TensorList , TensorList ]: \"\"\"[summary] Args: input (Tuple[torch.Tensor, torch.Tensor]): [description] Returns: Tuple[TensorList, TensorList]: [description] \"\"\" prediction , data = input poly_list = [] deriv_list = [] time_deriv_list = [] # Creating lists for all outputs for output in np . arange ( prediction . shape [ 1 ]): time_deriv , du = library_deriv ( data , prediction [:, output : output + 1 ], self . diff_order ) u = library_poly ( prediction [:, output : output + 1 ], self . poly_order ) poly_list . append ( u ) deriv_list . append ( du ) time_deriv_list . append ( time_deriv ) samples = time_deriv_list [ 0 ] . shape [ 0 ] total_terms = poly_list [ 0 ] . shape [ 1 ] * deriv_list [ 0 ] . shape [ 1 ] # Calculating theta if len ( poly_list ) == 1 : # If we have a single output, we simply calculate and flatten matrix product # between polynomials and derivatives to get library theta = torch . matmul ( poly_list [ 0 ][:, :, None ], deriv_list [ 0 ][:, None , :]) . view ( samples , total_terms ) else : theta_uv = reduce (( lambda x , y : ( x [:, :, None ] @ y [:, None , :]) . view ( samples , - 1 )), poly_list ) # calculate all unique combinations of derivatives theta_dudv = torch . cat ([ torch . matmul ( du [:, :, None ], dv [:, None , :]) . view ( samples , - 1 )[:, 1 :] for du , dv in combinations ( deriv_list , 2 )], 1 ) theta = torch . cat ([ theta_uv , theta_dudv ], dim = 1 ) return time_deriv_list , [ theta ] Library2D [summary] !!! args Library ( [ type ] ) : [ description ] library ( self , input ) [summary] Parameters: Name Type Description Default input Tuple[torch.Tensor, torch.Tensor] [description] required Returns: Type Description Tuple[TensorList, TensorList] Tuple[TensorList, TensorList]: [description] Source code in deepymod/model/library.py 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 def library ( self , input : Tuple [ torch . Tensor , torch . Tensor ]) -> Tuple [ TensorList , TensorList ]: \"\"\"[summary] Args: input (Tuple[torch.Tensor, torch.Tensor]): [description] Returns: Tuple[TensorList, TensorList]: [description] \"\"\" prediction , data = input # Polynomial u = torch . ones_like ( prediction ) for order in np . arange ( 1 , self . poly_order + 1 ): u = torch . cat (( u , u [:, order - 1 : order ] * prediction ), dim = 1 ) # Gradients du = grad ( prediction , data , grad_outputs = torch . ones_like ( prediction ), create_graph = True )[ 0 ] u_t = du [:, 0 : 1 ] u_x = du [:, 1 : 2 ] u_y = du [:, 2 : 3 ] du2 = grad ( u_x , data , grad_outputs = torch . ones_like ( prediction ), create_graph = True )[ 0 ] u_xx = du2 [:, 1 : 2 ] u_xy = du2 [:, 2 : 3 ] u_yy = grad ( u_y , data , grad_outputs = torch . ones_like ( prediction ), create_graph = True )[ 0 ][:, 2 : 3 ] du = torch . cat (( torch . ones_like ( u_x ), u_x , u_y , u_xx , u_yy , u_xy ), dim = 1 ) samples = du . shape [ 0 ] # Bringing it together theta = torch . matmul ( u [:, :, None ], du [:, None , :]) . view ( samples , - 1 ) return [ u_t ], [ theta ] library_deriv ( data , prediction , max_order ) [summary] Parameters: Name Type Description Default data Tensor [description] required prediction Tensor [description] required max_order int [description] required Returns: Type Description Tuple[torch.Tensor, torch.Tensor] Tuple[torch.Tensor, torch.Tensor]: [description] Source code in deepymod/model/library.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 def library_deriv ( data : torch . Tensor , prediction : torch . Tensor , max_order : int ) -> Tuple [ torch . Tensor , torch . Tensor ]: \"\"\"[summary] Args: data (torch.Tensor): [description] prediction (torch.Tensor): [description] max_order (int): [description] Returns: Tuple[torch.Tensor, torch.Tensor]: [description] \"\"\" dy = grad ( prediction , data , grad_outputs = torch . ones_like ( prediction ), create_graph = True )[ 0 ] time_deriv = dy [:, 0 : 1 ] if max_order == 0 : du = torch . ones_like ( time_deriv ) else : du = torch . cat (( torch . ones_like ( time_deriv ), dy [:, 1 : 2 ]), dim = 1 ) if max_order > 1 : for order in np . arange ( 1 , max_order ): du = torch . cat (( du , grad ( du [:, order : order + 1 ], data , grad_outputs = torch . ones_like ( prediction ), create_graph = True )[ 0 ][:, 1 : 2 ]), dim = 1 ) return time_deriv , du library_poly ( prediction , max_order ) [summary] Parameters: Name Type Description Default prediction Tensor [description] required max_order int [description] required Returns: Type Description Tensor torch.Tensor: [description] Source code in deepymod/model/library.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 def library_poly ( prediction : torch . Tensor , max_order : int ) -> torch . Tensor : \"\"\"[summary] Args: prediction (torch.Tensor): [description] max_order (int): [description] Returns: torch.Tensor: [description] \"\"\" u = torch . ones_like ( prediction ) for order in np . arange ( 1 , max_order + 1 ): u = torch . cat (( u , u [:, order - 1 : order ] * prediction ), dim = 1 ) return u","title":"Libraries"},{"location":"api/lib/#deepymod.model.library","text":"","title":"deepymod.model.library"},{"location":"api/lib/#deepymod.model.library.Library1D","text":"[summary] !!! args Library ( [ type ] ) : [ description ]","title":"Library1D"},{"location":"api/lib/#deepymod.model.library.Library1D.library","text":"[summary] Parameters: Name Type Description Default input Tuple[torch.Tensor, torch.Tensor] [description] required Returns: Type Description Tuple[TensorList, TensorList] Tuple[TensorList, TensorList]: [description] Source code in deepymod/model/library.py 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 def library ( self , input : Tuple [ torch . Tensor , torch . Tensor ]) -> Tuple [ TensorList , TensorList ]: \"\"\"[summary] Args: input (Tuple[torch.Tensor, torch.Tensor]): [description] Returns: Tuple[TensorList, TensorList]: [description] \"\"\" prediction , data = input poly_list = [] deriv_list = [] time_deriv_list = [] # Creating lists for all outputs for output in np . arange ( prediction . shape [ 1 ]): time_deriv , du = library_deriv ( data , prediction [:, output : output + 1 ], self . diff_order ) u = library_poly ( prediction [:, output : output + 1 ], self . poly_order ) poly_list . append ( u ) deriv_list . append ( du ) time_deriv_list . append ( time_deriv ) samples = time_deriv_list [ 0 ] . shape [ 0 ] total_terms = poly_list [ 0 ] . shape [ 1 ] * deriv_list [ 0 ] . shape [ 1 ] # Calculating theta if len ( poly_list ) == 1 : # If we have a single output, we simply calculate and flatten matrix product # between polynomials and derivatives to get library theta = torch . matmul ( poly_list [ 0 ][:, :, None ], deriv_list [ 0 ][:, None , :]) . view ( samples , total_terms ) else : theta_uv = reduce (( lambda x , y : ( x [:, :, None ] @ y [:, None , :]) . view ( samples , - 1 )), poly_list ) # calculate all unique combinations of derivatives theta_dudv = torch . cat ([ torch . matmul ( du [:, :, None ], dv [:, None , :]) . view ( samples , - 1 )[:, 1 :] for du , dv in combinations ( deriv_list , 2 )], 1 ) theta = torch . cat ([ theta_uv , theta_dudv ], dim = 1 ) return time_deriv_list , [ theta ]","title":"library()"},{"location":"api/lib/#deepymod.model.library.Library2D","text":"[summary] !!! args Library ( [ type ] ) : [ description ]","title":"Library2D"},{"location":"api/lib/#deepymod.model.library.Library2D.library","text":"[summary] Parameters: Name Type Description Default input Tuple[torch.Tensor, torch.Tensor] [description] required Returns: Type Description Tuple[TensorList, TensorList] Tuple[TensorList, TensorList]: [description] Source code in deepymod/model/library.py 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 def library ( self , input : Tuple [ torch . Tensor , torch . Tensor ]) -> Tuple [ TensorList , TensorList ]: \"\"\"[summary] Args: input (Tuple[torch.Tensor, torch.Tensor]): [description] Returns: Tuple[TensorList, TensorList]: [description] \"\"\" prediction , data = input # Polynomial u = torch . ones_like ( prediction ) for order in np . arange ( 1 , self . poly_order + 1 ): u = torch . cat (( u , u [:, order - 1 : order ] * prediction ), dim = 1 ) # Gradients du = grad ( prediction , data , grad_outputs = torch . ones_like ( prediction ), create_graph = True )[ 0 ] u_t = du [:, 0 : 1 ] u_x = du [:, 1 : 2 ] u_y = du [:, 2 : 3 ] du2 = grad ( u_x , data , grad_outputs = torch . ones_like ( prediction ), create_graph = True )[ 0 ] u_xx = du2 [:, 1 : 2 ] u_xy = du2 [:, 2 : 3 ] u_yy = grad ( u_y , data , grad_outputs = torch . ones_like ( prediction ), create_graph = True )[ 0 ][:, 2 : 3 ] du = torch . cat (( torch . ones_like ( u_x ), u_x , u_y , u_xx , u_yy , u_xy ), dim = 1 ) samples = du . shape [ 0 ] # Bringing it together theta = torch . matmul ( u [:, :, None ], du [:, None , :]) . view ( samples , - 1 ) return [ u_t ], [ theta ]","title":"library()"},{"location":"api/lib/#deepymod.model.library.library_deriv","text":"[summary] Parameters: Name Type Description Default data Tensor [description] required prediction Tensor [description] required max_order int [description] required Returns: Type Description Tuple[torch.Tensor, torch.Tensor] Tuple[torch.Tensor, torch.Tensor]: [description] Source code in deepymod/model/library.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 def library_deriv ( data : torch . Tensor , prediction : torch . Tensor , max_order : int ) -> Tuple [ torch . Tensor , torch . Tensor ]: \"\"\"[summary] Args: data (torch.Tensor): [description] prediction (torch.Tensor): [description] max_order (int): [description] Returns: Tuple[torch.Tensor, torch.Tensor]: [description] \"\"\" dy = grad ( prediction , data , grad_outputs = torch . ones_like ( prediction ), create_graph = True )[ 0 ] time_deriv = dy [:, 0 : 1 ] if max_order == 0 : du = torch . ones_like ( time_deriv ) else : du = torch . cat (( torch . ones_like ( time_deriv ), dy [:, 1 : 2 ]), dim = 1 ) if max_order > 1 : for order in np . arange ( 1 , max_order ): du = torch . cat (( du , grad ( du [:, order : order + 1 ], data , grad_outputs = torch . ones_like ( prediction ), create_graph = True )[ 0 ][:, 1 : 2 ]), dim = 1 ) return time_deriv , du","title":"library_deriv()"},{"location":"api/lib/#deepymod.model.library.library_poly","text":"[summary] Parameters: Name Type Description Default prediction Tensor [description] required max_order int [description] required Returns: Type Description Tensor torch.Tensor: [description] Source code in deepymod/model/library.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 def library_poly ( prediction : torch . Tensor , max_order : int ) -> torch . Tensor : \"\"\"[summary] Args: prediction (torch.Tensor): [description] max_order (int): [description] Returns: torch.Tensor: [description] \"\"\" u = torch . ones_like ( prediction ) for order in np . arange ( 1 , max_order + 1 ): u = torch . cat (( u , u [:, order - 1 : order ] * prediction ), dim = 1 ) return u","title":"library_poly()"},{"location":"api/spars_sched/","text":"Periodic Periodically applies sparsity evert periodicity iterations after initial_epoch. TrainTest Early stops the training if validation loss doesn't improve after a given patience. Note that periodicity should be multitude of write_iterations. save_checkpoint ( self , model , optimizer ) Saves model when validation loss decrease. Source code in deepymod/training/sparsity_scheduler.py 59 60 61 62 def save_checkpoint ( self , model , optimizer ): '''Saves model when validation loss decrease.''' checkpoint_path = self . path + 'checkpoint.pt' torch . save ({ 'model_state_dict' : model . state_dict (), 'optimizer_state_dict' : optimizer . state_dict (),}, checkpoint_path ) TrainTestPeriodic Early stops the training if validation loss doesn't improve after a given patience. Note that periodicity should be multitude of write_iterations. save_checkpoint ( self , model , optimizer ) Saves model when validation loss decrease. Source code in deepymod/training/sparsity_scheduler.py 115 116 117 118 def save_checkpoint ( self , model , optimizer ): '''Saves model when validation loss decrease.''' checkpoint_path = self . path + 'checkpoint.pt' torch . save ({ 'model_state_dict' : model . state_dict (), 'optimizer_state_dict' : optimizer . state_dict (),}, checkpoint_path )","title":"Sparsity Scheduler"},{"location":"api/spars_sched/#deepymod.training.sparsity_scheduler","text":"","title":"deepymod.training.sparsity_scheduler"},{"location":"api/spars_sched/#deepymod.training.sparsity_scheduler.Periodic","text":"Periodically applies sparsity evert periodicity iterations after initial_epoch.","title":"Periodic"},{"location":"api/spars_sched/#deepymod.training.sparsity_scheduler.TrainTest","text":"Early stops the training if validation loss doesn't improve after a given patience. Note that periodicity should be multitude of write_iterations.","title":"TrainTest"},{"location":"api/spars_sched/#deepymod.training.sparsity_scheduler.TrainTest.save_checkpoint","text":"Saves model when validation loss decrease. Source code in deepymod/training/sparsity_scheduler.py 59 60 61 62 def save_checkpoint ( self , model , optimizer ): '''Saves model when validation loss decrease.''' checkpoint_path = self . path + 'checkpoint.pt' torch . save ({ 'model_state_dict' : model . state_dict (), 'optimizer_state_dict' : optimizer . state_dict (),}, checkpoint_path )","title":"save_checkpoint()"},{"location":"api/spars_sched/#deepymod.training.sparsity_scheduler.TrainTestPeriodic","text":"Early stops the training if validation loss doesn't improve after a given patience. Note that periodicity should be multitude of write_iterations.","title":"TrainTestPeriodic"},{"location":"api/spars_sched/#deepymod.training.sparsity_scheduler.TrainTestPeriodic.save_checkpoint","text":"Saves model when validation loss decrease. Source code in deepymod/training/sparsity_scheduler.py 115 116 117 118 def save_checkpoint ( self , model , optimizer ): '''Saves model when validation loss decrease.''' checkpoint_path = self . path + 'checkpoint.pt' torch . save ({ 'model_state_dict' : model . state_dict (), 'optimizer_state_dict' : optimizer . state_dict (),}, checkpoint_path )","title":"save_checkpoint()"},{"location":"api/sparse/","text":"Periodic Periodically applies sparsity evert periodicity iterations after initial_epoch. TrainTest Early stops the training if validation loss doesn't improve after a given patience. Note that periodicity should be multitude of write_iterations. save_checkpoint ( self , model , optimizer ) Saves model when validation loss decrease. Source code in deepymod/training/sparsity_scheduler.py 59 60 61 62 def save_checkpoint ( self , model , optimizer ): '''Saves model when validation loss decrease.''' checkpoint_path = self . path + 'checkpoint.pt' torch . save ({ 'model_state_dict' : model . state_dict (), 'optimizer_state_dict' : optimizer . state_dict (),}, checkpoint_path ) TrainTestPeriodic Early stops the training if validation loss doesn't improve after a given patience. Note that periodicity should be multitude of write_iterations. save_checkpoint ( self , model , optimizer ) Saves model when validation loss decrease. Source code in deepymod/training/sparsity_scheduler.py 115 116 117 118 def save_checkpoint ( self , model , optimizer ): '''Saves model when validation loss decrease.''' checkpoint_path = self . path + 'checkpoint.pt' torch . save ({ 'model_state_dict' : model . state_dict (), 'optimizer_state_dict' : optimizer . state_dict (),}, checkpoint_path )","title":"Sparsity"},{"location":"api/sparse/#deepymod.training.sparsity_scheduler","text":"","title":"deepymod.training.sparsity_scheduler"},{"location":"api/sparse/#deepymod.training.sparsity_scheduler.Periodic","text":"Periodically applies sparsity evert periodicity iterations after initial_epoch.","title":"Periodic"},{"location":"api/sparse/#deepymod.training.sparsity_scheduler.TrainTest","text":"Early stops the training if validation loss doesn't improve after a given patience. Note that periodicity should be multitude of write_iterations.","title":"TrainTest"},{"location":"api/sparse/#deepymod.training.sparsity_scheduler.TrainTest.save_checkpoint","text":"Saves model when validation loss decrease. Source code in deepymod/training/sparsity_scheduler.py 59 60 61 62 def save_checkpoint ( self , model , optimizer ): '''Saves model when validation loss decrease.''' checkpoint_path = self . path + 'checkpoint.pt' torch . save ({ 'model_state_dict' : model . state_dict (), 'optimizer_state_dict' : optimizer . state_dict (),}, checkpoint_path )","title":"save_checkpoint()"},{"location":"api/sparse/#deepymod.training.sparsity_scheduler.TrainTestPeriodic","text":"Early stops the training if validation loss doesn't improve after a given patience. Note that periodicity should be multitude of write_iterations.","title":"TrainTestPeriodic"},{"location":"api/sparse/#deepymod.training.sparsity_scheduler.TrainTestPeriodic.save_checkpoint","text":"Saves model when validation loss decrease. Source code in deepymod/training/sparsity_scheduler.py 115 116 117 118 def save_checkpoint ( self , model , optimizer ): '''Saves model when validation loss decrease.''' checkpoint_path = self . path + 'checkpoint.pt' torch . save ({ 'model_state_dict' : model . state_dict (), 'optimizer_state_dict' : optimizer . state_dict (),}, checkpoint_path )","title":"save_checkpoint()"},{"location":"api/training/","text":"train ( model , data , target , optimizer , sparsity_scheduler , split = 0.8 , exp_ID = None , log_dir = None , max_iterations = 10000 , write_iterations = 25 , ** convergence_kwargs ) Trains the DeepMoD model. This function automatically splits the data set in a train and test set. Parameters: Name Type Description Default model DeepMoD A DeepMoD object. required data Tensor Tensor of shape (n_samples x (n_spatial + 1)) containing the coordinates, first column should be the time coordinate. required target Tensor Tensor of shape (n_samples x n_features) containing the target data. required optimizer [type] Pytorch optimizer. required sparsity_scheduler [type] Decides when to update the sparsity mask. required split float Fraction of the train set, by default 0.8. 0.8 exp_ID str Unique ID to identify tensorboard file. Not used if log_dir is given, see pytorch documentation. None log_dir str Directory where tensorboard file is written, by default None. None max_iterations int [description]. Max number of epochs , by default 10000. 10000 write_iterations int [description]. Sets how often data is written to tensorboard and checks train loss , by default 25. 25 Source code in deepymod/training/training.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 def train ( model : DeepMoD , data : torch . Tensor , target : torch . Tensor , optimizer , sparsity_scheduler , split : float = 0.8 , exp_ID : str = None , log_dir : str = None , max_iterations : int = 10000 , write_iterations : int = 25 , ** convergence_kwargs ) -> None : \"\"\"Trains the DeepMoD model. This function automatically splits the data set in a train and test set. Args: model (DeepMoD): A DeepMoD object. data (torch.Tensor): Tensor of shape (n_samples x (n_spatial + 1)) containing the coordinates, first column should be the time coordinate. target (torch.Tensor): Tensor of shape (n_samples x n_features) containing the target data. optimizer ([type]): Pytorch optimizer. sparsity_scheduler ([type]): Decides when to update the sparsity mask. split (float, optional): Fraction of the train set, by default 0.8. exp_ID (str, optional): Unique ID to identify tensorboard file. Not used if log_dir is given, see pytorch documentation. log_dir (str, optional): Directory where tensorboard file is written, by default None. max_iterations (int, optional): [description]. Max number of epochs , by default 10000. write_iterations (int, optional): [description]. Sets how often data is written to tensorboard and checks train loss , by default 25. \"\"\" logger = Logger ( exp_ID , log_dir ) sparsity_scheduler . path = logger . log_dir # write checkpoint to same folder as tb output. # Splitting data, assumes data is already randomized n_train = int ( split * data . shape [ 0 ]) n_test = data . shape [ 0 ] - n_train data_train , data_test = torch . split ( data , [ n_train , n_test ], dim = 0 ) target_train , target_test = torch . split ( target , [ n_train , n_test ], dim = 0 ) # Training convergence = Convergence ( ** convergence_kwargs ) for iteration in torch . arange ( 0 , max_iterations ): # ================== Training Model ============================ prediction , time_derivs , thetas = model ( data_train ) MSE = torch . mean (( prediction - target_train ) ** 2 , dim = 0 ) # loss per output Reg = torch . stack ([ torch . mean (( dt - theta @ coeff_vector ) ** 2 ) for dt , theta , coeff_vector in zip ( time_derivs , thetas , model . constraint_coeffs ( scaled = False , sparse = True ))]) loss = torch . sum ( MSE + Reg ) # Optimizer step optimizer . zero_grad () loss . backward () optimizer . step () if iteration % write_iterations == 0 : # ================== Validation costs ================ with torch . no_grad (): prediction_test = model . func_approx ( data_test )[ 0 ] MSE_test = torch . mean (( prediction_test - target_test ) ** 2 , dim = 0 ) # loss per output # ====================== Logging ======================= _ = model . sparse_estimator ( thetas , time_derivs ) # calculating estimator coeffs but not setting mask logger ( iteration , loss , MSE , Reg , model . constraint_coeffs ( sparse = True , scaled = True ), model . constraint_coeffs ( sparse = True , scaled = False ), model . estimator_coeffs (), MSE_test = MSE_test ) # ================== Sparsity update ============= # Updating sparsity update_sparsity = sparsity_scheduler ( iteration , torch . sum ( MSE_test ), model , optimizer ) if update_sparsity : model . constraint . sparsity_masks = model . sparse_estimator ( thetas , time_derivs ) # ================= Checking convergence l1_norm = torch . sum ( torch . abs ( torch . cat ( model . constraint_coeffs ( sparse = True , scaled = True ), dim = 1 ))) converged = convergence ( iteration , l1_norm ) if converged : break logger . close ( model )","title":"Training"},{"location":"api/training/#deepymod.training.training","text":"","title":"deepymod.training.training"},{"location":"api/training/#deepymod.training.training.train","text":"Trains the DeepMoD model. This function automatically splits the data set in a train and test set. Parameters: Name Type Description Default model DeepMoD A DeepMoD object. required data Tensor Tensor of shape (n_samples x (n_spatial + 1)) containing the coordinates, first column should be the time coordinate. required target Tensor Tensor of shape (n_samples x n_features) containing the target data. required optimizer [type] Pytorch optimizer. required sparsity_scheduler [type] Decides when to update the sparsity mask. required split float Fraction of the train set, by default 0.8. 0.8 exp_ID str Unique ID to identify tensorboard file. Not used if log_dir is given, see pytorch documentation. None log_dir str Directory where tensorboard file is written, by default None. None max_iterations int [description]. Max number of epochs , by default 10000. 10000 write_iterations int [description]. Sets how often data is written to tensorboard and checks train loss , by default 25. 25 Source code in deepymod/training/training.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 def train ( model : DeepMoD , data : torch . Tensor , target : torch . Tensor , optimizer , sparsity_scheduler , split : float = 0.8 , exp_ID : str = None , log_dir : str = None , max_iterations : int = 10000 , write_iterations : int = 25 , ** convergence_kwargs ) -> None : \"\"\"Trains the DeepMoD model. This function automatically splits the data set in a train and test set. Args: model (DeepMoD): A DeepMoD object. data (torch.Tensor): Tensor of shape (n_samples x (n_spatial + 1)) containing the coordinates, first column should be the time coordinate. target (torch.Tensor): Tensor of shape (n_samples x n_features) containing the target data. optimizer ([type]): Pytorch optimizer. sparsity_scheduler ([type]): Decides when to update the sparsity mask. split (float, optional): Fraction of the train set, by default 0.8. exp_ID (str, optional): Unique ID to identify tensorboard file. Not used if log_dir is given, see pytorch documentation. log_dir (str, optional): Directory where tensorboard file is written, by default None. max_iterations (int, optional): [description]. Max number of epochs , by default 10000. write_iterations (int, optional): [description]. Sets how often data is written to tensorboard and checks train loss , by default 25. \"\"\" logger = Logger ( exp_ID , log_dir ) sparsity_scheduler . path = logger . log_dir # write checkpoint to same folder as tb output. # Splitting data, assumes data is already randomized n_train = int ( split * data . shape [ 0 ]) n_test = data . shape [ 0 ] - n_train data_train , data_test = torch . split ( data , [ n_train , n_test ], dim = 0 ) target_train , target_test = torch . split ( target , [ n_train , n_test ], dim = 0 ) # Training convergence = Convergence ( ** convergence_kwargs ) for iteration in torch . arange ( 0 , max_iterations ): # ================== Training Model ============================ prediction , time_derivs , thetas = model ( data_train ) MSE = torch . mean (( prediction - target_train ) ** 2 , dim = 0 ) # loss per output Reg = torch . stack ([ torch . mean (( dt - theta @ coeff_vector ) ** 2 ) for dt , theta , coeff_vector in zip ( time_derivs , thetas , model . constraint_coeffs ( scaled = False , sparse = True ))]) loss = torch . sum ( MSE + Reg ) # Optimizer step optimizer . zero_grad () loss . backward () optimizer . step () if iteration % write_iterations == 0 : # ================== Validation costs ================ with torch . no_grad (): prediction_test = model . func_approx ( data_test )[ 0 ] MSE_test = torch . mean (( prediction_test - target_test ) ** 2 , dim = 0 ) # loss per output # ====================== Logging ======================= _ = model . sparse_estimator ( thetas , time_derivs ) # calculating estimator coeffs but not setting mask logger ( iteration , loss , MSE , Reg , model . constraint_coeffs ( sparse = True , scaled = True ), model . constraint_coeffs ( sparse = True , scaled = False ), model . estimator_coeffs (), MSE_test = MSE_test ) # ================== Sparsity update ============= # Updating sparsity update_sparsity = sparsity_scheduler ( iteration , torch . sum ( MSE_test ), model , optimizer ) if update_sparsity : model . constraint . sparsity_masks = model . sparse_estimator ( thetas , time_derivs ) # ================= Checking convergence l1_norm = torch . sum ( torch . abs ( torch . cat ( model . constraint_coeffs ( sparse = True , scaled = True ), dim = 1 ))) converged = convergence ( iteration , l1_norm ) if converged : break logger . close ( model )","title":"train()"},{"location":"api/data/base/","text":"Dataset This class automatically generates all the necessary proporties of a predifined data set with a single spatial dimension as input. In particular it calculates the solution, the time derivative and the library. Note that all the pytorch opperations such as automatic differentiation can be used on the results. create_dataset ( self , x , t , n_samples , noise , random = True , normalize = True , return_idx = False , random_state = 42 ) Function creates the data set in the precise format used by DeepMoD Parameters: Name Type Description Default x Tensor Input vector of spatial coordinates required t Tensor Input vector of temporal coordinates required n_samples int Number of samples, set n_samples=0 for all. required noise float Noise level in percentage of std. required random bool When true, data set is randomised. Defaults to True. True normalize bool When true, data set is normalized. Defaults to True. True return_idx bool When true, the id of the data, before randomizing is returned. Defaults to False. False random_state int Seed of the randomisation. Defaults to 42. 42 Returns: Type Description [type] Tensor containing the input and output and optionally the randomisation. Source code in deepymod/data/base.py 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 def create_dataset ( self , x , t , n_samples , noise , random = True , normalize = True , return_idx = False , random_state = 42 ): \"\"\"Function creates the data set in the precise format used by DeepMoD Args: x (Tensor): Input vector of spatial coordinates t (Tensor): Input vector of temporal coordinates n_samples (int): Number of samples, set n_samples=0 for all. noise (float): Noise level in percentage of std. random (bool, optional): When true, data set is randomised. Defaults to True. normalize (bool, optional): When true, data set is normalized. Defaults to True. return_idx (bool, optional): When true, the id of the data, before randomizing is returned. Defaults to False. random_state (int, optional): Seed of the randomisation. Defaults to 42. Returns: [type]: Tensor containing the input and output and optionally the randomisation. \"\"\" assert (( x . shape [ 1 ] == 1 ) & ( t . shape [ 1 ] == 1 )), 'x and t should have shape (n_samples x 1)' u = self . generate_solution ( x , t ) X = np . concatenate ([ t , x ], axis = 1 ) if random_state is None : y = u + noise * np . std ( u , axis = 0 ) * np . random . normal ( size = u . shape ) else : y = u + noise * np . std ( u , axis = 0 ) * np . random . RandomState ( seed = random_state ) . normal ( size = u . shape ) # creating random idx for samples N = y . shape [ 0 ] if n_samples == 0 else n_samples if random is True : if random_state is None : rand_idx = np . random . permutation ( y . shape [ 0 ])[: N ] else : rand_idx = np . random . RandomState ( seed = random_state ) . permutation ( y . shape [ 0 ])[: N ] else : rand_idx = np . arange ( y . shape [ 0 ])[: N ] # Normalizing if normalize : if ( self . scaling_factor is None ): self . scaling_factor = ( - ( np . max ( X , axis = 0 ) + np . min ( X , axis = 0 )) / 2 , ( np . max ( X , axis = 0 ) - np . min ( X , axis = 0 )) / 2 ) # only calculate the first time X = ( X + self . scaling_factor [ 0 ]) / self . scaling_factor [ 1 ] # Building dataset X_train = torch . tensor ( X [ rand_idx , :], dtype = torch . float32 ) y_train = torch . tensor ( y [ rand_idx , :], dtype = torch . float32 ) if return_idx is False : return X_train , y_train else : return X_train , y_train , rand_idx Dataset_2D This class automatically generates all the necessary proporties of a predifined data set with two spatial dimension as input. In particular it calculates the solution, the time derivative and the library. Note that all the pytorch opperations such as automatic differentiation can be used on the results. create_dataset ( self , x , t , n_samples , noise , random = True , return_idx = False , random_state = 42 ) Function creates the data set in the precise format used by DeepMoD Parameters: Name Type Description Default x Tensor Input vector of spatial coordinates required t Tensor Input vector of temporal coordinates required n_samples int Number of samples, set n_samples=0 for all. required noise float Noise level in percentage of std. required random bool When true, data set is randomised. Defaults to True. True normalize bool When true, data set is normalized. Defaults to True. required return_idx bool When true, the id of the data, before randomizing is returned. Defaults to False. False random_state int Seed of the randomisation. Defaults to 42. 42 Returns: Type Description [type] Tensor containing the input and output and optionally the randomisation. Source code in deepymod/data/base.py 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 def create_dataset ( self , x , t , n_samples , noise , random = True , return_idx = False , random_state = 42 ): \"\"\"Function creates the data set in the precise format used by DeepMoD Args: x (Tensor): Input vector of spatial coordinates t (Tensor): Input vector of temporal coordinates n_samples (int): Number of samples, set n_samples=0 for all. noise (float): Noise level in percentage of std. random (bool, optional): When true, data set is randomised. Defaults to True. normalize (bool, optional): When true, data set is normalized. Defaults to True. return_idx (bool, optional): When true, the id of the data, before randomizing is returned. Defaults to False. random_state (int, optional): Seed of the randomisation. Defaults to 42. Returns: [type]: Tensor containing the input and output and optionally the randomisation. \"\"\" assert (( x . shape [ 1 ] == 2 ) & ( t . shape [ 1 ] == 1 )), 'x and t should have shape (n_samples x 1)' u = self . generate_solution ( x , t ) X = np . concatenate ([ t , x ], axis = 1 ) y = u + noise * np . std ( u , axis = 0 ) * np . random . normal ( size = u . shape ) # creating random idx for samples N = y . shape [ 0 ] if n_samples == 0 else n_samples if random is True : rand_idx = np . random . RandomState ( seed = random_state ) . permutation ( y . shape [ 0 ])[: N ] # so we can get similar splits for different noise levels else : rand_idx = np . arange ( y . shape [ 0 ])[: N ] # Building dataset X_train = torch . tensor ( X [ rand_idx , :], requires_grad = True , dtype = torch . float32 ) y_train = torch . tensor ( y [ rand_idx , :], requires_grad = True , dtype = torch . float32 ) if return_idx is False : return X_train , y_train else : return X_train , y_train , rand_idx pytorch_func ( function ) Decorator to automatically transform arrays to tensors and back Parameters: Name Type Description Default function Tensor Pytorch tensor required Returns: Type Description Numpy array Source code in deepymod/data/base.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 def pytorch_func ( function ): \"\"\"Decorator to automatically transform arrays to tensors and back Args: function (Tensor): Pytorch tensor Returns: Numpy array \"\"\" def wrapper ( self , * args , ** kwargs ): torch_args = [ torch . tensor ( arg , requires_grad = True , dtype = torch . float64 ) if type ( arg ) is ndarray else arg for arg in args ] torch_kwargs = { key : torch . tensor ( kwarg , requires_grad = True , dtype = torch . float64 ) if type ( kwarg ) is ndarray else kwarg for key , kwarg in kwargs . items ()} result = function ( self , * torch_args , ** torch_kwargs ) return result . cpu () . detach () . numpy () return wrapper","title":"Base"},{"location":"api/data/base/#deepymod.data.base","text":"","title":"deepymod.data.base"},{"location":"api/data/base/#deepymod.data.base.Dataset","text":"This class automatically generates all the necessary proporties of a predifined data set with a single spatial dimension as input. In particular it calculates the solution, the time derivative and the library. Note that all the pytorch opperations such as automatic differentiation can be used on the results.","title":"Dataset"},{"location":"api/data/base/#deepymod.data.base.Dataset.create_dataset","text":"Function creates the data set in the precise format used by DeepMoD Parameters: Name Type Description Default x Tensor Input vector of spatial coordinates required t Tensor Input vector of temporal coordinates required n_samples int Number of samples, set n_samples=0 for all. required noise float Noise level in percentage of std. required random bool When true, data set is randomised. Defaults to True. True normalize bool When true, data set is normalized. Defaults to True. True return_idx bool When true, the id of the data, before randomizing is returned. Defaults to False. False random_state int Seed of the randomisation. Defaults to 42. 42 Returns: Type Description [type] Tensor containing the input and output and optionally the randomisation. Source code in deepymod/data/base.py 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 def create_dataset ( self , x , t , n_samples , noise , random = True , normalize = True , return_idx = False , random_state = 42 ): \"\"\"Function creates the data set in the precise format used by DeepMoD Args: x (Tensor): Input vector of spatial coordinates t (Tensor): Input vector of temporal coordinates n_samples (int): Number of samples, set n_samples=0 for all. noise (float): Noise level in percentage of std. random (bool, optional): When true, data set is randomised. Defaults to True. normalize (bool, optional): When true, data set is normalized. Defaults to True. return_idx (bool, optional): When true, the id of the data, before randomizing is returned. Defaults to False. random_state (int, optional): Seed of the randomisation. Defaults to 42. Returns: [type]: Tensor containing the input and output and optionally the randomisation. \"\"\" assert (( x . shape [ 1 ] == 1 ) & ( t . shape [ 1 ] == 1 )), 'x and t should have shape (n_samples x 1)' u = self . generate_solution ( x , t ) X = np . concatenate ([ t , x ], axis = 1 ) if random_state is None : y = u + noise * np . std ( u , axis = 0 ) * np . random . normal ( size = u . shape ) else : y = u + noise * np . std ( u , axis = 0 ) * np . random . RandomState ( seed = random_state ) . normal ( size = u . shape ) # creating random idx for samples N = y . shape [ 0 ] if n_samples == 0 else n_samples if random is True : if random_state is None : rand_idx = np . random . permutation ( y . shape [ 0 ])[: N ] else : rand_idx = np . random . RandomState ( seed = random_state ) . permutation ( y . shape [ 0 ])[: N ] else : rand_idx = np . arange ( y . shape [ 0 ])[: N ] # Normalizing if normalize : if ( self . scaling_factor is None ): self . scaling_factor = ( - ( np . max ( X , axis = 0 ) + np . min ( X , axis = 0 )) / 2 , ( np . max ( X , axis = 0 ) - np . min ( X , axis = 0 )) / 2 ) # only calculate the first time X = ( X + self . scaling_factor [ 0 ]) / self . scaling_factor [ 1 ] # Building dataset X_train = torch . tensor ( X [ rand_idx , :], dtype = torch . float32 ) y_train = torch . tensor ( y [ rand_idx , :], dtype = torch . float32 ) if return_idx is False : return X_train , y_train else : return X_train , y_train , rand_idx","title":"create_dataset()"},{"location":"api/data/base/#deepymod.data.base.Dataset_2D","text":"This class automatically generates all the necessary proporties of a predifined data set with two spatial dimension as input. In particular it calculates the solution, the time derivative and the library. Note that all the pytorch opperations such as automatic differentiation can be used on the results.","title":"Dataset_2D"},{"location":"api/data/base/#deepymod.data.base.Dataset_2D.create_dataset","text":"Function creates the data set in the precise format used by DeepMoD Parameters: Name Type Description Default x Tensor Input vector of spatial coordinates required t Tensor Input vector of temporal coordinates required n_samples int Number of samples, set n_samples=0 for all. required noise float Noise level in percentage of std. required random bool When true, data set is randomised. Defaults to True. True normalize bool When true, data set is normalized. Defaults to True. required return_idx bool When true, the id of the data, before randomizing is returned. Defaults to False. False random_state int Seed of the randomisation. Defaults to 42. 42 Returns: Type Description [type] Tensor containing the input and output and optionally the randomisation. Source code in deepymod/data/base.py 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 def create_dataset ( self , x , t , n_samples , noise , random = True , return_idx = False , random_state = 42 ): \"\"\"Function creates the data set in the precise format used by DeepMoD Args: x (Tensor): Input vector of spatial coordinates t (Tensor): Input vector of temporal coordinates n_samples (int): Number of samples, set n_samples=0 for all. noise (float): Noise level in percentage of std. random (bool, optional): When true, data set is randomised. Defaults to True. normalize (bool, optional): When true, data set is normalized. Defaults to True. return_idx (bool, optional): When true, the id of the data, before randomizing is returned. Defaults to False. random_state (int, optional): Seed of the randomisation. Defaults to 42. Returns: [type]: Tensor containing the input and output and optionally the randomisation. \"\"\" assert (( x . shape [ 1 ] == 2 ) & ( t . shape [ 1 ] == 1 )), 'x and t should have shape (n_samples x 1)' u = self . generate_solution ( x , t ) X = np . concatenate ([ t , x ], axis = 1 ) y = u + noise * np . std ( u , axis = 0 ) * np . random . normal ( size = u . shape ) # creating random idx for samples N = y . shape [ 0 ] if n_samples == 0 else n_samples if random is True : rand_idx = np . random . RandomState ( seed = random_state ) . permutation ( y . shape [ 0 ])[: N ] # so we can get similar splits for different noise levels else : rand_idx = np . arange ( y . shape [ 0 ])[: N ] # Building dataset X_train = torch . tensor ( X [ rand_idx , :], requires_grad = True , dtype = torch . float32 ) y_train = torch . tensor ( y [ rand_idx , :], requires_grad = True , dtype = torch . float32 ) if return_idx is False : return X_train , y_train else : return X_train , y_train , rand_idx","title":"create_dataset()"},{"location":"api/data/base/#deepymod.data.base.pytorch_func","text":"Decorator to automatically transform arrays to tensors and back Parameters: Name Type Description Default function Tensor Pytorch tensor required Returns: Type Description Numpy array Source code in deepymod/data/base.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 def pytorch_func ( function ): \"\"\"Decorator to automatically transform arrays to tensors and back Args: function (Tensor): Pytorch tensor Returns: Numpy array \"\"\" def wrapper ( self , * args , ** kwargs ): torch_args = [ torch . tensor ( arg , requires_grad = True , dtype = torch . float64 ) if type ( arg ) is ndarray else arg for arg in args ] torch_kwargs = { key : torch . tensor ( kwarg , requires_grad = True , dtype = torch . float64 ) if type ( kwarg ) is ndarray else kwarg for key , kwarg in kwargs . items ()} result = function ( self , * torch_args , ** torch_kwargs ) return result . cpu () . detach () . numpy () return wrapper","title":"pytorch_func()"},{"location":"api/data/burgers/","text":"BurgersCos ( x , t , v , a , b , k ) Function to generate analytical solutions of Burgers equation with cosine initial condition: u(x, 0) = b + a \\cos(kx) u(x, 0) = b + a \\cos(kx) Source: https://www.iist.ac.in/sites/default/files/people/IN08026/Burgers_equation_viscous.pdf Parameters: Name Type Description Default x <built-in method tensor of type object at 0x10d710ad0> Input vector of spatial coordinates. required t <built-in method tensor of type object at 0x10d710ad0> Input vector of temporal coordinates. required v float Velocity. required a float Amplitude of the initial periodic condition. required b float Offset of the initial condition. required k float Wavenumber of the initial condition. required Returns: Type Description [Tensor] solution. Source code in deepymod/data/burgers/burgers.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 def BurgersCos ( x : torch . tensor , t : torch . tensor , v : float , a : float , b : float , k : float ): \"\"\"Function to generate analytical solutions of Burgers equation with cosine initial condition: $u(x, 0) = b + a \\cos(kx)$ Source: https://www.iist.ac.in/sites/default/files/people/IN08026/Burgers_equation_viscous.pdf Args: x ([Tensor]): Input vector of spatial coordinates. t ([Tensor]): Input vector of temporal coordinates. v (Float): Velocity. a ([Float]): Amplitude of the initial periodic condition. b ([Float]): Offset of the initial condition. k ([Float]): Wavenumber of the initial condition. Returns: [Tensor]: solution. \"\"\" z = v * k ** 2 * t u = ( 2 * v * a * k * torch . exp ( - z ) * torch . sin ( k * x )) / ( b + a * torch . exp ( - z ) * torch . cos ( k * x )) return u BurgersDelta ( x , t , v , A ) Function to generate analytical solutions of Burgers equation with delta peak initial condition: u(x, 0) = A delta(x) Source: https://www.iist.ac.in/sites/default/files/people/IN08026/Burgers_equation_viscous.pdf Note that this source has an error in the erfc prefactor, should be sqrt(pi)/2, not sqrt(pi/2). Parameters: Name Type Description Default x <built-in method tensor of type object at 0x10d710ad0> Input vector of spatial coordinates. required t <built-in method tensor of type object at 0x10d710ad0> Input vector of temporal coordinates. required v float Velocity. required A float Amplitude of the initial condition. required Returns: Type Description <built-in method tensor of type object at 0x10d710ad0> Source code in deepymod/data/burgers/burgers.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 def BurgersDelta ( x : torch . tensor , t : torch . tensor , v : float , A : float ) -> torch . tensor : \"\"\" Function to generate analytical solutions of Burgers equation with delta peak initial condition: u(x, 0) = A delta(x) Source: https://www.iist.ac.in/sites/default/files/people/IN08026/Burgers_equation_viscous.pdf Note that this source has an error in the erfc prefactor, should be sqrt(pi)/2, not sqrt(pi/2). Args: x ([Tensor]): Input vector of spatial coordinates. t ([Tensor]): Input vector of temporal coordinates. v (Float): Velocity. A (Float): Amplitude of the initial condition. Returns: [Tensor]: solution. \"\"\" R = torch . tensor ( A / ( 2 * v )) # otherwise throws error z = x / torch . sqrt ( 4 * v * t ) u = torch . sqrt ( v / ( pi * t )) * (( torch . exp ( R ) - 1 ) * torch . exp ( - z ** 2 )) / ( 1 + ( torch . exp ( R ) - 1 ) / 2 * torch . erfc ( z )) return u BurgersSawtooth ( x , t , v ) Function to generate analytical solutions of Burgers equation with sawtooth initial condition (see soruce for exact expression). Solution only valid between for x in [0, 2pi] and t in [0, 0.5] http://www.thevisualroom.com/02_barba_projects/burgers_equation.html Parameters: Name Type Description Default x <built-in method tensor of type object at 0x10d710ad0> Input vector of spatial coordinates. required t <built-in method tensor of type object at 0x10d710ad0> Input vector of temporal coordinates. required v float Velocity. required Returns: Type Description <built-in method tensor of type object at 0x10d710ad0> Source code in deepymod/data/burgers/burgers.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 def BurgersSawtooth ( x : torch . tensor , t : torch . tensor , v : float ) -> torch . tensor : \"\"\"Function to generate analytical solutions of Burgers equation with sawtooth initial condition (see soruce for exact expression). Solution only valid between for x in [0, 2pi] and t in [0, 0.5] http://www.thevisualroom.com/02_barba_projects/burgers_equation.html Args: x ([Tensor]): Input vector of spatial coordinates. t ([Tensor]): Input vector of temporal coordinates. v (Float): Velocity. Returns: [Tensor]: solution. \"\"\" z_left = ( x - 4 * t ) z_right = ( x - 4 * t - 2 * pi ) l = 4 * v * ( t + 1 ) phi = torch . exp ( - z_left ** 2 / l ) + torch . exp ( - z_right ** 2 / l ) dphi_x = - 2 * z_left / l * torch . exp ( - z_left ** 2 / l ) - 2 * z_right / l * torch . exp ( - z_right ** 2 / l ) u = - 2 * v * dphi_x / phi + 4 return u","title":"Burgers"},{"location":"api/data/burgers/#deepymod.data.burgers.burgers","text":"","title":"deepymod.data.burgers.burgers"},{"location":"api/data/burgers/#deepymod.data.burgers.burgers.BurgersCos","text":"Function to generate analytical solutions of Burgers equation with cosine initial condition: u(x, 0) = b + a \\cos(kx) u(x, 0) = b + a \\cos(kx) Source: https://www.iist.ac.in/sites/default/files/people/IN08026/Burgers_equation_viscous.pdf Parameters: Name Type Description Default x <built-in method tensor of type object at 0x10d710ad0> Input vector of spatial coordinates. required t <built-in method tensor of type object at 0x10d710ad0> Input vector of temporal coordinates. required v float Velocity. required a float Amplitude of the initial periodic condition. required b float Offset of the initial condition. required k float Wavenumber of the initial condition. required Returns: Type Description [Tensor] solution. Source code in deepymod/data/burgers/burgers.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 def BurgersCos ( x : torch . tensor , t : torch . tensor , v : float , a : float , b : float , k : float ): \"\"\"Function to generate analytical solutions of Burgers equation with cosine initial condition: $u(x, 0) = b + a \\cos(kx)$ Source: https://www.iist.ac.in/sites/default/files/people/IN08026/Burgers_equation_viscous.pdf Args: x ([Tensor]): Input vector of spatial coordinates. t ([Tensor]): Input vector of temporal coordinates. v (Float): Velocity. a ([Float]): Amplitude of the initial periodic condition. b ([Float]): Offset of the initial condition. k ([Float]): Wavenumber of the initial condition. Returns: [Tensor]: solution. \"\"\" z = v * k ** 2 * t u = ( 2 * v * a * k * torch . exp ( - z ) * torch . sin ( k * x )) / ( b + a * torch . exp ( - z ) * torch . cos ( k * x )) return u","title":"BurgersCos()"},{"location":"api/data/burgers/#deepymod.data.burgers.burgers.BurgersDelta","text":"Function to generate analytical solutions of Burgers equation with delta peak initial condition: u(x, 0) = A delta(x) Source: https://www.iist.ac.in/sites/default/files/people/IN08026/Burgers_equation_viscous.pdf Note that this source has an error in the erfc prefactor, should be sqrt(pi)/2, not sqrt(pi/2). Parameters: Name Type Description Default x <built-in method tensor of type object at 0x10d710ad0> Input vector of spatial coordinates. required t <built-in method tensor of type object at 0x10d710ad0> Input vector of temporal coordinates. required v float Velocity. required A float Amplitude of the initial condition. required Returns: Type Description <built-in method tensor of type object at 0x10d710ad0> Source code in deepymod/data/burgers/burgers.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 def BurgersDelta ( x : torch . tensor , t : torch . tensor , v : float , A : float ) -> torch . tensor : \"\"\" Function to generate analytical solutions of Burgers equation with delta peak initial condition: u(x, 0) = A delta(x) Source: https://www.iist.ac.in/sites/default/files/people/IN08026/Burgers_equation_viscous.pdf Note that this source has an error in the erfc prefactor, should be sqrt(pi)/2, not sqrt(pi/2). Args: x ([Tensor]): Input vector of spatial coordinates. t ([Tensor]): Input vector of temporal coordinates. v (Float): Velocity. A (Float): Amplitude of the initial condition. Returns: [Tensor]: solution. \"\"\" R = torch . tensor ( A / ( 2 * v )) # otherwise throws error z = x / torch . sqrt ( 4 * v * t ) u = torch . sqrt ( v / ( pi * t )) * (( torch . exp ( R ) - 1 ) * torch . exp ( - z ** 2 )) / ( 1 + ( torch . exp ( R ) - 1 ) / 2 * torch . erfc ( z )) return u","title":"BurgersDelta()"},{"location":"api/data/burgers/#deepymod.data.burgers.burgers.BurgersSawtooth","text":"Function to generate analytical solutions of Burgers equation with sawtooth initial condition (see soruce for exact expression). Solution only valid between for x in [0, 2pi] and t in [0, 0.5] http://www.thevisualroom.com/02_barba_projects/burgers_equation.html Parameters: Name Type Description Default x <built-in method tensor of type object at 0x10d710ad0> Input vector of spatial coordinates. required t <built-in method tensor of type object at 0x10d710ad0> Input vector of temporal coordinates. required v float Velocity. required Returns: Type Description <built-in method tensor of type object at 0x10d710ad0> Source code in deepymod/data/burgers/burgers.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 def BurgersSawtooth ( x : torch . tensor , t : torch . tensor , v : float ) -> torch . tensor : \"\"\"Function to generate analytical solutions of Burgers equation with sawtooth initial condition (see soruce for exact expression). Solution only valid between for x in [0, 2pi] and t in [0, 0.5] http://www.thevisualroom.com/02_barba_projects/burgers_equation.html Args: x ([Tensor]): Input vector of spatial coordinates. t ([Tensor]): Input vector of temporal coordinates. v (Float): Velocity. Returns: [Tensor]: solution. \"\"\" z_left = ( x - 4 * t ) z_right = ( x - 4 * t - 2 * pi ) l = 4 * v * ( t + 1 ) phi = torch . exp ( - z_left ** 2 / l ) + torch . exp ( - z_right ** 2 / l ) dphi_x = - 2 * z_left / l * torch . exp ( - z_left ** 2 / l ) - 2 * z_right / l * torch . exp ( - z_right ** 2 / l ) u = - 2 * v * dphi_x / phi + 4 return u","title":"BurgersSawtooth()"},{"location":"api/data/kdv/","text":"DoubleSoliton ( x , t , c , x0 ) Single soliton solution of the KdV equation (u_t + u_{xxx} - 6 u u_x = 0) source: http://lie.math.brocku.ca/~sanco/solitons/kdv_solitons.php Parameters: Name Type Description Default x <built-in method tensor of type object at 0x10d710ad0> Input vector of spatial coordinates. required t <built-in method tensor of type object at 0x10d710ad0> Input vector of temporal coordinates. required c float Array containing the velocities of the two solitons, note that c[0] > c[1]. required x0 float Array containing the offsets of the two solitons. required Returns: Type Description <built-in method tensor of type object at 0x10d710ad0> Source code in deepymod/data/kdv/kdv.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 def DoubleSoliton ( x : torch . tensor , t : torch . tensor , c : float , x0 : float ) -> torch . tensor : \"\"\" Single soliton solution of the KdV equation (u_t + u_{xxx} - 6 u u_x = 0) source: http://lie.math.brocku.ca/~sanco/solitons/kdv_solitons.php Args: x ([Tensor]): Input vector of spatial coordinates. t ([Tensor]): Input vector of temporal coordinates. c ([Array]): Array containing the velocities of the two solitons, note that c[0] > c[1]. x0 ([Array]): Array containing the offsets of the two solitons. Returns: [Tensor]: Solution. \"\"\" assert c [ 0 ] > c [ 1 ], 'c1 has to be bigger than c[2]' xi0 = np . sqrt ( c [ 0 ]) / 2 * ( x - c [ 0 ] * t - x0 [ 0 ]) # switch to moving coordinate frame xi1 = np . sqrt ( c [ 1 ]) / 2 * ( x - c [ 1 ] * t - x0 [ 1 ]) part_1 = 2 * ( c [ 0 ] - c [ 1 ]) numerator = c [ 0 ] * torch . cosh ( xi1 ) ** 2 + c [ 1 ] * torch . sinh ( xi0 ) ** 2 denominator_1 = ( np . sqrt ( c [ 0 ]) - np . sqrt ( c [ 1 ])) * torch . cosh ( xi0 + xi1 ) denominator_2 = ( np . sqrt ( c [ 0 ]) + np . sqrt ( c [ 1 ])) * torch . cosh ( xi0 - xi1 ) u = part_1 * numerator / ( denominator_1 + denominator_2 ) ** 2 return u SingleSoliton ( x , t , c , x0 ) Single soliton solution of the KdV equation (u_t + u_{xxx} - 6 u u_x = 0) Parameters: Name Type Description Default x <built-in method tensor of type object at 0x10d710ad0> Input vector of spatial coordinates. required t <built-in method tensor of type object at 0x10d710ad0> Input vector of temporal coordinates. required c float Velocity. required x0 float Offset. required Returns: Type Description <built-in method tensor of type object at 0x10d710ad0> Source code in deepymod/data/kdv/kdv.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 def SingleSoliton ( x : torch . tensor , t : torch . tensor , c : float , x0 : float ) -> torch . tensor : \"\"\"Single soliton solution of the KdV equation (u_t + u_{xxx} - 6 u u_x = 0) Args: x ([Tensor]): Input vector of spatial coordinates. t ([Tensor]): Input vector of temporal coordinates. c ([Float]): Velocity. x0 ([Float]): Offset. Returns: [Tensor]: Solution. \"\"\" xi = np . sqrt ( c ) / 2 * ( x - c * t - x0 ) # switch to moving coordinate frame u = c / 2 * 1 / torch . cosh ( xi ) ** 2 return u","title":"Korteweg-de Vries"},{"location":"api/data/kdv/#deepymod.data.kdv.kdv","text":"","title":"deepymod.data.kdv.kdv"},{"location":"api/data/kdv/#deepymod.data.kdv.kdv.DoubleSoliton","text":"Single soliton solution of the KdV equation (u_t + u_{xxx} - 6 u u_x = 0) source: http://lie.math.brocku.ca/~sanco/solitons/kdv_solitons.php Parameters: Name Type Description Default x <built-in method tensor of type object at 0x10d710ad0> Input vector of spatial coordinates. required t <built-in method tensor of type object at 0x10d710ad0> Input vector of temporal coordinates. required c float Array containing the velocities of the two solitons, note that c[0] > c[1]. required x0 float Array containing the offsets of the two solitons. required Returns: Type Description <built-in method tensor of type object at 0x10d710ad0> Source code in deepymod/data/kdv/kdv.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 def DoubleSoliton ( x : torch . tensor , t : torch . tensor , c : float , x0 : float ) -> torch . tensor : \"\"\" Single soliton solution of the KdV equation (u_t + u_{xxx} - 6 u u_x = 0) source: http://lie.math.brocku.ca/~sanco/solitons/kdv_solitons.php Args: x ([Tensor]): Input vector of spatial coordinates. t ([Tensor]): Input vector of temporal coordinates. c ([Array]): Array containing the velocities of the two solitons, note that c[0] > c[1]. x0 ([Array]): Array containing the offsets of the two solitons. Returns: [Tensor]: Solution. \"\"\" assert c [ 0 ] > c [ 1 ], 'c1 has to be bigger than c[2]' xi0 = np . sqrt ( c [ 0 ]) / 2 * ( x - c [ 0 ] * t - x0 [ 0 ]) # switch to moving coordinate frame xi1 = np . sqrt ( c [ 1 ]) / 2 * ( x - c [ 1 ] * t - x0 [ 1 ]) part_1 = 2 * ( c [ 0 ] - c [ 1 ]) numerator = c [ 0 ] * torch . cosh ( xi1 ) ** 2 + c [ 1 ] * torch . sinh ( xi0 ) ** 2 denominator_1 = ( np . sqrt ( c [ 0 ]) - np . sqrt ( c [ 1 ])) * torch . cosh ( xi0 + xi1 ) denominator_2 = ( np . sqrt ( c [ 0 ]) + np . sqrt ( c [ 1 ])) * torch . cosh ( xi0 - xi1 ) u = part_1 * numerator / ( denominator_1 + denominator_2 ) ** 2 return u","title":"DoubleSoliton()"},{"location":"api/data/kdv/#deepymod.data.kdv.kdv.SingleSoliton","text":"Single soliton solution of the KdV equation (u_t + u_{xxx} - 6 u u_x = 0) Parameters: Name Type Description Default x <built-in method tensor of type object at 0x10d710ad0> Input vector of spatial coordinates. required t <built-in method tensor of type object at 0x10d710ad0> Input vector of temporal coordinates. required c float Velocity. required x0 float Offset. required Returns: Type Description <built-in method tensor of type object at 0x10d710ad0> Source code in deepymod/data/kdv/kdv.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 def SingleSoliton ( x : torch . tensor , t : torch . tensor , c : float , x0 : float ) -> torch . tensor : \"\"\"Single soliton solution of the KdV equation (u_t + u_{xxx} - 6 u u_x = 0) Args: x ([Tensor]): Input vector of spatial coordinates. t ([Tensor]): Input vector of temporal coordinates. c ([Float]): Velocity. x0 ([Float]): Offset. Returns: [Tensor]: Solution. \"\"\" xi = np . sqrt ( c ) / 2 * ( x - c * t - x0 ) # switch to moving coordinate frame u = c / 2 * 1 / torch . cosh ( xi ) ** 2 return u","title":"SingleSoliton()"},{"location":"background/background/","text":"Background Framework Deep learning-based model discovery typically uses a neural network to construct a noiseless surrogate \\hat{u} \\hat{u} of the data u u . A library of potential terms \\Theta \\Theta is constructed using automatic differentiation from \\hat{u} \\hat{u} and the neural network is constrained to solutions allowed by this library . The loss function of the network thus consists of two contributions, (i) a mean square error to learn the mapping (\\vec{x},t) \\rightarrow \\hat{u} (\\vec{x},t) \\rightarrow \\hat{u} and (ii) a term to constrain the network, $$ \\mathcal{L} = \\frac{1}{N}\\sum_{i=1}^{N}\\left( u_i - \\hat{u} i \\right) ^2 +\\frac{1}{N}\\sum {i=1}^{N}\\left( \\partial_t \\hat{u} i - \\Theta {i}\\xi \\right)^2 . \\label{eq:deepmod} $$ The sparse coefficient vector \\xi \\xi is learned concurrently with the network parameters and plays two roles: 1) determining the active (i.e. non-zero) components of the underlying PDE and 2) constraining the network according to these active terms. We propose to separate these two tasks by decoupling the constraint from the sparsity selection process itself. We first calculate a sparsity mask g g and then constrain the network only by the active terms in the mask. Mathematically, we replace \\xi \\xi by \\xi \\circ \\ g \\xi \\circ \\ g . The sparsity mask g g need not be calculated differentiably, so that any classical, non-differentiable sparse estimator can be used. Our approach has several additional advantages: i) It provides an unbiased estimate of the coefficient vector since we do not apply l_1 l_1 or l_2 l_2 regularisation on \\xi \\xi , ii) the sparsity pattern is determined from the full library \\Theta \\Theta , rather than only from the remaining active terms, allowing dynamic addition and removal of active terms throughout training, and iii) we can use cross validation or similar methods in the sparse estimator to find the optimal hyperparameters for model selection. Using this change, we constructed a general framework for deep learning based model discovery with any classical sparsity promoting algorithm in the above. A function approximator constructs a surrogate of the data, (II) from which a Library of possible terms and the time derivative is constructed using automatic differentiation. (III) A sparsity estimator selects the active terms in the library using sparse regression and (IV) the function approximator is constrained to solutions allowed by the active terms by the constraint . Training As the sparsity estimator is non-differentiable, determining the sparsity mask before the function approximator has reasonably approximated the data can adversely affect training if the wrong terms are selected. We thus split the dataset into a train- and test-set and update the sparsity mask only when the MSE on the test-set starts to increase. After updating the mask, the model needs to adjust to the tighter constraint and we hence update the sparsity pattern every 25 epochs after the first update. Final convergence is reached when the l_1 l_1 norm of the coefficient vector remains constant. In practice we observe that large datasets with little noise might discover the correct equation after a single sparsity update, but that highly noisy datasets typically require several updates, removing only a few terms at a time.","title":"Background"},{"location":"background/background/#background","text":"","title":"Background"},{"location":"background/background/#framework","text":"Deep learning-based model discovery typically uses a neural network to construct a noiseless surrogate \\hat{u} \\hat{u} of the data u u . A library of potential terms \\Theta \\Theta is constructed using automatic differentiation from \\hat{u} \\hat{u} and the neural network is constrained to solutions allowed by this library . The loss function of the network thus consists of two contributions, (i) a mean square error to learn the mapping (\\vec{x},t) \\rightarrow \\hat{u} (\\vec{x},t) \\rightarrow \\hat{u} and (ii) a term to constrain the network, $$ \\mathcal{L} = \\frac{1}{N}\\sum_{i=1}^{N}\\left( u_i - \\hat{u} i \\right) ^2 +\\frac{1}{N}\\sum {i=1}^{N}\\left( \\partial_t \\hat{u} i - \\Theta {i}\\xi \\right)^2 . \\label{eq:deepmod} $$ The sparse coefficient vector \\xi \\xi is learned concurrently with the network parameters and plays two roles: 1) determining the active (i.e. non-zero) components of the underlying PDE and 2) constraining the network according to these active terms. We propose to separate these two tasks by decoupling the constraint from the sparsity selection process itself. We first calculate a sparsity mask g g and then constrain the network only by the active terms in the mask. Mathematically, we replace \\xi \\xi by \\xi \\circ \\ g \\xi \\circ \\ g . The sparsity mask g g need not be calculated differentiably, so that any classical, non-differentiable sparse estimator can be used. Our approach has several additional advantages: i) It provides an unbiased estimate of the coefficient vector since we do not apply l_1 l_1 or l_2 l_2 regularisation on \\xi \\xi , ii) the sparsity pattern is determined from the full library \\Theta \\Theta , rather than only from the remaining active terms, allowing dynamic addition and removal of active terms throughout training, and iii) we can use cross validation or similar methods in the sparse estimator to find the optimal hyperparameters for model selection. Using this change, we constructed a general framework for deep learning based model discovery with any classical sparsity promoting algorithm in the above. A function approximator constructs a surrogate of the data, (II) from which a Library of possible terms and the time derivative is constructed using automatic differentiation. (III) A sparsity estimator selects the active terms in the library using sparse regression and (IV) the function approximator is constrained to solutions allowed by the active terms by the constraint .","title":"Framework"},{"location":"background/background/#training","text":"As the sparsity estimator is non-differentiable, determining the sparsity mask before the function approximator has reasonably approximated the data can adversely affect training if the wrong terms are selected. We thus split the dataset into a train- and test-set and update the sparsity mask only when the MSE on the test-set starts to increase. After updating the mask, the model needs to adjust to the tighter constraint and we hence update the sparsity pattern every 25 epochs after the first update. Final convergence is reached when the l_1 l_1 norm of the coefficient vector remains constant. In practice we observe that large datasets with little noise might discover the correct equation after a single sparsity update, but that highly noisy datasets typically require several updates, removing only a few terms at a time.","title":"Training"},{"location":"examples/2DAD/2DAD/","text":"2D Advection-Diffusion equation in this notebook we provide a simple example of the DeepMoD algorithm and apply it on the 2D advection-diffusion equation. # General imports import numpy as np import torch import matplotlib.pylab as plt # DeepMoD functions from deepymod import DeepMoD from deepymod.model.func_approx import NN from deepymod.model.library import Library2D from deepymod.model.constraint import LeastSquares from deepymod.model.sparse_estimators import Threshold , PDEFIND from deepymod.training import train from deepymod.training.sparsity_scheduler import TrainTestPeriodic from scipy.io import loadmat # Settings for reproducibility np . random . seed ( 42 ) torch . manual_seed ( 0 ) % load_ext autoreload % autoreload 2 Prepare the data Next, we prepare the dataset. data = loadmat ( 'data/advection_diffusion.mat' ) usol = np . real ( data [ 'Expression1' ]) usol = usol . reshape (( 51 , 51 , 61 , 4 )) x_v = usol [:,:,:, 0 ] y_v = usol [:,:,:, 1 ] t_v = usol [:,:,:, 2 ] u_v = usol [:,:,:, 3 ] Next we plot the dataset for three different time-points fig , axes = plt . subplots ( ncols = 3 , figsize = ( 15 , 4 )) im0 = axes [ 0 ] . contourf ( x_v [:,:, 0 ], y_v [:,:, 0 ], u_v [:,:, 0 ], cmap = 'coolwarm' ) axes [ 0 ] . set_xlabel ( 'x' ) axes [ 0 ] . set_ylabel ( 'y' ) axes [ 0 ] . set_title ( 't = 0' ) im1 = axes [ 1 ] . contourf ( x_v [:,:, 10 ], y_v [:,:, 10 ], u_v [:,:, 10 ], cmap = 'coolwarm' ) axes [ 1 ] . set_xlabel ( 'x' ) axes [ 1 ] . set_title ( 't = 10' ) im2 = axes [ 2 ] . contourf ( x_v [:,:, 20 ], y_v [:,:, 20 ], u_v [:,:, 20 ], cmap = 'coolwarm' ) axes [ 2 ] . set_xlabel ( 'x' ) axes [ 2 ] . set_title ( 't= 20' ) fig . colorbar ( im1 , ax = axes . ravel () . tolist ()) plt . show () We flatten it to give it the right dimensions for feeding it to the network: X = np . transpose (( t_v . flatten (), x_v . flatten (), y_v . flatten ())) y = np . float32 ( u_v . reshape (( u_v . size , 1 ))) We select the noise level we add to the data-set noise_level = 0.01 y_noisy = y + noise_level * np . std ( y ) * np . random . randn ( y . size , 1 ) Select the number of samples: number_of_samples = 1000 idx = np . random . permutation ( y . shape [ 0 ]) X_train = torch . tensor ( X [ idx , :][: number_of_samples ], dtype = torch . float32 , requires_grad = True ) y_train = torch . tensor ( y [ idx , :][: number_of_samples ], dtype = torch . float32 ) Configuration of DeepMoD Configuration of the function approximator: Here the first argument is the number of input and the last argument the number of output layers. network = NN ( 3 , [ 50 , 50 , 50 , 50 ], 1 ) Configuration of the library function: We select athe library with a 2D spatial input. Note that that the max differential order has been pre-determined here out of convinience. So, for poly_order 1 the library contains the following 12 terms: * [ 1, u_x, u_y, u_{xx}, u_{yy}, u_{xy}, u, u u_x, u u_y, u u_{xx}, u u_{yy}, u u_{xy} 1, u_x, u_y, u_{xx}, u_{yy}, u_{xy}, u, u u_x, u u_y, u u_{xx}, u u_{yy}, u u_{xy} ] library = Library2D ( poly_order = 1 ) Configuration of the sparsity estimator and sparsity scheduler used. In this case we use the most basic threshold-based Lasso estimator and a scheduler that asseses the validation loss after a given patience. If that value is smaller than 1e-5, the algorithm is converged. estimator = Threshold ( 0.1 ) sparsity_scheduler = TrainTestPeriodic ( periodicity = 50 , patience = 10 , delta = 1e-5 ) Configuration of the sparsity estimator constraint = LeastSquares () # Configuration of the sparsity scheduler Now we instantiate the model and select the optimizer model = DeepMoD ( network , library , estimator , constraint ) # Defining optimizer optimizer = torch . optim . Adam ( model . parameters (), betas = ( 0.99 , 0.99 ), amsgrad = True , lr = 1e-3 ) Run DeepMoD We can now run DeepMoD using all the options we have set and the training data: * The directory where the tensorboard file is written (log_dir) * The ratio of train/test set used (split) * The maximum number of iterations performed (max_iterations) * The absolute change in L1 norm considered converged (delta) * The amount of epochs over which the absolute change in L1 norm is calculated (patience) train ( model , X_train , y_train , optimizer , sparsity_scheduler , log_dir = 'runs/2DAD/' , split = 0.8 , max_iterations = 100000 , delta = 1e-4 , patience = 8 ) | Iteration | Progress | Time remaining | Loss | MSE | Reg | L1 norm | 7000 7.00% 3733s 1.07e-04 3.60e-05 7.08e-05 1.87e+00 Algorithm converged. Stopping training. Sparsity masks provide the active and non-active terms in the PDE: model . sparsity_masks [tensor([False, True, True, True, True, False, False, False, False, False, False, False])] estimatior_coeffs gives the magnitude of the active terms: print ( model . estimator_coeffs ()) [array([[0. ], [0.3770935 ], [0.7139108 ], [0.389949 ], [0.32122847], [0. ], [0. ], [0. ], [0. ], [0. ], [0. ], [0. ]], dtype=float32)]","title":"2D Advection Diffusion"},{"location":"examples/2DAD/2DAD/#2d-advection-diffusion-equation","text":"in this notebook we provide a simple example of the DeepMoD algorithm and apply it on the 2D advection-diffusion equation. # General imports import numpy as np import torch import matplotlib.pylab as plt # DeepMoD functions from deepymod import DeepMoD from deepymod.model.func_approx import NN from deepymod.model.library import Library2D from deepymod.model.constraint import LeastSquares from deepymod.model.sparse_estimators import Threshold , PDEFIND from deepymod.training import train from deepymod.training.sparsity_scheduler import TrainTestPeriodic from scipy.io import loadmat # Settings for reproducibility np . random . seed ( 42 ) torch . manual_seed ( 0 ) % load_ext autoreload % autoreload 2","title":"2D Advection-Diffusion equation"},{"location":"examples/2DAD/2DAD/#prepare-the-data","text":"Next, we prepare the dataset. data = loadmat ( 'data/advection_diffusion.mat' ) usol = np . real ( data [ 'Expression1' ]) usol = usol . reshape (( 51 , 51 , 61 , 4 )) x_v = usol [:,:,:, 0 ] y_v = usol [:,:,:, 1 ] t_v = usol [:,:,:, 2 ] u_v = usol [:,:,:, 3 ] Next we plot the dataset for three different time-points fig , axes = plt . subplots ( ncols = 3 , figsize = ( 15 , 4 )) im0 = axes [ 0 ] . contourf ( x_v [:,:, 0 ], y_v [:,:, 0 ], u_v [:,:, 0 ], cmap = 'coolwarm' ) axes [ 0 ] . set_xlabel ( 'x' ) axes [ 0 ] . set_ylabel ( 'y' ) axes [ 0 ] . set_title ( 't = 0' ) im1 = axes [ 1 ] . contourf ( x_v [:,:, 10 ], y_v [:,:, 10 ], u_v [:,:, 10 ], cmap = 'coolwarm' ) axes [ 1 ] . set_xlabel ( 'x' ) axes [ 1 ] . set_title ( 't = 10' ) im2 = axes [ 2 ] . contourf ( x_v [:,:, 20 ], y_v [:,:, 20 ], u_v [:,:, 20 ], cmap = 'coolwarm' ) axes [ 2 ] . set_xlabel ( 'x' ) axes [ 2 ] . set_title ( 't= 20' ) fig . colorbar ( im1 , ax = axes . ravel () . tolist ()) plt . show () We flatten it to give it the right dimensions for feeding it to the network: X = np . transpose (( t_v . flatten (), x_v . flatten (), y_v . flatten ())) y = np . float32 ( u_v . reshape (( u_v . size , 1 ))) We select the noise level we add to the data-set noise_level = 0.01 y_noisy = y + noise_level * np . std ( y ) * np . random . randn ( y . size , 1 ) Select the number of samples: number_of_samples = 1000 idx = np . random . permutation ( y . shape [ 0 ]) X_train = torch . tensor ( X [ idx , :][: number_of_samples ], dtype = torch . float32 , requires_grad = True ) y_train = torch . tensor ( y [ idx , :][: number_of_samples ], dtype = torch . float32 )","title":"Prepare the data"},{"location":"examples/2DAD/2DAD/#configuration-of-deepmod","text":"Configuration of the function approximator: Here the first argument is the number of input and the last argument the number of output layers. network = NN ( 3 , [ 50 , 50 , 50 , 50 ], 1 ) Configuration of the library function: We select athe library with a 2D spatial input. Note that that the max differential order has been pre-determined here out of convinience. So, for poly_order 1 the library contains the following 12 terms: * [ 1, u_x, u_y, u_{xx}, u_{yy}, u_{xy}, u, u u_x, u u_y, u u_{xx}, u u_{yy}, u u_{xy} 1, u_x, u_y, u_{xx}, u_{yy}, u_{xy}, u, u u_x, u u_y, u u_{xx}, u u_{yy}, u u_{xy} ] library = Library2D ( poly_order = 1 ) Configuration of the sparsity estimator and sparsity scheduler used. In this case we use the most basic threshold-based Lasso estimator and a scheduler that asseses the validation loss after a given patience. If that value is smaller than 1e-5, the algorithm is converged. estimator = Threshold ( 0.1 ) sparsity_scheduler = TrainTestPeriodic ( periodicity = 50 , patience = 10 , delta = 1e-5 ) Configuration of the sparsity estimator constraint = LeastSquares () # Configuration of the sparsity scheduler Now we instantiate the model and select the optimizer model = DeepMoD ( network , library , estimator , constraint ) # Defining optimizer optimizer = torch . optim . Adam ( model . parameters (), betas = ( 0.99 , 0.99 ), amsgrad = True , lr = 1e-3 )","title":"Configuration of DeepMoD"},{"location":"examples/2DAD/2DAD/#run-deepmod","text":"We can now run DeepMoD using all the options we have set and the training data: * The directory where the tensorboard file is written (log_dir) * The ratio of train/test set used (split) * The maximum number of iterations performed (max_iterations) * The absolute change in L1 norm considered converged (delta) * The amount of epochs over which the absolute change in L1 norm is calculated (patience) train ( model , X_train , y_train , optimizer , sparsity_scheduler , log_dir = 'runs/2DAD/' , split = 0.8 , max_iterations = 100000 , delta = 1e-4 , patience = 8 ) | Iteration | Progress | Time remaining | Loss | MSE | Reg | L1 norm | 7000 7.00% 3733s 1.07e-04 3.60e-05 7.08e-05 1.87e+00 Algorithm converged. Stopping training. Sparsity masks provide the active and non-active terms in the PDE: model . sparsity_masks [tensor([False, True, True, True, True, False, False, False, False, False, False, False])] estimatior_coeffs gives the magnitude of the active terms: print ( model . estimator_coeffs ()) [array([[0. ], [0.3770935 ], [0.7139108 ], [0.389949 ], [0.32122847], [0. ], [0. ], [0. ], [0. ], [0. ], [0. ], [0. ]], dtype=float32)]","title":"Run DeepMoD"},{"location":"examples/Burgers/PDE_Burgers/","text":"Example Burgers' equation In this notebook we provide a simple example of the DeepMoD algorithm by applying it on the Burgers' equation. We start by importing the required libraries and setting the plotting style: # General imports import numpy as np import torch import matplotlib.pylab as plt # DeepMoD functions from deepymod import DeepMoD from deepymod.model.func_approx import NN from deepymod.model.library import Library1D from deepymod.model.constraint import LeastSquares from deepymod.model.sparse_estimators import Threshold , PDEFIND from deepymod.training import train from deepymod.training.sparsity_scheduler import TrainTestPeriodic from scipy.io import loadmat # Settings for reproducibility np . random . seed ( 42 ) torch . manual_seed ( 0 ) % load_ext autoreload % autoreload 2 Next, we prepare the dataset. data = np . load ( 'data/burgers.npy' , allow_pickle = True ) . item () print ( 'Shape of grid:' , data [ 'x' ] . shape ) Shape of grid: (256, 101) Let's plot it to get an idea of the data: fig , ax = plt . subplots () im = ax . contourf ( data [ 'x' ], data [ 't' ], np . real ( data [ 'u' ])) ax . set_xlabel ( 'x' ) ax . set_ylabel ( 't' ) fig . colorbar ( mappable = im ) plt . show () X = np . transpose (( data [ 't' ] . flatten (), data [ 'x' ] . flatten ())) y = np . real ( data [ 'u' ]) . reshape (( data [ 'u' ] . size , 1 )) print ( X . shape , y . shape ) (25856, 2) (25856, 1) As we can see, X X has 2 dimensions, \\{x, t\\} \\{x, t\\} , while y y has only one, \\{u\\} \\{u\\} . Always explicity set the shape (i.e. N\\times 1 N\\times 1 , not N N ) or you'll get errors. This dataset is noiseless, so let's add 5\\% 5\\% noise: noise_level = 0.05 y_noisy = y + noise_level * np . std ( y ) * np . random . randn ( y [:, 0 ] . size , 1 ) The dataset is also much larger than needed, so let's hussle it and pick out a 1000 samples: number_of_samples = 2000 idx = np . random . permutation ( y . shape [ 0 ]) X_train = torch . tensor ( X [ idx , :][: number_of_samples ], dtype = torch . float32 , requires_grad = True ) y_train = torch . tensor ( y_noisy [ idx , :][: number_of_samples ], dtype = torch . float32 ) print ( X_train . shape , y_train . shape ) torch.Size([2000, 2]) torch.Size([2000, 1]) We now have a dataset which we can use. Let's plot, for a final time, the original dataset, the noisy set and the samples points: fig , axes = plt . subplots ( ncols = 3 , figsize = ( 15 , 4 )) im0 = axes [ 0 ] . contourf ( data [ 'x' ], data [ 't' ], np . real ( data [ 'u' ]), cmap = 'coolwarm' ) axes [ 0 ] . set_xlabel ( 'x' ) axes [ 0 ] . set_ylabel ( 't' ) axes [ 0 ] . set_title ( 'Ground truth' ) im1 = axes [ 1 ] . contourf ( data [ 'x' ], data [ 't' ], y_noisy . reshape ( data [ 'x' ] . shape ), cmap = 'coolwarm' ) axes [ 1 ] . set_xlabel ( 'x' ) axes [ 1 ] . set_title ( 'Noisy' ) sampled = np . array ([ y_noisy [ index , 0 ] if index in idx [: number_of_samples ] else np . nan for index in np . arange ( data [ 'x' ] . size )]) sampled = np . rot90 ( sampled . reshape ( data [ 'x' ] . shape )) #array needs to be rotated because of imshow im2 = axes [ 2 ] . imshow ( sampled , aspect = 'auto' , cmap = 'coolwarm' ) axes [ 2 ] . set_xlabel ( 'x' ) axes [ 2 ] . set_title ( 'Sampled' ) fig . colorbar ( im1 , ax = axes . ravel () . tolist ()) plt . show () Configuring DeepMoD Configuration of the function approximator: Here the first argument is the number of input and the last argument the number of output layers. network = NN ( 2 , [ 50 , 50 , 50 , 50 ], 1 ) Configuration of the library function: We select athe library with a 2D spatial input. Note that that the max differential order has been pre-determined here out of convinience. So, for poly_order 1 the library contains the following 12 terms: * [ 1, u_x, u_{xx}, u_{xxx}, u, u u_{x}, u u_{xx}, u u_{xxx}, u^2, u^2 u_{x}, u^2 u_{xx}, u^2 u_{xxx} 1, u_x, u_{xx}, u_{xxx}, u, u u_{x}, u u_{xx}, u u_{xxx}, u^2, u^2 u_{x}, u^2 u_{xx}, u^2 u_{xxx} ] library = Library1D ( poly_order = 2 , diff_order = 3 ) Configuration of the sparsity estimator and sparsity scheduler used. In this case we use the most basic threshold-based Lasso estimator and a scheduler that asseses the validation loss after a given patience. If that value is smaller than 1e-5, the algorithm is converged. estimator = Threshold ( 0.1 ) sparsity_scheduler = TrainTestPeriodic ( periodicity = 50 , patience = 200 , delta = 1e-5 ) Configuration of the sparsity estimator constraint = LeastSquares () # Configuration of the sparsity scheduler Now we instantiate the model and select the optimizer model = DeepMoD ( network , library , estimator , constraint ) # Defining optimizer optimizer = torch . optim . Adam ( model . parameters (), betas = ( 0.99 , 0.99 ), amsgrad = True , lr = 1e-3 ) Run DeepMoD We can now run DeepMoD using all the options we have set and the training data: * The directory where the tensorboard file is written (log_dir) * The ratio of train/test set used (split) * The maximum number of iterations performed (max_iterations) * The absolute change in L1 norm considered converged (delta) * The amount of epochs over which the absolute change in L1 norm is calculated (patience) train ( model , X_train , y_train , optimizer , sparsity_scheduler , log_dir = 'runs/Burgers/' , split = 0.8 , max_iterations = 100000 ) 9750 MSE: 8.48e-05 Reg: 1.16e-05 L1: 1.45e+00 Algorithm converged. Writing model to disk. Sparsity masks provide the active and non-active terms in the PDE: model . sparsity_masks [tensor([False, False, True, False, False, True, False, False, False, False, False, False])] estimatior_coeffs gives the magnitude of the active terms: print ( model . estimator_coeffs ()) [array([[ 0. ], [ 0. ], [ 0.40999228], [ 0. ], [ 0. ], [-0.99958825], [ 0. ], [ 0. ], [ 0. ], [ 0. ], [ 0. ], [ 0. ]], dtype=float32)]","title":"Burgers"},{"location":"examples/Burgers/PDE_Burgers/#example-burgers-equation","text":"In this notebook we provide a simple example of the DeepMoD algorithm by applying it on the Burgers' equation. We start by importing the required libraries and setting the plotting style: # General imports import numpy as np import torch import matplotlib.pylab as plt # DeepMoD functions from deepymod import DeepMoD from deepymod.model.func_approx import NN from deepymod.model.library import Library1D from deepymod.model.constraint import LeastSquares from deepymod.model.sparse_estimators import Threshold , PDEFIND from deepymod.training import train from deepymod.training.sparsity_scheduler import TrainTestPeriodic from scipy.io import loadmat # Settings for reproducibility np . random . seed ( 42 ) torch . manual_seed ( 0 ) % load_ext autoreload % autoreload 2 Next, we prepare the dataset. data = np . load ( 'data/burgers.npy' , allow_pickle = True ) . item () print ( 'Shape of grid:' , data [ 'x' ] . shape ) Shape of grid: (256, 101) Let's plot it to get an idea of the data: fig , ax = plt . subplots () im = ax . contourf ( data [ 'x' ], data [ 't' ], np . real ( data [ 'u' ])) ax . set_xlabel ( 'x' ) ax . set_ylabel ( 't' ) fig . colorbar ( mappable = im ) plt . show () X = np . transpose (( data [ 't' ] . flatten (), data [ 'x' ] . flatten ())) y = np . real ( data [ 'u' ]) . reshape (( data [ 'u' ] . size , 1 )) print ( X . shape , y . shape ) (25856, 2) (25856, 1) As we can see, X X has 2 dimensions, \\{x, t\\} \\{x, t\\} , while y y has only one, \\{u\\} \\{u\\} . Always explicity set the shape (i.e. N\\times 1 N\\times 1 , not N N ) or you'll get errors. This dataset is noiseless, so let's add 5\\% 5\\% noise: noise_level = 0.05 y_noisy = y + noise_level * np . std ( y ) * np . random . randn ( y [:, 0 ] . size , 1 ) The dataset is also much larger than needed, so let's hussle it and pick out a 1000 samples: number_of_samples = 2000 idx = np . random . permutation ( y . shape [ 0 ]) X_train = torch . tensor ( X [ idx , :][: number_of_samples ], dtype = torch . float32 , requires_grad = True ) y_train = torch . tensor ( y_noisy [ idx , :][: number_of_samples ], dtype = torch . float32 ) print ( X_train . shape , y_train . shape ) torch.Size([2000, 2]) torch.Size([2000, 1]) We now have a dataset which we can use. Let's plot, for a final time, the original dataset, the noisy set and the samples points: fig , axes = plt . subplots ( ncols = 3 , figsize = ( 15 , 4 )) im0 = axes [ 0 ] . contourf ( data [ 'x' ], data [ 't' ], np . real ( data [ 'u' ]), cmap = 'coolwarm' ) axes [ 0 ] . set_xlabel ( 'x' ) axes [ 0 ] . set_ylabel ( 't' ) axes [ 0 ] . set_title ( 'Ground truth' ) im1 = axes [ 1 ] . contourf ( data [ 'x' ], data [ 't' ], y_noisy . reshape ( data [ 'x' ] . shape ), cmap = 'coolwarm' ) axes [ 1 ] . set_xlabel ( 'x' ) axes [ 1 ] . set_title ( 'Noisy' ) sampled = np . array ([ y_noisy [ index , 0 ] if index in idx [: number_of_samples ] else np . nan for index in np . arange ( data [ 'x' ] . size )]) sampled = np . rot90 ( sampled . reshape ( data [ 'x' ] . shape )) #array needs to be rotated because of imshow im2 = axes [ 2 ] . imshow ( sampled , aspect = 'auto' , cmap = 'coolwarm' ) axes [ 2 ] . set_xlabel ( 'x' ) axes [ 2 ] . set_title ( 'Sampled' ) fig . colorbar ( im1 , ax = axes . ravel () . tolist ()) plt . show ()","title":"Example Burgers' equation"},{"location":"examples/Burgers/PDE_Burgers/#configuring-deepmod","text":"Configuration of the function approximator: Here the first argument is the number of input and the last argument the number of output layers. network = NN ( 2 , [ 50 , 50 , 50 , 50 ], 1 ) Configuration of the library function: We select athe library with a 2D spatial input. Note that that the max differential order has been pre-determined here out of convinience. So, for poly_order 1 the library contains the following 12 terms: * [ 1, u_x, u_{xx}, u_{xxx}, u, u u_{x}, u u_{xx}, u u_{xxx}, u^2, u^2 u_{x}, u^2 u_{xx}, u^2 u_{xxx} 1, u_x, u_{xx}, u_{xxx}, u, u u_{x}, u u_{xx}, u u_{xxx}, u^2, u^2 u_{x}, u^2 u_{xx}, u^2 u_{xxx} ] library = Library1D ( poly_order = 2 , diff_order = 3 ) Configuration of the sparsity estimator and sparsity scheduler used. In this case we use the most basic threshold-based Lasso estimator and a scheduler that asseses the validation loss after a given patience. If that value is smaller than 1e-5, the algorithm is converged. estimator = Threshold ( 0.1 ) sparsity_scheduler = TrainTestPeriodic ( periodicity = 50 , patience = 200 , delta = 1e-5 ) Configuration of the sparsity estimator constraint = LeastSquares () # Configuration of the sparsity scheduler Now we instantiate the model and select the optimizer model = DeepMoD ( network , library , estimator , constraint ) # Defining optimizer optimizer = torch . optim . Adam ( model . parameters (), betas = ( 0.99 , 0.99 ), amsgrad = True , lr = 1e-3 )","title":"Configuring DeepMoD"},{"location":"examples/Burgers/PDE_Burgers/#run-deepmod","text":"We can now run DeepMoD using all the options we have set and the training data: * The directory where the tensorboard file is written (log_dir) * The ratio of train/test set used (split) * The maximum number of iterations performed (max_iterations) * The absolute change in L1 norm considered converged (delta) * The amount of epochs over which the absolute change in L1 norm is calculated (patience) train ( model , X_train , y_train , optimizer , sparsity_scheduler , log_dir = 'runs/Burgers/' , split = 0.8 , max_iterations = 100000 ) 9750 MSE: 8.48e-05 Reg: 1.16e-05 L1: 1.45e+00 Algorithm converged. Writing model to disk. Sparsity masks provide the active and non-active terms in the PDE: model . sparsity_masks [tensor([False, False, True, False, False, True, False, False, False, False, False, False])] estimatior_coeffs gives the magnitude of the active terms: print ( model . estimator_coeffs ()) [array([[ 0. ], [ 0. ], [ 0.40999228], [ 0. ], [ 0. ], [-0.99958825], [ 0. ], [ 0. ], [ 0. ], [ 0. ], [ 0. ], [ 0. ]], dtype=float32)]","title":"Run DeepMoD"},{"location":"examples/ODE_nonlin/ODE/","text":"Example ODE with custom library In this notebook we provide a simple example of the DeepMoD algorithm by applying it on the a non-linear ODE We start by importing the required DeepMoD functions: # General imports import numpy as np import torch import matplotlib.pylab as plt # DeepMoD functions from deepymod import DeepMoD from deepymod.model.func_approx import NN from deepymod.model.constraint import LeastSquares from deepymod.model.sparse_estimators import Threshold , PDEFIND from deepymod.training import train from deepymod.training.sparsity_scheduler import TrainTestPeriodic from scipy.io import loadmat import torch from torch.autograd import grad from itertools import combinations from functools import reduce from typing import Tuple from deepymod.utils.types import TensorList from deepymod import Library % load_ext autoreload % autoreload 2 from scipy.integrate import odeint # Settings for reproducibility np . random . seed ( 40 ) torch . manual_seed ( 0 ) The autoreload extension is already loaded . To reload it , use : %reload_ext autoreload < torch . _C . Generator at 0x105245f70 > Next, we prepare the dataset. The set of ODEs we consider here are d[y, z]/dt = [z, -z+ 5 \\sin y] d[y, z]/dt = [z, -z+ 5 \\sin y] def dU_dt_sin ( U , t ): return [ U [ 1 ], - 1 * U [ 1 ] - 5 * np . sin ( U [ 0 ])] U0 = [ 2.5 , 0.4 ] ts = np . linspace ( 0 , 5 , 100 ) Y = odeint ( dU_dt_sin , U0 , ts ) T = ts . reshape ( - 1 , 1 ) Here we can potentially rescale the Y and T axis and we plot the results T_rs = T / np . max ( np . abs ( T ), axis = 0 ) Y_rs = Y / np . max ( np . abs ( Y ), axis = 0 ) Let's plot it to get an idea of the data: fig , ax = plt . subplots () ax . plot ( T_rs , Y_rs [:, 0 ]) ax . plot ( T_rs , Y_rs [:, 1 ]) ax . set_xlabel ( 't' ) plt . show () number_of_samples = 500 idx = np . random . permutation ( Y . shape [ 0 ]) X = torch . tensor ( T_rs [ idx , :][: number_of_samples ], dtype = torch . float32 , requires_grad = True ) y = torch . tensor ( Y_rs [ idx , :][: number_of_samples ], dtype = torch . float32 ) print ( X . shape , y . shape ) torch.Size([100, 1]) torch.Size([100, 2]) Setup a custom library In this notebook we show how the user can create a custom build library.The library function, \\theta \\theta , in this case contains [1,u,v, sin(u)] [1,u,v, sin(u)] to showcase that non-linear terms can easily be added to the library from torch.autograd import grad from itertools import combinations , product from functools import reduce class Library_nonlinear ( Library ): \"\"\"[summary] Args: Library ([type]): [description] \"\"\" def __init__ ( self ) -> None : super () . __init__ () def library ( self , input : Tuple [ torch . Tensor , torch . Tensor ]) -> Tuple [ TensorList , TensorList ]: prediction , data = input samples = prediction . shape [ 0 ] poly_list = [] deriv_list = [] time_deriv_list = [] # Construct the theta matrix C = torch . ones_like ( prediction [:, 0 ]) . view ( samples , - 1 ) u = prediction [:, 0 ] . view ( samples , - 1 ) v = prediction [:, 1 ] . view ( samples , - 1 ) theta = torch . cat (( C , u , v , torch . sin ( u )), dim = 1 ) # Construct a list of time_derivatives time_deriv_list = [] for output in torch . arange ( prediction . shape [ 1 ]): dy = grad ( prediction [:, output ], data , grad_outputs = torch . ones_like ( prediction [:, output ]), create_graph = True )[ 0 ] time_deriv = dy [:, 0 : 1 ] time_deriv_list . append ( time_deriv ) return time_deriv_list , [ theta , theta ] Configuring DeepMoD Configuration of the function approximator: Here the first argument is the number of input and the last argument the number of output layers. network = NN ( 1 , [ 30 , 30 , 30 , 30 ], 2 ) Configuration of the library function: We select the custom build library we created earlier library = Library_nonlinear () Configuration of the sparsity estimator and sparsity scheduler used. In this case we use the most basic threshold-based Lasso estimator and a scheduler that asseses the validation loss after a given patience. If that value is smaller than 1e-5, the algorithm is converged. estimator = Threshold ( 0.5 ) sparsity_scheduler = TrainTestPeriodic ( periodicity = 50 , patience = 200 , delta = 1e-5 ) Configuration of the sparsity estimator constraint = LeastSquares () # Configuration of the sparsity scheduler Now we instantiate the model and select the optimizer model = DeepMoD ( network , library , estimator , constraint ) # Defining optimizer optimizer = torch . optim . Adam ( model . parameters (), betas = ( 0.99 , 0.99 ), amsgrad = True , lr = 1e-3 ) Run DeepMoD We can now run DeepMoD using all the options we have set and the training data. We need to slightly preprocess the input data for the derivatives: train ( model , X , y , optimizer , sparsity_scheduler , log_dir = 'runs/coupled2/' , split = 0.8 , max_iterations = 100000 , delta = 1e-3 , patience = 100 ) 21450 MSE: 2.99e-02 Reg: 3.16e-03 L1: 2.65e+00 Algorithm converged. Writing model to disk. Now that DeepMoD has converged, it has found the following numbers: model . sparsity_masks [tensor([False, False, True, False]), tensor([False, False, True, True])] print ( model . estimator_coeffs ()) [array([[0. ], [0. ], [0.99987924], [0. ]], dtype=float32), array([[ 0. ], [ 0. ], [-0.56510067], [-1.076641 ]], dtype=float32)]","title":"Non-linear ODE"},{"location":"examples/ODE_nonlin/ODE/#example-ode-with-custom-library","text":"In this notebook we provide a simple example of the DeepMoD algorithm by applying it on the a non-linear ODE We start by importing the required DeepMoD functions: # General imports import numpy as np import torch import matplotlib.pylab as plt # DeepMoD functions from deepymod import DeepMoD from deepymod.model.func_approx import NN from deepymod.model.constraint import LeastSquares from deepymod.model.sparse_estimators import Threshold , PDEFIND from deepymod.training import train from deepymod.training.sparsity_scheduler import TrainTestPeriodic from scipy.io import loadmat import torch from torch.autograd import grad from itertools import combinations from functools import reduce from typing import Tuple from deepymod.utils.types import TensorList from deepymod import Library % load_ext autoreload % autoreload 2 from scipy.integrate import odeint # Settings for reproducibility np . random . seed ( 40 ) torch . manual_seed ( 0 ) The autoreload extension is already loaded . To reload it , use : %reload_ext autoreload < torch . _C . Generator at 0x105245f70 > Next, we prepare the dataset. The set of ODEs we consider here are d[y, z]/dt = [z, -z+ 5 \\sin y] d[y, z]/dt = [z, -z+ 5 \\sin y] def dU_dt_sin ( U , t ): return [ U [ 1 ], - 1 * U [ 1 ] - 5 * np . sin ( U [ 0 ])] U0 = [ 2.5 , 0.4 ] ts = np . linspace ( 0 , 5 , 100 ) Y = odeint ( dU_dt_sin , U0 , ts ) T = ts . reshape ( - 1 , 1 ) Here we can potentially rescale the Y and T axis and we plot the results T_rs = T / np . max ( np . abs ( T ), axis = 0 ) Y_rs = Y / np . max ( np . abs ( Y ), axis = 0 ) Let's plot it to get an idea of the data: fig , ax = plt . subplots () ax . plot ( T_rs , Y_rs [:, 0 ]) ax . plot ( T_rs , Y_rs [:, 1 ]) ax . set_xlabel ( 't' ) plt . show () number_of_samples = 500 idx = np . random . permutation ( Y . shape [ 0 ]) X = torch . tensor ( T_rs [ idx , :][: number_of_samples ], dtype = torch . float32 , requires_grad = True ) y = torch . tensor ( Y_rs [ idx , :][: number_of_samples ], dtype = torch . float32 ) print ( X . shape , y . shape ) torch.Size([100, 1]) torch.Size([100, 2])","title":"Example ODE with custom library"},{"location":"examples/ODE_nonlin/ODE/#setup-a-custom-library","text":"In this notebook we show how the user can create a custom build library.The library function, \\theta \\theta , in this case contains [1,u,v, sin(u)] [1,u,v, sin(u)] to showcase that non-linear terms can easily be added to the library from torch.autograd import grad from itertools import combinations , product from functools import reduce class Library_nonlinear ( Library ): \"\"\"[summary] Args: Library ([type]): [description] \"\"\" def __init__ ( self ) -> None : super () . __init__ () def library ( self , input : Tuple [ torch . Tensor , torch . Tensor ]) -> Tuple [ TensorList , TensorList ]: prediction , data = input samples = prediction . shape [ 0 ] poly_list = [] deriv_list = [] time_deriv_list = [] # Construct the theta matrix C = torch . ones_like ( prediction [:, 0 ]) . view ( samples , - 1 ) u = prediction [:, 0 ] . view ( samples , - 1 ) v = prediction [:, 1 ] . view ( samples , - 1 ) theta = torch . cat (( C , u , v , torch . sin ( u )), dim = 1 ) # Construct a list of time_derivatives time_deriv_list = [] for output in torch . arange ( prediction . shape [ 1 ]): dy = grad ( prediction [:, output ], data , grad_outputs = torch . ones_like ( prediction [:, output ]), create_graph = True )[ 0 ] time_deriv = dy [:, 0 : 1 ] time_deriv_list . append ( time_deriv ) return time_deriv_list , [ theta , theta ]","title":"Setup a custom library"},{"location":"examples/ODE_nonlin/ODE/#configuring-deepmod","text":"Configuration of the function approximator: Here the first argument is the number of input and the last argument the number of output layers. network = NN ( 1 , [ 30 , 30 , 30 , 30 ], 2 ) Configuration of the library function: We select the custom build library we created earlier library = Library_nonlinear () Configuration of the sparsity estimator and sparsity scheduler used. In this case we use the most basic threshold-based Lasso estimator and a scheduler that asseses the validation loss after a given patience. If that value is smaller than 1e-5, the algorithm is converged. estimator = Threshold ( 0.5 ) sparsity_scheduler = TrainTestPeriodic ( periodicity = 50 , patience = 200 , delta = 1e-5 ) Configuration of the sparsity estimator constraint = LeastSquares () # Configuration of the sparsity scheduler Now we instantiate the model and select the optimizer model = DeepMoD ( network , library , estimator , constraint ) # Defining optimizer optimizer = torch . optim . Adam ( model . parameters (), betas = ( 0.99 , 0.99 ), amsgrad = True , lr = 1e-3 )","title":"Configuring DeepMoD"},{"location":"examples/ODE_nonlin/ODE/#run-deepmod","text":"We can now run DeepMoD using all the options we have set and the training data. We need to slightly preprocess the input data for the derivatives: train ( model , X , y , optimizer , sparsity_scheduler , log_dir = 'runs/coupled2/' , split = 0.8 , max_iterations = 100000 , delta = 1e-3 , patience = 100 ) 21450 MSE: 2.99e-02 Reg: 3.16e-03 L1: 2.65e+00 Algorithm converged. Writing model to disk. Now that DeepMoD has converged, it has found the following numbers: model . sparsity_masks [tensor([False, False, True, False]), tensor([False, False, True, True])] print ( model . estimator_coeffs ()) [array([[0. ], [0. ], [0.99987924], [0. ]], dtype=float32), array([[ 0. ], [ 0. ], [-0.56510067], [-1.076641 ]], dtype=float32)]","title":"Run DeepMoD"},{"location":"examples/PDE_KdV/PDE_KdV/","text":"Example Korteweg de Vries equation In this notebook we provide a simple example of the DeepMoD algorithm by applying it on the KdV equation. # General imports import numpy as np import torch import matplotlib.pylab as plt # DeepMoD functions from deepymod import DeepMoD from deepymod.model.func_approx import NN from deepymod.model.library import Library1D from deepymod.model.constraint import LeastSquares from deepymod.model.sparse_estimators import Threshold , PDEFIND from deepymod.training import train from deepymod.training.sparsity_scheduler import TrainTestPeriodic from scipy.io import loadmat # Settings for reproducibility np . random . seed ( 42 ) torch . manual_seed ( 0 ) % load_ext autoreload % autoreload 2 Next, we prepare the dataset. data = np . load ( '../data/kdv.npy' , allow_pickle = True ) . item () print ( 'Shape of grid:' , data [ 'x' ] . shape ) Shape of grid: (512, 201) Let's plot it to get an idea of the data: fig , ax = plt . subplots () im = ax . contourf ( data [ 'x' ], data [ 't' ], np . real ( data [ 'u' ])) ax . set_xlabel ( 'x' ) ax . set_ylabel ( 't' ) fig . colorbar ( mappable = im ) plt . show () X = np . transpose (( data [ 't' ] . flatten (), data [ 'x' ] . flatten ())) y = np . real ( data [ 'u' ]) . reshape (( data [ 'u' ] . size , 1 )) print ( X . shape , y . shape ) (102912, 2) (102912, 1) As we can see, X X has 2 dimensions, \\{x, t\\} \\{x, t\\} , while y y has only one, \\{u\\} \\{u\\} . Always explicity set the shape (i.e. N\\times 1 N\\times 1 , not N N ) or you'll get errors. This dataset is noiseless, so let's add 2.5\\% 2.5\\% noise: noise_level = 0.025 y_noisy = y + noise_level * np . std ( y ) * np . random . randn ( y [:, 0 ] . size , 1 ) The dataset is also much larger than needed, so let's hussle it and pick out a 1000 samples: number_of_samples = 1000 idx = np . random . permutation ( y . shape [ 0 ]) X_train = torch . tensor ( X [ idx , :][: number_of_samples ], dtype = torch . float32 , requires_grad = True ) y_train = torch . tensor ( y_noisy [ idx , :][: number_of_samples ], dtype = torch . float32 ) print ( X_train . shape , y_train . shape ) torch.Size([1000, 2]) torch.Size([1000, 1]) We now have a dataset which we can use. Let's plot, for a final time, the original dataset, the noisy set and the samples points: fig , axes = plt . subplots ( ncols = 3 , figsize = ( 15 , 4 )) im0 = axes [ 0 ] . contourf ( data [ 'x' ], data [ 't' ], np . real ( data [ 'u' ]), cmap = 'coolwarm' ) axes [ 0 ] . set_xlabel ( 'x' ) axes [ 0 ] . set_ylabel ( 't' ) axes [ 0 ] . set_title ( 'Ground truth' ) im1 = axes [ 1 ] . contourf ( data [ 'x' ], data [ 't' ], y_noisy . reshape ( data [ 'x' ] . shape ), cmap = 'coolwarm' ) axes [ 1 ] . set_xlabel ( 'x' ) axes [ 1 ] . set_title ( 'Noisy' ) sampled = np . array ([ y_noisy [ index , 0 ] if index in idx [: number_of_samples ] else np . nan for index in np . arange ( data [ 'x' ] . size )]) sampled = np . rot90 ( sampled . reshape ( data [ 'x' ] . shape )) #array needs to be rotated because of imshow im2 = axes [ 2 ] . imshow ( sampled , aspect = 'auto' , cmap = 'coolwarm' ) axes [ 2 ] . set_xlabel ( 'x' ) axes [ 2 ] . set_title ( 'Sampled' ) fig . colorbar ( im1 , ax = axes . ravel () . tolist ()) plt . show () Configuring DeepMoD Configuration of the function approximator: Here the first argument is the number of input and the last argument the number of output layers. network = NN ( 2 , [ 50 , 50 , 50 , 50 ], 1 ) Configuration of the library function: We select athe library with a 2D spatial input. Note that that the max differential order has been pre-determined here out of convinience. So, for poly_order 1 the library contains the following 12 terms: * [ 1, u_x, u_{xx}, u_{xxx}, u, u u_{x}, u u_{xx}, u u_{xxx}, u^2, u^2 u_{x}, u^2 u_{xx}, u^2 u_{xxx} 1, u_x, u_{xx}, u_{xxx}, u, u u_{x}, u u_{xx}, u u_{xxx}, u^2, u^2 u_{x}, u^2 u_{xx}, u^2 u_{xxx} ] library = Library1D ( poly_order = 2 , diff_order = 3 ) Configuration of the sparsity estimator and sparsity scheduler used. In this case we use the most basic threshold-based Lasso estimator and a scheduler that asseses the validation loss after a given patience. If that value is smaller than 1e-5, the algorithm is converged. estimator = Threshold ( 0.1 ) sparsity_scheduler = TrainTestPeriodic ( periodicity = 50 , patience = 10 , delta = 1e-5 ) Configuration of the sparsity estimator constraint = LeastSquares () # Configuration of the sparsity scheduler Now we instantiate the model and select the optimizer model = DeepMoD ( network , library , estimator , constraint ) # Defining optimizer optimizer = torch . optim . Adam ( model . parameters (), betas = ( 0.99 , 0.99 ), amsgrad = True , lr = 1e-3 ) Run DeepMoD We can now run DeepMoD using all the options we have set and the training data: * The directory where the tensorboard file is written (log_dir) * The ratio of train/test set used (split) * The maximum number of iterations performed (max_iterations) * The absolute change in L1 norm considered converged (delta) * The amount of epochs over which the absolute change in L1 norm is calculated (patience) train ( model , X_train , y_train , optimizer , sparsity_scheduler , log_dir = 'runs/KDV/' , split = 0.8 , max_iterations = 100000 , delta = 1e-4 , patience = 8 ) | Iteration | Progress | Time remaining | Loss | MSE | Reg | L1 norm | 25775 25.77% 3510s 7.15e-06 6.44e-06 7.10e-07 2.88e+00 Algorithm converged. Stopping training. Sparsity masks provide the active and non-active terms in the PDE: model . sparsity_masks [tensor([False, False, False, True, False, True, False, False, False, False, False, False])] estimatior_coeffs gives the magnitude of the active terms: print ( model . estimator_coeffs ()) [array([[ 0. ], [ 0. ], [ 0. ], [-0.9088099], [ 0. ], [-1.7334794], [ 0. ], [ 0. ], [ 0. ], [ 0. ], [ 0. ], [ 0. ]], dtype=float32)]","title":"KdV"},{"location":"examples/PDE_KdV/PDE_KdV/#example-korteweg-de-vries-equation","text":"In this notebook we provide a simple example of the DeepMoD algorithm by applying it on the KdV equation. # General imports import numpy as np import torch import matplotlib.pylab as plt # DeepMoD functions from deepymod import DeepMoD from deepymod.model.func_approx import NN from deepymod.model.library import Library1D from deepymod.model.constraint import LeastSquares from deepymod.model.sparse_estimators import Threshold , PDEFIND from deepymod.training import train from deepymod.training.sparsity_scheduler import TrainTestPeriodic from scipy.io import loadmat # Settings for reproducibility np . random . seed ( 42 ) torch . manual_seed ( 0 ) % load_ext autoreload % autoreload 2 Next, we prepare the dataset. data = np . load ( '../data/kdv.npy' , allow_pickle = True ) . item () print ( 'Shape of grid:' , data [ 'x' ] . shape ) Shape of grid: (512, 201) Let's plot it to get an idea of the data: fig , ax = plt . subplots () im = ax . contourf ( data [ 'x' ], data [ 't' ], np . real ( data [ 'u' ])) ax . set_xlabel ( 'x' ) ax . set_ylabel ( 't' ) fig . colorbar ( mappable = im ) plt . show () X = np . transpose (( data [ 't' ] . flatten (), data [ 'x' ] . flatten ())) y = np . real ( data [ 'u' ]) . reshape (( data [ 'u' ] . size , 1 )) print ( X . shape , y . shape ) (102912, 2) (102912, 1) As we can see, X X has 2 dimensions, \\{x, t\\} \\{x, t\\} , while y y has only one, \\{u\\} \\{u\\} . Always explicity set the shape (i.e. N\\times 1 N\\times 1 , not N N ) or you'll get errors. This dataset is noiseless, so let's add 2.5\\% 2.5\\% noise: noise_level = 0.025 y_noisy = y + noise_level * np . std ( y ) * np . random . randn ( y [:, 0 ] . size , 1 ) The dataset is also much larger than needed, so let's hussle it and pick out a 1000 samples: number_of_samples = 1000 idx = np . random . permutation ( y . shape [ 0 ]) X_train = torch . tensor ( X [ idx , :][: number_of_samples ], dtype = torch . float32 , requires_grad = True ) y_train = torch . tensor ( y_noisy [ idx , :][: number_of_samples ], dtype = torch . float32 ) print ( X_train . shape , y_train . shape ) torch.Size([1000, 2]) torch.Size([1000, 1]) We now have a dataset which we can use. Let's plot, for a final time, the original dataset, the noisy set and the samples points: fig , axes = plt . subplots ( ncols = 3 , figsize = ( 15 , 4 )) im0 = axes [ 0 ] . contourf ( data [ 'x' ], data [ 't' ], np . real ( data [ 'u' ]), cmap = 'coolwarm' ) axes [ 0 ] . set_xlabel ( 'x' ) axes [ 0 ] . set_ylabel ( 't' ) axes [ 0 ] . set_title ( 'Ground truth' ) im1 = axes [ 1 ] . contourf ( data [ 'x' ], data [ 't' ], y_noisy . reshape ( data [ 'x' ] . shape ), cmap = 'coolwarm' ) axes [ 1 ] . set_xlabel ( 'x' ) axes [ 1 ] . set_title ( 'Noisy' ) sampled = np . array ([ y_noisy [ index , 0 ] if index in idx [: number_of_samples ] else np . nan for index in np . arange ( data [ 'x' ] . size )]) sampled = np . rot90 ( sampled . reshape ( data [ 'x' ] . shape )) #array needs to be rotated because of imshow im2 = axes [ 2 ] . imshow ( sampled , aspect = 'auto' , cmap = 'coolwarm' ) axes [ 2 ] . set_xlabel ( 'x' ) axes [ 2 ] . set_title ( 'Sampled' ) fig . colorbar ( im1 , ax = axes . ravel () . tolist ()) plt . show ()","title":"Example Korteweg de Vries equation"},{"location":"examples/PDE_KdV/PDE_KdV/#configuring-deepmod","text":"Configuration of the function approximator: Here the first argument is the number of input and the last argument the number of output layers. network = NN ( 2 , [ 50 , 50 , 50 , 50 ], 1 ) Configuration of the library function: We select athe library with a 2D spatial input. Note that that the max differential order has been pre-determined here out of convinience. So, for poly_order 1 the library contains the following 12 terms: * [ 1, u_x, u_{xx}, u_{xxx}, u, u u_{x}, u u_{xx}, u u_{xxx}, u^2, u^2 u_{x}, u^2 u_{xx}, u^2 u_{xxx} 1, u_x, u_{xx}, u_{xxx}, u, u u_{x}, u u_{xx}, u u_{xxx}, u^2, u^2 u_{x}, u^2 u_{xx}, u^2 u_{xxx} ] library = Library1D ( poly_order = 2 , diff_order = 3 ) Configuration of the sparsity estimator and sparsity scheduler used. In this case we use the most basic threshold-based Lasso estimator and a scheduler that asseses the validation loss after a given patience. If that value is smaller than 1e-5, the algorithm is converged. estimator = Threshold ( 0.1 ) sparsity_scheduler = TrainTestPeriodic ( periodicity = 50 , patience = 10 , delta = 1e-5 ) Configuration of the sparsity estimator constraint = LeastSquares () # Configuration of the sparsity scheduler Now we instantiate the model and select the optimizer model = DeepMoD ( network , library , estimator , constraint ) # Defining optimizer optimizer = torch . optim . Adam ( model . parameters (), betas = ( 0.99 , 0.99 ), amsgrad = True , lr = 1e-3 )","title":"Configuring DeepMoD"},{"location":"examples/PDE_KdV/PDE_KdV/#run-deepmod","text":"We can now run DeepMoD using all the options we have set and the training data: * The directory where the tensorboard file is written (log_dir) * The ratio of train/test set used (split) * The maximum number of iterations performed (max_iterations) * The absolute change in L1 norm considered converged (delta) * The amount of epochs over which the absolute change in L1 norm is calculated (patience) train ( model , X_train , y_train , optimizer , sparsity_scheduler , log_dir = 'runs/KDV/' , split = 0.8 , max_iterations = 100000 , delta = 1e-4 , patience = 8 ) | Iteration | Progress | Time remaining | Loss | MSE | Reg | L1 norm | 25775 25.77% 3510s 7.15e-06 6.44e-06 7.10e-07 2.88e+00 Algorithm converged. Stopping training. Sparsity masks provide the active and non-active terms in the PDE: model . sparsity_masks [tensor([False, False, False, True, False, True, False, False, False, False, False, False])] estimatior_coeffs gives the magnitude of the active terms: print ( model . estimator_coeffs ()) [array([[ 0. ], [ 0. ], [ 0. ], [-0.9088099], [ 0. ], [-1.7334794], [ 0. ], [ 0. ], [ 0. ], [ 0. ], [ 0. ], [ 0. ]], dtype=float32)]","title":"Run DeepMoD"}]}